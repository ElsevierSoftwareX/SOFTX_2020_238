#!/usr/bin/env python
#
# Copyright (C) 2009-2014  Kipp Cannon, Chad Hanna, Drew Keppel
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
"""Stream-based inspiral analysis tool"""


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#

## @file gstlal_inspiral
# A program to analyze gravitational wave data for compact binary coalescence in real time or in an offline mode
#
# @dot
# digraph llpipe {
# 	labeljust = "r";
# 	label="gstlal_inspiral"
# 	rankdir=LR;
# 	graph [fontname="Roman", fontsize=24];
# 	edge [ fontname="Roman", fontsize=10 ];
# 	node [fontname="Roman", shape=box, fontsize=11];
# 
# 	gracedb [label="GW\nCandidate\nDatabase", shape=oval, color=tomato3, style=filled];
# 
# 
# 	subgraph clusterNodeN {
# 
# 		style=rounded;
# 		label="gstreamer pipeline";
# 		labeljust = "r";
# 		fontsize = 14;
# 
# 		H1src [label="H1 data source:\n mkbasicsrc()", color=red4, URL="\ref pipeparts.mkbasicsrc()"];
# 		L1src [label="L1 data source:\n mkbasicsrc()", color=green4, URL="\ref pipeparts.mkbasicsrc()"];
# 		V1src [label="V1 data source:\n mkbasicsrc()", color=magenta4];
# 		
# 		H1multirate [label="H1 whitening and downsampling:\nmkwhitened_multirate_src()", color=red4, URL="\ref multirate_datasource.mkwhitened_multirate_src()"];
# 		L1multirate [label="L1 whitening and downsampling:\nmkwhitened_multirate_src()", color=green4, URL="\ref multirate_datasource.mkwhitened_multirate_src()"];
# 		V1multirate [label="V1 whitening and downsampling:\nmkwhitened_multirate_src()", color=magenta4, URL="\ref multirate_datasource.mkwhitened_multirate_src()"];
# 		
# 		H1LLOID [label="H1 LLOID filtering engine:\nmkLLOIDmulti()", color=red4, URL="\ref lloidparts.mkLLOIDmulti()"];
# 		L1LLOID [label="L1 LLOID filtering engine:\nmkLLOIDmulti()", color=green4, URL="\ref lloidparts.mkLLOIDmulti()"];
# 		V1LLOID [label="V1 LLOID filtering engine:\nmkLLOIDmulti()", color=magenta4, URL="\ref lloidparts.mkLLOIDmulti()"];
# 
# 		H1Trig1 [label="H1 Triggering:\nsub bank 1", color=red4];
# 		L1Trig1 [label="L1 Triggering:\nsub bank 1", color=green4];
# 		V1Trig1 [label="V1 Triggering:\nsub bank 1", color=magenta4];
# 		H1Trig2 [label="H1 Triggering:\nsub bank 2", color=red4];
# 		L1Trig2 [label="L1 Triggering:\nsub bank 2", color=green4];
# 		V1Trig2 [label="V1 Triggering:\nsub bank 2", color=magenta4];
# 		H1TrigN [label="H1 Triggering:\nsub bank N", color=red4];
# 		L1TrigN [label="L1 Triggering:\nsub bank N", color=green4];
# 		V1TrigN [label="V1 Triggering:\nsub bank N", color=magenta4];
# 		
# 		H1src -> H1multirate;
# 		L1src -> L1multirate;
# 		V1src -> V1multirate;
# 
# 		H1multirate -> H1LLOID [label="h(t) 4096Hz"];
# 		L1multirate -> L1LLOID [label="h(t) 4096Hz"];
# 		V1multirate -> V1LLOID [label="h(t) 4096Hz"];
# 		H1multirate -> H1LLOID [label="h(t) 2048Hz"];
# 		L1multirate -> L1LLOID [label="h(t) 2048Hz"];
# 		V1multirate -> V1LLOID [label="h(t) 2048Hz"];
# 		H1multirate -> H1LLOID [label="h(t) Nth-pow-of-2 Hz"];
# 		L1multirate -> L1LLOID [label="h(t) Nth-pow-of-2 Hz"];
# 		V1multirate -> V1LLOID [label="h(t) Nth-pow-of-2 Hz"];
# 	
# 		H1LLOID -> H1Trig1 [label="SNRs sub bank 1"];
# 		L1LLOID -> L1Trig1 [label="SNRs sub bank 1"];
# 		V1LLOID -> V1Trig1 [label="SNRs sub bank 1"];
# 		H1LLOID -> H1Trig2 [label="SNRs sub bank 2"];
# 		L1LLOID -> L1Trig2 [label="SNRs sub bank 2"];
# 		V1LLOID -> V1Trig2 [label="SNRs sub bank 2"];
# 		H1LLOID -> H1TrigN [label="SNRs sub bank N"];
# 		L1LLOID -> L1TrigN [label="SNRs sub bank N"];
# 		V1LLOID -> V1TrigN [label="SNRs sub bank N"];
# 	}
# 
# 
# 	Coincidence [label="Coincidence\nO(1)s latency"];
# 	SigEst [label="Significance\nEstimation\nO(1)s latency"];
# 	Thresh [label="Thresholding\nO(1)s latency"];
# 	EventGen [label="Event\nGeneration\nO(1)s latency"];
# 		
# 	H1Trig1 -> Coincidence [label="Trigs sub bank 1"];
# 	L1Trig1 -> Coincidence [label="Trigs sub bank 1"];
# 	V1Trig1 -> Coincidence [label="Trigs sub bank 1"];
# 	H1Trig2 -> Coincidence [label="Trigs sub bank 2"];
# 	L1Trig2 -> Coincidence [label="Trigs sub bank 2"];
# 	V1Trig2 -> Coincidence [label="Trigs sub bank 2"];
# 	H1TrigN -> Coincidence [label="Trigs sub bank N"];
# 	L1TrigN -> Coincidence [label="Trigs sub bank N"];
# 	V1TrigN -> Coincidence [label="Trigs sub bank N"];
# 	
# 	Coincidence -> SigEst -> Thresh -> EventGen;
# 
# 	EventGen -> gracedb;
# 	
# }
# @enddot
#
# ### Command line interface
#
#	+ `--local-frame-caching`
#	+ `--psd-fft-length` [s] (int): The length of the FFT used to used to whiten the data (default 32 s).
#	+ `--veto-segments-file` [filename]: Set the name of the LIGO light-weight XML file from which to load vetoes (optional).
#	+ `--veto-segments-name` [name]: Set the name of the segments to extract from the segment tables and use as the veto list, default = "vetoes".
#	+ `--nxydump-segment` [start:stop]: Set the time interval to dump from nxydump elments (optional).  The default is \":\", i.e. dump all time."
#	+ `--output` [filename]: Set the name of the LIGO light-weight XML output file *.{xml,xml.gz} or an SQLite database *.sqlite (required).
#	+ `--reference-psd` [filename]: Instead of measuring the noise spectrum, load the spectrum from this LIGO light-weight XML file (optional).
#	+ `--track-psd`: Track PSD even if a reference is given.
#	+ `--svd-bank` [filename]: Set the name of the LIGO light-weight XML file from which to load the svd bank for a given instrument in the form ifo:file, These can be given as a comma separated list such as H1:file1,H2:file2,L1:file3 to analyze multiple instruments.  This option can be given multiple times in order to analyze bank serially.  At least one svd bank for at least 2 detectors is required.
#	+ `--time-slide-file` [filename]: Set the name of the xml file to get time slide offsets (required).
#	+ `--control-peak-time` [time] (int): Set a time window in seconds to find peaks in the control signal.
#	+ `--fir-stride` [time] (int): Set the length of the fir filter stride in seconds, default = 8.
#	+ `--ht-gate-threshold` [threshold] (float): Set the threshold on whitened h(t) to mark samples as gaps (glitch removal), default = infinity. Should be given per bank file.
#	+ `--chisq-type" [type]: Choose the type of chisq computation to perform. Must be one of (autochisq|timeslicechisq). The default is autochisq.
#	+ `--coincidence-threshold` [value] (float): Set the coincidence window in seconds (default = 0.005).  The light-travel time between instruments will be added automatically in the coincidence test.
#	+ `--min-instruments` [count] (int): Set the minimum number of instruments that must contribute triggers to form a candidate (default = 2).
#	+ `--min-log-L` [log likelihood ratio] (float): Discard candidates that get assigned log likelihood ratios below this threshold (default = keep all).
#	+ `--write-pipeline` [filename]: Write a DOT graph description of the as-built pipeline to this file (optional).  The environment variable GST_DEBUG_DUMP_DOT_DIR must be set for this option to work.
#	+ `--comment`: Set the string to be recorded in comment and tag columns in various places in the output file (optional).
#	+ `--check-time-stamps`: Turn on time stamp checking.
#	+ `--verbose`: Be verbose (optional).
#	+ `--tmp-space` [path]: Path to a directory suitable for use as a work area while manipulating the database file.  The database file will be worked on in this directory, and then moved to the final location when complete.  This option is intended to improve performance when running in a networked environment, where there might be a local disk with higher bandwidth than is available to the filesystem on which the final output will reside.
#	+ `--blind-injections` [filename]: Set the name of an injection file that will be added to the data without saving the sim_inspiral_table or otherwise processing the data differently.  Has the effect of having hidden signals in the input data.  --injections must not be specified in this case.
#	+ `--job-tag`: Set the string to identify this job and register the resources it provides on a node.  Should be 4 digits of the form 0001, 0002, etc..
#	+ `--likelihood-file` [filename]: Set the name of the likelihood ratio data file to use for ranking events (either --likelihood-file or --reference-likelihood-file must be provided).
#	+ `--reference-likelihood-file` [filename]: Set the name of the likelihood ratio data file to use for ranking events (--data-source must be lvshm or framexmit) (--likelihood-snapshot-interval must provided) (either --likelihood-file or --reference-likelihood-file must be provided).
#	+ `--zero-lag-ranking-stat-file` [filename]: Record a histogram of the likelihood ratio ranking statistic values assigned to zero-lag candidates in this XML file, which must exist at start up and contain a RankingData object.  The counts will be added to the file.  Optional.  Can be given multiple times.
#	+ `--likelihood-snapshot-interval` [seconds] (float): How often to reread the marginalized likelihoood data. If --likelihood-file is provided, the likelihood file will be overwritten by a snapshot of the trigger files and a duplicate snapshot will be generated to keep a record of past ranking statistics.
#	+ `--marginalized-likelihood-file` [filename]: Set the name of the file from which to load initial marginalized likelihood ratio data.  This is required for online operation (when --data-source is framexmit or lvshm) and is forbidden for offline operation (all other data sources).
#	+ `--gracedb-far-threshold` (float): False alarm rate threshold for gracedb (Hz), if not given gracedb events are not sent.
#	+ `--gracedb-search`: gracedb type (default is LowMass).
#	+ `--gracedb-pipeline`: gracedb pipeline (default is gstlal).
#	+ `--gracedb-group`: gracedb group (default is Test).
#	+ `--gracedb-service-url`: gracedb service url (default is https://gracedb.ligo.org/api/)
#	+ `--thinca-interval` [secs] (float): Set the thinca interval, default = 30s.
#	+ `--singles-threshold` [SNR] (float): Set the SNR threshold at which to record single-instrument events in the output (default = +inf, i.e., don't retain singles).
#
# ### Review Status
#
# | Names                                          | Hash                                        | Date       | Diff to Head of Master      |
# | -------------------------------------------    | ------------------------------------------- | ---------- | --------------------------- |
# | Florent, Sathya, Duncan Me, Jolien, Kipp, Chad | 9074294d6b57f43651143b5f93210751de1fe55a    | 2014-05-02 | <a href="@gstlal_inspiral_cgit_diff/bin/gstlal_inspiral?id=HEAD&id2=9074294d6b57f43651143b5f93210751de1fe55a">gstlal_inspiral</a> |
#
# #### Actions
#
# - Consider cleaning up the nxydump segment option.  Currently it only works with modifying source code
# - Consider changing the thinca-interval name to thinca-cadence
# - consider replacing  the 'in ("framexmitsrc", "lvshmsrc")' tests with a property or method in the data source class
# - Consider allowing a single detector analysis
# - Consider deleting timeslicechisq


from collections import namedtuple
try:
	from fpconst import PosInf
except ImportError:
	# fpconst is not part of the standard library and might not be
	# available
	PosInf = float("+inf")
import gzip
import itertools
import math
from optparse import OptionParser
import os
import resource
import signal
import socket
import sys
import tempfile
import time

import gi
gi.require_version('Gst', '1.0')
from gi.repository import GObject, Gst
GObject.threads_init()
Gst.init(None)

import lal
from lal import LIGOTimeGPS
from lal.utils import CacheEntry

from glue import segments
from glue import segmentsUtils
from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import utils as ligolw_utils
from glue.ligolw.utils import segments as ligolw_segments
from gstlal import bottle
from gstlal import datasource
from gstlal import lloidparts
from gstlal import far
from gstlal import httpinterface
from gstlal import hoftcache
from gstlal import inspiral
from gstlal import inspiral_pipe
from gstlal import pipeparts
from gstlal import simulation

@lsctables.use_in
class LIGOLWContentHandler(ligolw.LIGOLWContentHandler):
	pass

#
# Make sure we have sufficient resources
# We allocate far more memory than we need, so this is okay
#

def setrlimit(res, lim):
	hard_lim = resource.getrlimit(res)[1]
	resource.setrlimit(res, (lim if lim is not None else hard_lim, hard_lim))

# set the number of processes and total set size up to hard limit and
# shrink the per-thread stack size (default is 10 MiB)
setrlimit(resource.RLIMIT_NPROC, None)
setrlimit(resource.RLIMIT_AS, None)
setrlimit(resource.RLIMIT_RSS, None)
# FIXME:  tests at CIT show that this next tweak has no effect.  it's
# possible that SL7 has lowered the default stack size from SL6 and we
# don't need to do this anymore.  remove?
setrlimit(resource.RLIMIT_STACK, 1024 * 1024) # 1 MiB per thread


def now():
	return LIGOTimeGPS(lal.UTCToGPS(time.gmtime()))


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		description = __doc__
	)

	# append all the datasource specific options
	datasource.append_options(parser)

	# local caching to help with I/O for offline running
	parser.add_option("--local-frame-caching", action = "store_true", help = "blah")

	parser.add_option("--psd-fft-length", metavar = "s", default = 32, type = "int", help = "The length of the FFT used to used to whiten the data (default 32 s).")
	parser.add_option("--veto-segments-file", metavar = "filename", help = "Set the name of the LIGO light-weight XML file from which to load vetoes (optional).")
	parser.add_option("--veto-segments-name", metavar = "name", help = "Set the name of the segments to extract from the segment tables and use as the veto list.", default = "vetoes")
	parser.add_option("--nxydump-segment", metavar = "start:stop", default = ":", help = "Set the time interval to dump from nxydump elments (optional).  The default is \":\", i.e. dump all time.")
	parser.add_option("--output", metavar = "filename", action = "append", default = [], help = "Set the name of the LIGO light-weight XML output file *.{xml,xml.gz} or an SQLite database *.sqlite (required).")
	parser.add_option("--output-cache", metavar = "filename", help = "Provide a cache file with the names of the LIGO light-weight XML output file *.{xml,xml.gz} or an SQLite database *.sqlite (required).")
	parser.add_option("--reference-psd", metavar = "filename", help = "Instead of measuring the noise spectrum, load the spectrum from this LIGO light-weight XML file (optional).")
	parser.add_option("--track-psd", action = "store_true", help = "Track PSD even if a reference is given")
	parser.add_option("--svd-bank", metavar = "filename", action = "append", default = [], help = "Set the name of the LIGO light-weight XML file from which to load the svd bank for a given instrument in the form ifo:file, These can be given as a comma separated list such as H1:file1,H2:file2,L1:file3 to analyze multiple instruments.  This option can be given multiple times in order to analyze bank serially.  At least one svd bank for at least 2 detectors is required.")
	parser.add_option("--svd-bank-cache", metavar = "filename", help = "Provide a cache file of svd-bank files")
	parser.add_option("--time-slide-file", metavar = "filename", help = "Set the name of the xml file to get time slide offsets (required).")
	parser.add_option("--control-peak-time", metavar = "time", type = "int", help = "Set a time window in seconds to find peaks in the control signal")
	parser.add_option("--fir-stride", metavar = "time", type = "int", default = 8, help = "Set the length of the fir filter stride in seconds. default = 8")
	parser.add_option("--ht-gate-threshold", metavar = "threshold", type = "float", action = "append", default = [], help = "Set the threshold on whitened h(t) to mark samples as gaps (glitch removal)")
	parser.add_option("--chisq-type", metavar = "type", default = "autochisq", help = "Choose the type of chisq computation to perform. Must be one of (autochisq|timeslicechisq). The default is autochisq.")
	parser.add_option("--coincidence-threshold", metavar = "value", type = "float", default = 0.005, help = "Set the coincidence window in seconds (default = 0.005).  The light-travel time between instruments will be added automatically in the coincidence test.")
	parser.add_option("--min-instruments", metavar = "count", type = "int", default = 2, help = "Set the minimum number of instruments that must contribute triggers to form a candidate (default = 2).")
	parser.add_option("--min-log-L", metavar = "log likelihood ratio", type = "float", help = "Discard candidates that get assigned log likelihood ratios below this threshold (default = keep all).")
	parser.add_option("--write-pipeline", metavar = "filename", help = "Write a DOT graph description of the as-built pipeline to this file (optional).  The environment variable GST_DEBUG_DUMP_DOT_DIR must be set for this option to work.")
	parser.add_option("--comment", help = "Set the string to be recorded in comment and tag columns in various places in the output file (optional).")
	parser.add_option("--check-time-stamps", action = "store_true", help = "Turn on time stamp checking")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose (optional).")
	parser.add_option("-t", "--tmp-space", metavar = "path", help = "Path to a directory suitable for use as a work area while manipulating the database file.  The database file will be worked on in this directory, and then moved to the final location when complete.  This option is intended to improve performance when running in a networked environment, where there might be a local disk with higher bandwidth than is available to the filesystem on which the final output will reside.")
	parser.add_option("--blind-injections", metavar = "filename", help = "Set the name of an injection file that will be added to the data without saving the sim_inspiral_table or otherwise processing the data differently.  Has the effect of having hidden signals in the input data.  --injections must not be specified in this case")

	# Online options

	parser.add_option("--job-tag", help = "Set the string to identify this job and register the resources it provides on a node.  Should be 4 digits of the form 0001, 0002, etc..")
	parser.add_option("--likelihood-file", metavar = "filename", action = "append", default = [], help = "Set the name of the likelihood ratio data file to use for ranking events (either --likelihood-file or --reference-likelihood-file must be provided)")
	parser.add_option("--likelihood-file-cache", metavar = "filename", help = "Cache file for likelihood ratio data to use for ranking events")
	parser.add_option("--reference-likelihood-file", metavar = "filename", help = "Set the name of the likelihood ratio data file to use for ranking events (--data-source must be lvshm or framexmit) (--likelihood-snapshot-interval must provided) (either --likelihood-file or --reference-likelihood-file must be provided)")
	parser.add_option("--zero-lag-ranking-stat-file", metavar = "filename", action = "append", help = "Record a histogram of the likelihood ratio ranking statistic values assigned to zero-lag candidates in this XML file, which must exist at start up and contain a RankingData object.  The counts will be added to the file.  Optional.  Can be given multiple times.")
	parser.add_option("--likelihood-snapshot-interval", type = "float", metavar = "seconds", help = "How often to reread the marginalized likelihoood data. If --likelihood-file is provided, the likelihood file will be overwritten by a snapshot of the trigger files and a duplicate snapshot will be generated to keep a record of past ranking statistics.")
	parser.add_option("--marginalized-likelihood-file", metavar = "filename", help = "Set the name of the file from which to load initial marginalized likelihood ratio data.  This is required for online operation (when --data-source is framexmit or lvshm) and is forbidden for offline operation (all other data sources).")
	parser.add_option("--gracedb-far-threshold", type = "float", help = "false alarm rate threshold for gracedb (Hz), if not given gracedb events are not sent")
	parser.add_option("--gracedb-search", default = "LowMass", help = "gracedb search, default is LowMass")
	parser.add_option("--gracedb-pipeline", default = "gstlal", help = "gracedb pipeline, default is gstlal")
	parser.add_option("--gracedb-group", default = "Test", help = "gracedb group, default is Test")
	parser.add_option("--gracedb-service-url", default = "https://gracedb.ligo.org/api/", help = "gracedb service url, default is https://gracedb.ligo.org/api/")
	parser.add_option("--thinca-interval", metavar = "secs", type = "float", default = 30.0, help = "Set the thinca interval, default = 30s")
	# NOTE:  the clustering SQL scripts search for this option in the
	# process_params table to determine the threshold below which it
	# can delete uninteresting singles after the coincs are ranked.  if
	# the name of this option is changed, be sure to update
	# simplify_and_cluster.sql and derivatives
	parser.add_option("--singles-threshold", metavar = "SNR", type = "float", default = PosInf, help = "Set the SNR threshold at which to record single-instrument events in the output (default = +inf, i.e. don't retain singles).")

	options, filenames = parser.parse_args()
	missing_options = []

	#
	# do this before messing with options object
	#

	process_params = options.__dict__.copy()

	#
	# check for options, files that are always required
	#

	# FIXME Put all svd banks for different detectors in one file.
	svd_banks = []
	if options.svd_bank_cache:
		svd_bank_cache = map(CacheEntry, open(options.svd_bank_cache))
		svd_bank_cache.sort(key = lambda cache_entry: cache_entry.description)
		for key, seq in itertools.groupby(svd_bank_cache, key = lambda cache_entry: cache_entry.description):
			svd_banks.append(dict((cache_entry.observatory, cache_entry.url) for cache_entry in seq))
	if options.svd_bank:
		try:
			svd_banks += map(inspiral.parse_svdbank_string, options.svd_bank)
		except ValueError as e:
			print "Unable to parse --svd-bank"
			raise
	if not svd_banks:
		missing_options.append("must supply at least one of --svd-bank or --svd-bank-cache")

	if options.output_cache:
		# do this out-of-place to preserve process_params' contents
		options.output = options.output + [CacheEntry(line).url for line in open(options.output_cache)]
	missing_options += ["--%s" % option.replace("_", "-") for option in ["output"] if getattr(options, option) is None]

	if options.likelihood_file_cache:
		# do this out-of-place to preserve process_params' contents
		options.likelihood_file = options.likelihood_file + [CacheEntry(line).url for line in open(options.likelihood_file_cache)]
	if options.likelihood_file is None and options.reference_likelihood_file is None:
		missing_options.append("either --likelihood-file or --reference-likelihood-file")

	if not options.time_slide_file:
		missing_options.append("--time-slide-file")

	if missing_options:
		raise ValueError("missing required option(s) %s" % ", ".join(sorted(missing_options)))

	detectors = datasource.GWDataSourceInfo(options)

	# FIXME: should also check for read permissions
	required_urls = []
	for svd_bank_set in svd_banks: 
		required_urls += svd_bank_set.values()
	if options.veto_segments_file:
		required_urls += [options.veto_segments_file]
	for i in range(len(required_urls)):
		try:
			required_urls[i] = ligolw_utils.local_path_from_url(required_urls[i])
		except ValueError:
			required_urls[i] = None
	missing_files = [ligolw_utils.local_path_from_url(url) for url in required_urls if url is not None and not os.path.exists(ligolw_utils.local_path_from_url(url))]

	if missing_files:
		raise ValueError("files %s do not exist" % ", ".join("'%s'" % filename for filename in sorted(missing_files)))

	#
	# check for mutually exclusive options
	#

	bad_combos = []
	if options.blind_injections and options.injections:
		bad_combos.append("(--blind-injections, --injections)")
	if bad_combos:
		raise ValueError("must use only one option from each set: %s" % ','.join(bad_combos))

	#
	# check sanity of options
	#

	# Online specific initialization
	# FIXME someday support other online sources
	if options.data_source in ("lvshm", "framexmit"):
		missed_options = []
		for option in ["job_tag", "marginalized_likelihood_file"]:
			if getattr(options, option) is None:
				missed_options.append("--%s" %option.replace("_","-"))

		if missed_options:
			raise ValueError("%s required for --data-source is lvshm or framexmit" % ", ".join(missed_options))

		if len(svd_banks) > 1:
			raise ValueError("more than one --svd-bank not allowed for --datasource lvshm or framexmit, %d given" % len(options.likelihood_file))

		# make an "infinite" extent segment
		detectors.seg = segments.segment(LIGOTimeGPS(0), LIGOTimeGPS(2000000000))

		# this gets set so that if you log into a node you can find out what the job id is easily
		os.environ['GSTLAL_LL_JOB'] = options.job_tag
	else:
		bad_options = []
		for option in ["job_tag", "marginalized_likelihood_file", "likelihood_snapshot_interval"]:
			if getattr(options, option) is not None:
				bad_options.append("--%s" % option.replace("_","-"))
		if bad_options:
			raise ValueError("%s options can only be given for --data-source is lvshm or framexmit " % ", ".join(bad_options))

	if options.reference_psd is None and not options.track_psd:
		raise ValueError("must use --track-psd if no reference psd is given, you can use both simultaneously")
	if options.psd_fft_length < 32:
		raise ValueError("--psd-fft-length cannot be less than 32")
	if options.local_frame_caching and not options.data_source == "frames":
		raise ValueError('--local-frame-caching can only be used if --data-source = "frames"')
	if options.chisq_type not in ["autochisq", "timeslicechisq"]:
		raise ValueError("--chisq-type must be one of (autochisq|timeslicechisq), given %s" % (options.chisq_type))
	
	if options.reference_likelihood_file and options.likelihood_file:
		likelihood_url_namedtuples_list = [namedtuple('likelihood_url_namedtuple',('likelihood_url','reference_likelihood_url'))(likelihood_file, options.reference_likelihood_file) for likelihood_file in options.likelihood_file]
	elif options.reference_likelihood_file and not options.likelihood_file:
		likelihood_url_namedtuples_list = [namedtuple('likelihood_url_namedtuple',('likelihood_url','reference_likelihood_url'))(None, options.reference_likelihood_file)]
	else:
		likelihood_url_namedtuples_list = [namedtuple('likelihood_url_namedtuple',('likelihood_url','reference_likelihood_url'))(likelihood_file, None) for likelihood_file in options.likelihood_file]

	# Checking options.ht_gate_threshold
	# If no gate threshold is given, use threshold of infinity
	if options.ht_gate_threshold == []:
		options.ht_gate_threshold = [float("inf")]*len(svd_banks)
	if not (len(options.ht_gate_threshold) == len(svd_banks)):
		raise ValueError("must have equal numbers of svd banks (%d) and ht-gate-threshold values (%d)" % (len(svd_banks), len(options.ht_gate_threshold)))

	if not (len(svd_banks) == len(options.output) == len(likelihood_url_namedtuples_list)):
		raise ValueError("must have equal numbers of --svd-bank (%d), --output (%d) and --likelihood-file (%d) (and/or entries in the corresponding caches)" % (len(svd_banks), len(options.output), len(likelihood_url_namedtuples_list)))

	if options.min_instruments < 1:
		raise ValueError("--min-instruments must be >= 1")
	if options.min_instruments > len(detectors.channel_dict):
		raise ValueError("--min-instruments is greater than the number of --channel-name's")

	#
	# Option checks complete
	#

	options.nxydump_segment, = segmentsUtils.from_range_strings([options.nxydump_segment], boundtype = LIGOTimeGPS)

	if options.blind_injections is not None:
		detectors.injection_filename = options.blind_injections

	# Setup local caching
	if options.local_frame_caching:
		f, fname = tempfile.mkstemp(".cache")
		if options.verbose:
			print >> sys.stderr, "caching frame data locally to ", fname
		f = open(fname, "w")
		# FIXME:  should try to down-sample if possible.  there are
		# MDCs data sets floating around whose streams do not start
		# on integer second boundaries, however, and it's possible
		# to trigger a failure in the frame muxer if those get
		# down-sampled so for now we're not doing any resampling.
		# later, when we don't care about those MDCs, we can go
		# back to down-sampling.  if not being able to down-sample
		# is a problem in the meantime, I think the best way
		# forward is to clip the start of said streams using a drop
		# element and a (to be written) buffer probe that figures
		# out how many samples to clip off the start of the stream.
		# FIXME shouldn't use tempfile.gettempdir() directly, use
		# _CONDOR_SCRATCH_DIR like glue??
		# FIXME, note that at least for now condor sets TMPDIR to the
		# run scratch space so this *will* work properly
		detectors.local_cache_list = hoftcache.cache_hoft(detectors, output_path = tempfile.gettempdir(), verbose = options.verbose)
		for cacheentry in detectors.local_cache_list:
			# Guarantee a lal cache complient file with only integer starts and durations.
			cacheentry.segment = segments.segment( int(cacheentry.segment[0]), int(math.ceil(cacheentry.segment[1])) )
			print >>f, str(cacheentry)
		detectors.frame_cache = fname

		# the injections are now present in the data so we don't want to do them twice
		detectors.injection_filename = None

	if options.zero_lag_ranking_stat_file is None and likelihood_url_namedtuples_list:
		options.zero_lag_ranking_stat_file = [None] * len(likelihood_url_namedtuples_list)

	# we're done
	return options, filenames, process_params, svd_banks, detectors, likelihood_url_namedtuples_list


#
# =============================================================================
#
#                                Signal Handler
#
# =============================================================================
#


class OneTimeSignalHandler(object):
	def __init__(self, pipeline):
		self.pipeline = pipeline
		self.count = 0

	def __call__(self, signum, frame):
		self.count += 1
		if self.count == 1:
			print >>sys.stderr, "*** SIG %d attempting graceful shutdown (this might take several minutes) ... ***" % signum
			try:
				#FIXME how do I choose a timestamp?
				self.pipeline.get_bus().post(inspiral.message_new_checkpoint(self.pipeline, timestamp=now().ns()))
				if not self.pipeline.send_event(Gst.Event.new_eos()):
					raise Exception("pipeline.send_event(EOS) returned failure")
			except Exception as e:
				print >>sys.stderr, "graceful shutdown failed: %s\naborting." % str(e)
				os._exit(1)
		else:
			print >>sys.stderr, "*** received SIG %d %d times... ***" % (signum, self.count)


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


#
# parse command line
#


options, filenames, process_params, svd_banks, detectors, likelihood_url_namedtuples_list = parse_command_line()

if not options.check_time_stamps:
	pipeparts.mkchecktimestamps = lambda pipeline, src, *args: src


#
# Parse the vetos segments file(s) if provided
#


if options.veto_segments_file is not None:
	veto_segments = ligolw_segments.segmenttable_get_by_name(ligolw_utils.load_filename(options.veto_segments_file, verbose = options.verbose, contenthandler = LIGOLWContentHandler), options.veto_segments_name).coalesce()
else:
	veto_segments = None


#
# Load the time slide vectors, and use them to define the complete
# instrument list for the network
#


offsetvectors = lsctables.TimeSlideTable.get_table(ligolw_utils.load_filename(options.time_slide_file, contenthandler = LIGOLWContentHandler, verbose = options.verbose)).as_dict().values()
all_instruments = reduce(lambda a, b: a | set(b), offsetvectors, set())
all_instruments -= set(("V1",))	# hide Virgo. FIXME:  remove after O2
if len(all_instruments) < options.min_instruments:
	raise ValueError("--time-slide-file \"%s\" names %s but we need at least %d instruments" % (options.time_slide_file, ", ".join(sorted(all_instruments)), options.min_instruments))


#
# set up the PSDs
#
# There are three modes for psds in this program
# 1) --reference-psd without --track-psd - a fixed psd (provided by the user) will be used to whiten the data
# 2) --track-psd without --reference-psd - a psd will me measured and used on the fly
# 3) --track-psd with --reference-psd - a psd will be measured on the fly, but the first guess will come from the users provided psd
#


if options.reference_psd is not None:
	psd = lal.series.read_psd_xmldoc(ligolw_utils.load_filename(options.reference_psd, verbose = options.verbose, contenthandler = lal.series.PSDContentHandler))
else:
	psd = dict((instrument, None) for instrument in detectors.channel_dict)


#
# Process svd banks in serial
#


for output_file_number, (svd_bank_url_dict, output_url, likelihood_url_namedtuple, zero_lag_ranking_stats_file, ht_gate_threshold) in enumerate(zip(svd_banks, options.output, likelihood_url_namedtuples_list, options.zero_lag_ranking_stat_file, options.ht_gate_threshold)):
	#
	# Checkpointing only supported for gzip files in offline analysis
	# FIXME Implement a means by which to check for sqlite file
	# validity
	#

	if options.data_source not in ("lvshm", "framexmit") and output_url.endswith('.gz'):
		try:
			# a single .read() would be easier but looping over
			# lines uses less memory
			for line in gzip.open(ligolw_utils.local_path_from_url(output_url)):
				pass
			for line in gzip.open(ligolw_utils.local_path_from_url(likelihood_url_namedtuple[0])):
				pass
			# File is OK and there is no need to process it,
			# skip ahead in the loop
			continue
		except IOError:
			# File does not exist or is corrupted, need to
			# reprocess
			print >>sys.stderr, "Checkpoint: {0} of {1} files completed and continuing with {2}".format(output_file_number, len(options.output), os.path.basename(output_url))
			pass

	#
	# create a new, empty, Bottle application and make it the current
	# default, then start http server(s) to serve it up
	#


	bottle.default_app.push()
	# uncomment the next line to show tracebacks when something fails
	# in the web server
	#bottle.app().catchall = False
	import base64, uuid	# FIXME:  don't import when the uniquification scheme is fixed
	httpservers = httpinterface.HTTPServers(
		# FIXME:  either switch to using avahi's native name
		# uniquification system or adopt a naturally unique naming
		# scheme (e.g., include a search identifier and job
		# number).
		service_name = "gstlal_inspiral (%s)" % base64.urlsafe_b64encode(uuid.uuid4().bytes),
		service_properties = {
			"job_tag": os.getcwd(),
			"GSTLAL_LL_JOB": os.environ.get("GSTLAL_LL_JOB", "")
		},
		verbose = options.verbose
	)


	#
	# Set up a registry of the resources that this job provides
	#


	@bottle.route("/")
	@bottle.route("/index.html")
	def index(job_tag = options.job_tag, instruments = all_instruments):
		# get the host and port to report in the links from the
		# request we've received so that the URLs contain the IP
		# address by which client has contacted us
		netloc = bottle.request.urlparts[1]
		server_address = "http://%s" % netloc
		yield "<html><body>\n<h3>%s %s %s %s</h3>\n<p>\n" % (job_tag, os.environ.get("GSTLAL_LL_JOB"), netloc, " ".join(sorted(instruments)))
		for route in sorted(bottle.default_app().routes, key = lambda route: route.rule):
			# don't create links back to this page
			if route.rule in ("/", "/index.html"):
				continue
			# only create links for GET methods
			if route.method != "GET":
				continue
			yield "<a href=\"%s%s\">%s</a><br>\n" % (server_address, route.rule, route.rule)
		yield "</p>\n</body></html>"
	# FIXME:  get service-discovery working, then don't do this
	if "GSTLAL_LL_JOB" in os.environ:
		open("%s_registry.txt" % os.environ["GSTLAL_LL_JOB"], "w").write("http://%s:%s/\n" % (socket.gethostname(), httpservers[0][0].port))


	#
	# parse SVD template bank and expose bank metadata
	#


	banks = inspiral.parse_bank_files(svd_bank_url_dict, verbose = options.verbose)

	# Choose to optionally reconstruct segments around injections (not
	# blind injections!)
	if options.injections:
		offset_padding = max([int(abs(float(offset)))+2 for bank in banks.items()[0][-1] for offset in bank.sngl_inspiral_table.get_end()]) # Add 2 to achieve rounding up and adding 1
		reconstruction_segment_list = simulation.sim_inspiral_to_segment_list(options.injections, pad = offset_padding)
	else:
		reconstruction_segment_list = None


	@bottle.route("/bank.txt")
	def get_filter_length_and_chirpmass(banks = banks):
		bank = banks.values()[0][0] #FIXME maybe shouldn't just take the first ones
		yield '%.14g %.4g %.4g' % (float(now()), bank.filter_length, bank.sngl_inspiral_table[0].mchirp)


	#
	# Build pipeline
	#


	if options.verbose:
		print >>sys.stderr, "assembling pipeline ...",

	pipeline = Gst.Pipeline(name="gstlal_inspiral")
	mainloop = GObject.MainLoop()
		
	triggersrc = lloidparts.mkLLOIDmulti(
		pipeline,
		detectors = detectors,
		banks = banks,
		psd = psd,
		psd_fft_length = options.psd_fft_length,
		ht_gate_threshold = ht_gate_threshold,
		veto_segments = veto_segments,
		verbose = options.verbose,
		nxydump_segment = options.nxydump_segment,
		chisq_type = options.chisq_type,
		track_psd = options.track_psd,
		control_peak_time = options.control_peak_time,
		fir_stride = options.fir_stride,
		reconstruction_segment_list = reconstruction_segment_list
	)
	

	if options.verbose:
		print >>sys.stderr, "done"


	#
	# Load likelihood ratio data, assume injections are present!
	#


	if options.data_source in ("lvshm", "framexmit"):
		filename = likelihood_url_namedtuple[0] if likelihood_url_namedtuple[0] is not None else likelihood_url_namedtuple[1]
		coinc_params_distributions, _, seglists = far.parse_likelihood_control_doc(ligolw_utils.load_url(filename, verbose = options.verbose, contenthandler = far.ThincaCoincParamsDistributions.LIGOLWContentHandler))
		if set(seglists) != set(detectors.channel_dict):
			raise ValueError("segment lists in \"%s\" are for instruments %s but we need %s" % (filename, ", ".join(sorted(seglists)), ", ".join(sorted(detectors.channel_dict))))
		if coinc_params_distributions is None:
			raise ValueError("\"%s\" does not contain parameter distribution data" % filename)
		if coinc_params_distributions.delta_t != options.coincidence_threshold:
			raise ValueError("\"%s\" is for delta_t=%g, we need %g" % (filename, coinc_params_distributions.delta_t, options.coincidence_threshold))
		if coinc_params_distributions.min_instruments != options.min_instruments:
			raise ValueError("\"%s\" is for min instruments = %d but we need %d" % (filename, coinc_params_distributions.min_instruments, options.min_instruments))
		if coinc_params_distributions.instruments != all_instruments:
			raise ValueError("\"%s\" is for %s but we need %s" % (filename, ", ".join(sorted(coinc_params_distributions.instruments)), ", ".join(sorted(all_instruments))))
	else:
		coinc_params_distributions, seglists = far.ThincaCoincParamsDistributions(instruments = all_instruments, delta_t = options.coincidence_threshold, min_instruments = options.min_instruments), segments.segmentlistdict((instrument, segments.segmentlist()) for instrument in detectors.channel_dict)
		if "V1" in seglists: del seglists["V1"]	# drop Virgo FIXME:  remove after O2

	if zero_lag_ranking_stats_file is None:
		zero_lag_ranking_stats = None
	else:
		_, zero_lag_ranking_stats, _ = far.parse_likelihood_control_doc(ligolw_utils.load_filename(zero_lag_ranking_stats_file, verbose = options.verbose, contenthandler = far.ThincaCoincParamsDistributions.LIGOLWContentHandler))
		if zero_lag_ranking_stats is None:
			raise ValueError("\"%s\" does not contain ranking statistic PDF data" % zero_lag_ranking_stat_file)


	#
	# build output document
	#


	if options.verbose:
		print >>sys.stderr, "initializing output document ..."
	output = inspiral.Data(
		url = output_url or "%s-%s_LLOID-%d-%d.xml.gz" % (lsctables.instrumentsproperty.set(detectors.channel_dict.keys()).replace(",", ""), options.job_tag, int(detectors.seg[0]), int(abs(detectors.seg))),
		process_params = process_params,
		pipeline = pipeline,
		seg = detectors.seg or segments.segment(LIGOTimeGPS(0), LIGOTimeGPS(2000000000)), # online data doesn't have a segment so make it all possible time
		coinc_params_distributions = coinc_params_distributions,
		zero_lag_ranking_stats = zero_lag_ranking_stats,
		marginalized_likelihood_file = options.marginalized_likelihood_file,
		likelihood_url_namedtuple = likelihood_url_namedtuple,
		injection_filename = options.injections,
		offsetvectors = offsetvectors,
		comment = options.comment,
		tmp_path = options.tmp_space,
		likelihood_snapshot_interval = options.likelihood_snapshot_interval,	# seconds
		thinca_interval = options.thinca_interval,
		min_log_L = options.min_log_L,
		sngls_snr_threshold = options.singles_threshold,
		gracedb_far_threshold = options.gracedb_far_threshold,
		gracedb_min_instruments = 2,
		gracedb_group = options.gracedb_group,
		gracedb_search = options.gracedb_search,
		gracedb_pipeline = options.gracedb_pipeline,
		gracedb_service_url = options.gracedb_service_url,
		upload_auxiliary_data_to_gracedb = (options.gracedb_service_url == "https://gracedb.ligo.org/api/"),
		verbose = options.verbose
	)
	if options.verbose:
		print >>sys.stderr, "... output document initialized"

	handler = lloidparts.Handler(mainloop, pipeline,
		output,
		instruments = coinc_params_distributions.instruments,
		tag = options.job_tag,
		seglistdict = seglists,
		zero_lag_ranking_stats_filename = zero_lag_ranking_stats_file,
		verbose = options.verbose
	)

	if options.verbose:
		print >>sys.stderr, "attaching appsinks to pipeline ...",
	appsync = pipeparts.AppSync(appsink_new_buffer = output.appsink_new_buffer)
	appsinks = set(appsync.add_sink(pipeline, src, caps = Gst.Caps.from_string("application/x-lal-snglinspiral"), name = "%s_sink_%d" % (instrument, n)) for instrument, srcs in triggersrc.items() for n, src in enumerate(srcs))
	if options.verbose:
		print >>sys.stderr, "attached %d, done" % len(appsinks)


	#
	# if we request a dot graph of the pipeline, set it up
	#


	if options.write_pipeline is not None:
		pipeparts.connect_appsink_dump_dot(pipeline, appsinks, options.write_pipeline, options.verbose)
		pipeparts.write_dump_dot(pipeline, "%s.%s" % (options.write_pipeline, "NULL"), verbose = options.verbose)

	#
	# put a 0 into all horizon histories at the boundaries of the
	# analysis segment so that queries for horizon distance at times
	# outside the segment and for horizon distance for instruments for
	# which there is no data report 0.  NOTE:  if this job is
	# contiguous with another job then this makes it seem like there is
	# a 0 in the horizon distance history at the boundary between them
	# when that is probably not correct.  because it's a single sample
	# it shouldn't lead to significant problems, but this is something
	# to be aware of.
	#

	if detectors.seg:
		for instrument in coinc_params_distributions.instruments:
			handler.record_horizon_distance(instrument, float(detectors.seg[0]), 0.0)
			handler.record_horizon_distance(instrument, float(detectors.seg[1]), 0.0)


	#
	# Run pipeline
	#


	if options.data_source in ("lvshm", "framexmit"):
		#
		# setup sigint handler to shutdown pipeline.  this is how
		# the program stops gracefully, it is the only way to stop
		# it.  Otherwise it runs forever man.
		#
		# this is only done in the online case because we want
		# offline jobs to just die and not write partial databases
		#

		signal.signal(signal.SIGINT, OneTimeSignalHandler(pipeline))
		signal.signal(signal.SIGTERM, OneTimeSignalHandler(pipeline))


	if options.verbose:
		print >>sys.stderr, "setting pipeline state to ready ..."
	if pipeline.set_state(Gst.State.READY) != Gst.StateChangeReturn.SUCCESS:
		raise RuntimeError("pipeline did not enter ready state")
	datasource.pipeline_seek_for_gps(pipeline, *detectors.seg)
	if options.verbose:
		print >>sys.stderr, "setting pipeline state to playing ..."
	if pipeline.set_state(Gst.State.PLAYING) != Gst.StateChangeReturn.SUCCESS:
		raise RuntimeError("pipeline did not enter playing state")

	if options.write_pipeline is not None:
		pipeparts.write_dump_dot(pipeline, "%s.%s" % (options.write_pipeline, "PLAYING"), verbose = options.verbose)

	if options.verbose:
		print >>sys.stderr, "running pipeline ..."
	mainloop.run()
	

	#
	# write output file
	#


	output.write_output_url(url = output_url or output.T050017_filename("%s_LLOID" % options.job_tag, "xml.gz"), description = "%s_LLOID" % options.job_tag, verbose = options.verbose)


	#
	# Cleanup template bank temp files
	#


	for ifo in banks:
		for bank in banks[ifo]:
			if options.verbose:
				print >> sys.stderr, "removing file: ", bank.template_bank_filename
			os.remove(bank.template_bank_filename)


	#
	# Shutdown the web interface servers and garbage collect the Bottle
	# app.  This should release the references the Bottle app's routes
	# hold to the pipeline's data (like template banks and so on).
	#


	del httpservers
	bottle.default_app.pop()


	#
	# Set pipeline state to NULL and garbage collect the handler
	#


	if pipeline.set_state(Gst.State.NULL) != Gst.StateChangeReturn.SUCCESS:
		raise RuntimeError("pipeline could not be set to NULL")
	del handler.pipeline
	del output.pipeline
	del handler
	del bank
	del banks


#
# Cleanup local frame file cache
#


if options.local_frame_caching:
	if options.verbose:
		print >>sys.stderr, "deleting temporary cache file ", detectors.frame_cache
	os.remove(detectors.frame_cache)
	del detectors.local_cache_list


#
# done.  online pipeline always ends with an error code so that dagman does
# not mark the job "done" and the job will be restarted when the dag is
# restarted.
#


if options.data_source in ("lvshm", "framexmit"):
	sys.exit(1)
