#!/usr/bin/env python
#
# Copyright (C) 2016  Kipp Cannon, Chad Hanna
# Copyright (C) 2019  Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


import argparse
import json
import logging
from multiprocessing import Pool
import sys, os
import time
import timeit
import StringIO
import numpy
import threading 
from gstlal import bottle
import socket
import base64
import uuid
import shutil

#from ligo.scald import io
from gstlal import far
from gstlal import httpinterface
from gstlal import servicediscovery
from ligo import lw
from ligo.lw import utils as ligolw_utils
from ligo.lw.utils import process as ligolw_process


def service_domain(gracedb_search, gracedb_pipeline):
	return "%s_%s.%s" % (gracedb_pipeline.lower(), gracedb_search.lower(), servicediscovery.DEFAULT_SERVICE_DOMAIN)

#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#

class ZeroLagCounts(object):
	"""
	A Class to keep track of zero lag counts clustered over 10s fixed windows
	"""

	def __init__(self, fname, verbose = False):
		"""
		fname is the name of the zerolag file read from disk to start the counting
		"""
		self.lock = threading.Lock()
		self.fname = fname
		_, self.zerolag_rankingstatpdf = far.parse_likelihood_control_doc(ligolw_utils.load_url(fname, verbose = verbose, contenthandler = far.RankingStat.LIGOLWContentHandler))
		self.counts_dict = {}

	def add_coincs(self, coincs):
		"""
		Iterate over a list of coincs to extract their gps time and
		likelihood ratio value.  GPS times are rounded to the nearest 10s in order to
		cluster events. The max likelihood is tracked in the dictionary over the 10s
		window. Nothing is added to the zerolag histograms. The counts are purely
		internal. Only when a new zerolag histogram is requested will the zerolag
		histogram be updated. That makes it easier to manage late buffers.  No
		additional logic required.  The number of coincs added is returned for 
		logging purposes
		"""
		cnt = 0
		for c in coincs:
			# avoid trigs before LR assigned
			if "likelihood" in c:
				# equiv to 10s clustering
				key = round(c["end"], -1)
				lr = c["likelihood"]
				self.counts_dict[key] = max(self.counts_dict.setdefault(key, lr), lr)
				cnt += 1
		return cnt
		
	def __get_zerolag_rankingstatpdf_xmldoc(self, zlpdf):
		"""
		generate an in memory XML document
		"""
		xmldoc = lw.ligolw.Document()
		xmldoc.appendChild(lw.ligolw.LIGO_LW())
		process = ligolw_process.register_to_xmldoc(xmldoc, u"gstlal_inspiral", paramdict = {})
		far.gen_likelihood_control_doc(xmldoc, None, zlpdf)
		ligolw_process.set_process_end_time(process)
		return xmldoc

	def __add_counts_to_zerolag_pdf(self, zlpdf):
		"""
		Add the internal counts to a zerolag histogram. NOTE!!!! This
		should never be called on the internal zerolag histogram, only on a copy. That
		is why this method is not invoked unless someone is requesting an updated file
		"""
		for lr in self.counts_dict.values():
			zlpdf.zero_lag_lr_lnpdf.count[lr,] += 1

	def web_get_zerolag_rankingstatpdf(self):
		"""
		offer up a new XML doc with the internal counts added to the
		counts that this program was seeded with.  Suitable as a bottle route.
		"""
		with self.lock:
			# always make a copy of the *original* pdf that was
			# loaded at start, never modify it. Just update a copy.
			# Avoids annoying logic for late buffers.  The idea is
			# that you always get the most up to date.
			zlpdf = self.zerolag_rankingstatpdf.copy()
			self.__add_counts_to_zerolag_pdf(zlpdf)
			output = StringIO.StringIO()
			ligolw_utils.write_fileobj(self.__get_zerolag_rankingstatpdf_xmldoc(zlpdf), output)
			outstr = output.getvalue()
			output.close()
			return outstr

	def snapshot_output_url(self, fname, verbose = False):
		"""
		Write a new XML doc with the internal counts added to disk with fname
		"""
		with self.lock:
			zlpdf = self.zerolag_rankingstatpdf.copy()
			self.__add_counts_to_zerolag_pdf(zlpdf)
			ligolw_utils.write_url(self.__get_zerolag_rankingstatpdf_xmldoc(zlpdf), fname, gz = fname.endswith(".gz"), verbose = verbose, trap_signals = None)



def retrieve_triggers(consumer, jobs, route_name = 'coinc', timeout = 1000, max_records = 1000):
	"""!
	A function to pull triggers from kafka for a set of jobs (topics) and
	route_name (key in the incoming json messages)
	"""
	triggers = []
	
	### retrieve timeseries for all routes and topics
	msg_pack = consumer.poll(timeout_ms = timeout, max_records = max_records)
	for tp, messages in msg_pack.items():
		for message in messages:
			try:
				triggers.extend(message.value)
			except KeyError: ### no route in message
				pass
	
	return triggers

# Read command line options
def parse_command_line():

	parser = argparse.ArgumentParser(description="Online trigger counter")

	# directory to put everything in
	parser.add_argument("--job-start", type=int, help="job id to start aggregating from")
	parser.add_argument("--route", action="store", default="coinc", help="Specify the route where triggers are stored in.")
	parser.add_argument("--output-period", type = float, default = 3600., help = "Wait this many seconds between writing the output file (default = 3600)")
	parser.add_argument("--num-jobs", action="store", type=int, default=10, help="number of running jobs")
	parser.add_argument("--job-tag", help = "Collect URLs for jobs reporting this job tag (default = collect all gstlal_inspiral URLs).")
	parser.add_argument("--num-threads", type = int, default = 16, help = "Number of threads to use concurrently, default 16.")
	parser.add_argument("--kafka-server", action="store", help="Specify kakfa server to read data from, example: 10.14.0.112:9092")
	parser.add_argument("--output", default="zerolag_rankingstatpdf.xml.gz", help = "Choose the output file. Default zerolag_rankingstatpdf.xml.gz")
	parser.add_argument("--gracedb-pipeline", default = "gstlal", help = "Name of pipeline to provide in GracedB uploads (default is gstlal).")
	parser.add_argument("--gracedb-search", default = "LowMass", help = "Name of search to provide in GracedB uploads (default is LowMass).")
	parser.add_argument("-v","--verbose", default=False, action="store_true", help = "Print to stdout in addition to writing to automatically generated log.")

	args = parser.parse_args()

	return args


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#

if __name__ == '__main__':

	time_since_last = time.time()
	options = parse_command_line()
	ZLC = ZeroLagCounts(options.output)

	#
	# Stuff to handle bottle route to retrieve xml doc with new counts
	#

	bottle.default_app.push()
	bottle.route('/zerolag_rankingstatpdf.xml')(ZLC.web_get_zerolag_rankingstatpdf)
	httpservers = httpinterface.HTTPServers(
		service_name = "%s.gstlal_ll_inspiral_trigger_counter" % (base64.urlsafe_b64encode(uuid.uuid4().bytes)),
		service_domain = service_domain(options.gracedb_search, options.gracedb_pipeline),
		service_properties = {
			"cwd": os.getcwd(),
			"pid": str(os.getpid()),
		},
		service_discovery = False,#FIXME
		verbose = options.verbose
	)
	open("gstlal_ll_inspiral_trigger_counter_registry.txt", "w").write("http://%s:%s/\n" % (socket.gethostname(), httpservers[0][0].port))


	# FIXME don't hardcode some of these?
	jobs = ["%04d" % b for b in numpy.arange(options.job_start, options.job_start + options.num_jobs)]

	log_level = logging.DEBUG if options.verbose else logging.INFO
	logging.basicConfig(format = '%(asctime)s | gstlal_ll_inspiral_trigger_counter : %(levelname)s : %(message)s')
	logging.getLogger().setLevel(log_level)

	pool = Pool(options.num_threads)

	# We instantiate multiple consumers (based on --num-threads) to subscribe to all of our topics, i.e., jobs
	if options.kafka_server:
		from kafka import KafkaConsumer
		consumer = KafkaConsumer(
			options.route,
			bootstrap_servers=[options.kafka_server],
			key_deserializer=lambda m: json.loads(m.decode('utf-8')),
			value_deserializer=lambda m: json.loads(m.decode('utf-8')),
			group_id='%s_trigger_counter' % jobs[0],
			auto_offset_reset='latest',
			max_poll_interval_ms = 60000,
			session_timeout_ms=30000,
			heartbeat_interval_ms=10000,
			reconnect_backoff_ms=5000,
			reconnect_backoff_max_ms=30000
		)
	else:
		consumer = None

	# start an infinite loop to keep updating and aggregating data
	logging.info("starting up...")
	while True:

		if consumer:
			# this is not threadsafe!
			logging.debug("retrieving data from kafka")
			start = timeit.default_timer()
			triggers = retrieve_triggers(consumer, jobs, route_name = options.route, max_records = 2 * len(jobs))
			elapsed = timeit.default_timer() - start
			logging.debug("time to retrieve data: %.1f s" % elapsed)
		else:
			logging.debug("retrieving data from bottle routes")
			triggers = io.http.retrieve_triggers(options.base_dir, jobs, options.job_tag, route_name = options.route, num_threads=options.num_threads)


		# store and reduce data for each job
		if triggers:
			start = timeit.default_timer()
			logging.debug("adding triggers")
		
			num = ZLC.add_coincs(triggers)

			elapsed = timeit.default_timer() - start
			logging.debug("time to store/reduce %d triggers: %.1f s" % (num, elapsed))
		else:
			logging.debug("no triggers to process")

		if (time.time() - time_since_last) > options.output_period:
			shutil.move(options.output, options.output + ".bk")
			ZLC.snapshot_output_url(options.output, verbose = True)
			time_since_last = time.time()

	# close connection to consumer if using kafka
	if consumer:
		logging.info("shutting down consumer...")
		consumer.close()

	#
	# always end on an error so that condor won't think we're done and will
	# restart us
	#

	sys.exit(1)
