#!/usr/bin/env python3
#
# Copyright (C) 2016  Kipp Cannon, Chad Hanna
# Copyright (C) 2019  Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


import argparse
import json
import logging
import sys, os
import time
import timeit

import numpy

from ligo.scald import aggregator
from ligo.scald import io


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


# Read command line options
def parse_command_line():

	parser = argparse.ArgumentParser(description="Online data aggregator")

	# directory to put everything in
	parser.add_argument("--base-dir", action="store", default="aggregator", help="Specify output path")
	parser.add_argument("--job-start", type=int, help="job id to start aggregating from")
	parser.add_argument("--route", action="append", help="Specify routes to download. Can be given multiple times.")
	parser.add_argument("--data-type", action="append", help="Specify datatypes to aggregate from 'min', 'max', 'median'. Can be given multiple times. Default all")
	parser.add_argument("--dump-period", type = float, default = 1., help = "Wait this many seconds between dumps of the  URLs (default = 1., set to 0 to disable)")
	parser.add_argument("--num-jobs", action="store", type=int, default=10, help="number of running jobs")
	parser.add_argument("--job-tag", help = "Collect URLs for jobs reporting this job tag (default = collect all gstlal_inspiral URLs).")
	parser.add_argument("--num-threads", type = int, default = 16, help = "Number of threads to use concurrently, default 16.")
	parser.add_argument("--kafka-server", action="store", help="Specify kakfa server to read data from, example: 10.14.0.112:9092")
	parser.add_argument("--data-backend", default="hdf5", help = "Choose the backend for data to be stored into, options: [hdf5|influx]. default = hdf5.")
	parser.add_argument("--influx-hostname", help = "Specify the hostname for the influxDB database. Required if --data-backend = influx.")
	parser.add_argument("--influx-port", help = "Specify the port for the influxDB database. Required if --data-backend = influx.")
	parser.add_argument("--influx-database-name", help = "Specify the database name for the influxDB database. Required if --data-backend = influx.")
	parser.add_argument("--enable-auth", action = "store_true", help = "If set, enables authentication for the influx aggregator.")
	parser.add_argument("--enable-https", action = "store_true", help = "If set, enables HTTPS connections for the influx aggregator.")
	parser.add_argument("--across-jobs", action = "store_true", help = "If set, aggregate data across jobs as well.")
	parser.add_argument("-v","--verbose", default=False, action="store_true", help = "Print to stdout in addition to writing to automatically generated log.")

	args = parser.parse_args()

	#FIXME do error checking
	if args.data_type is None:
		args.data_type = ["min", "max", "median"]

	assert args.data_backend in ('hdf5', 'influx'), '--data-backend must be one of [hdf5|influx]'

	return args


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#

if __name__ == '__main__':
	options = parse_command_line()

	# FIXME don't hardcode some of these?
	jobs = ["%04d" % b for b in numpy.arange(options.job_start, options.job_start + options.num_jobs)]
	routes = options.route

	log_level = logging.DEBUG if options.verbose else logging.INFO
	logging.basicConfig(format = '%(asctime)s | ll_inspiral_aggregator : %(levelname)s : %(message)s')
	logging.getLogger().setLevel(log_level)

	# We instantiate multiple consumers (based on --num-threads) to subscribe to all of our topics, i.e., jobs
	if options.kafka_server:
		from kafka import KafkaConsumer
		consumer = KafkaConsumer(
			*routes,
			bootstrap_servers=[options.kafka_server],
			key_deserializer=lambda m: json.loads(m.decode('utf-8')),
			value_deserializer=lambda m: json.loads(m.decode('utf-8')),
			group_id='%s_%s_aggregator' % (routes[0], options.data_type[0]),
			auto_offset_reset='latest',
			max_poll_interval_ms = 60000,
			session_timeout_ms=30000,
			heartbeat_interval_ms=10000,
			reconnect_backoff_ms=5000,
			reconnect_backoff_max_ms=30000
		)
	else:
		consumer = None

	# set up aggregator sink
	logging.info("setting up aggregator...")
	if options.data_backend == 'influx':
		agg_sink = io.influx.Aggregator(
			hostname=options.influx_hostname,
			port=options.influx_port,
			db=options.influx_database_name,
			auth=options.enable_auth,
			https=options.enable_https,
			reduce_across_tags=options.across_jobs
		)
	else: ### hdf5 data backend
		agg_sink = io.hdf5.Aggregator(rootdir=options.base_dir, num_processes=options.num_threads)

	# register measurement schemas for aggregators
	for route in routes:
		agg_sink.register_schema(route, columns='data', column_key='data', tags='job', tag_key='job')

	# start an infinite loop to keep updating and aggregating data
	logging.info("starting up...")
	while True:
		logging.debug("sleeping for %.1f s" % options.dump_period)
		time.sleep(options.dump_period)

		if consumer:
			# this is not threadsafe!
			logging.debug("retrieving data from kafka")
			start = timeit.default_timer()
			datadata = io.kafka.retrieve_timeseries(consumer, routes, max_records = 2 * len(jobs) * len(routes))
			elapsed = timeit.default_timer() - start
			logging.debug("time to retrieve data: %.1f s" % elapsed)
		else:
			logging.debug("retrieving data from bottle routes")
			datadata = io.http.retrieve_timeseries(options.base_dir, jobs, routes, options.job_tag, num_threads=options.num_threads)

		# store and reduce data for each job
		start = timeit.default_timer()
		for route in routes:
			logging.debug("storing and reducing timeseries for measurement: %s" % route)
			for aggregate in options.data_type:
				agg_sink.store_columns(route, datadata[route], aggregate=aggregate)
		elapsed = timeit.default_timer() - start
		logging.debug("time to store/reduce timeseries: %.1f s" % elapsed)

	# close connection to consumer if using kafka
	if consumer:
		logging.info("shutting down consumer...")
		consumer.close()

	#
	# always end on an error so that condor won't think we're done and will
	# restart us
	#

	sys.exit(1)
