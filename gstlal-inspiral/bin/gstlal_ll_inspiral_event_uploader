#!/usr/bin/env python

# Copyright (C) 2019  Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

__usage__ = "gstlal_ll_inspiral_event_uploader [--options]"
__description__ = "an executable to aggregate and upload GraceDB events from gstlal_inspiral jobs"
__author__ = "Patrick Godwin (patrick.godwin@ligo.org)"

#-------------------------------------------------
#				   Preamble
#-------------------------------------------------

from collections import deque, OrderedDict
import httplib
from itertools import chain
import json
import logging
from optparse import OptionParser
import signal
import sys
import time
import timeit

from confluent_kafka import Producer, Consumer, KafkaError
import numpy

from ligo.segments import segment
from ligo.gracedb.rest import GraceDb, HTTPError
from ligo.gracedb.rest import DEFAULT_SERVICE_URL as DEFAULT_GRACEDB_URL
from ligo.scald import utils

from lal import LIGOTimeGPS

from gstlal import inspiral

#-------------------------------------------------
#				   Functions
#-------------------------------------------------

def parse_command_line():

	parser = OptionParser(usage=__usage__, description=__description__)
	parser.add_option("-v", "--verbose", default=False, action="store_true", help = "Be verbose.")
	parser.add_option("--rootdir", metavar = "path", default = ".", help = "Sets the root directory where logs and metadata are stored.")
	parser.add_option("--num-jobs", type = int, default = 10, help="number of jobs to listen to")
	parser.add_option("--tag", metavar = "string", default = "test", help = "Sets the name of the tag used. Default = 'test'")
	parser.add_option("--max-event-time", type = "int", default = 7200, help = "Maximum time to keep around an event. Default = 2 hours.")
	parser.add_option("--processing-cadence", type = "float", default = 0.1, help = "Rate at which the event uploader acquires and processes data. Default = 0.1 seconds.")
	parser.add_option("--request-timeout", type = "float", default = 0.2, help = "Timeout for requesting messages from a topic. Default = 0.2 seconds.")
	parser.add_option("--kafka-server", metavar = "string", help = "Sets the server url that the kafka topic is hosted on. Required.")
	parser.add_option("--input-topic", metavar = "string", help = "Sets the input kafka topic. Required.")
	parser.add_option("--gracedb-group", metavar = "name", default = "Test", help = "Gracedb group to which to upload events (default is Test).")
	parser.add_option("--gracedb-pipeline", metavar = "name", default = "gstlal", help = "Name of pipeline to provide in GracedB uploads (default is gstlal).")
	parser.add_option("--gracedb-search", metavar = "name", default = "LowMass", help = "Name of search to provide in GracedB uploads (default is LowMass).")
	parser.add_option("--gracedb-service-url", metavar = "url", default = DEFAULT_GRACEDB_URL, help = "Override default GracedB service url (optional, default is {}).".format(DEFAULT_GRACEDB_URL))

	options, args = parser.parse_args()

	return options, args

#-------------------------------------------------
#					Classes
#-------------------------------------------------

class EventUploader(object):
	"""
	manages handling of incoming events, selecting the best and uploading to GraceDB.
	"""
	def __init__(self, options):
		logging.info('setting up...')

		### initialize timing options
		self.processing_cadence = options.processing_cadence
		self.request_timeout = options.request_timeout
		self.max_event_time = options.max_event_time
		self.retries = 5
		self.retry_delay = 1
		self.is_running = False

		### kafka settings
		self.kafka_settings = {'bootstrap.servers': options.kafka_server}

		### initialize consumer
		consumer_kafka_settings = self.kafka_settings
		consumer_kafka_settings['group.id'] = '-'.join(['uploader', options.tag])
		self.consumer = Consumer(consumer_kafka_settings)
		self.consumer.subscribe([options.input_topic])
		self.num_jobs = options.num_jobs

		### initialize gracedb client
		if options.gracedb_service_url.startswith("file"):
			self.client = inspiral.FakeGracedbClient(options.gracedb_service_url)
		else:
			self.client = GraceDb(options.gracedb_service_url)

		### gracedb settings
		self.gracedb_group = options.gracedb_group
		self.gracedb_pipeline = options.gracedb_pipeline
		self.gracedb_search = options.gracedb_search

		### initialize event store
		self.events = OrderedDict()



	def fetch_data(self):
		"""
		requests for a new event from a topic,
		and add to candidate list
		"""
		messages = self.consumer.consume(num_messages=self.num_jobs, timeout=self.request_timeout)

		for message in messages:
			### only add to queue if no errors in receiving data
			if message and not message.error():

				### process candidate event
				candidate = json.loads(message.value())
				candidate['time'] = LIGOTimeGPS(candidate['time'], candidate.pop('time_ns'))
				self.process_candidate(candidate)


	def process_candidate(self, candidate):
		"""
		handles the processing of a candidate, creating
		a new event if necessary
		"""
		key = self.event_window(candidate['time'])
		if key in self.events:
			logging.info('adding new candidate for event: [{:.1f}, {:.1f}]'.format(*key))
			self.events[key]['candidates'].append(candidate)
		else:
			new_event = True
			for seg, event in self.events.items():
				if segment(candidate['time'], candidate['time']) in seg:
					logging.info('adding new candidate for time window: [{:.1f}, {:.1f}]'.format(*seg))
					event['candidates'].append(candidate)
					new_event = False

			### event not found, create a new event
			if new_event:
				logging.info('found new event for time window: [{:.1f}, {:.1f}]'.format(*key))
				self.events[key] = self.new_event()
				self.events[key]['candidates'].append(candidate)


	def event_window(self, t):
		"""
		returns the event window representing the event
		"""
		dt = 0.2
		return segment(utils.floor_div(t - dt, 0.5), utils.floor_div(t + dt, 0.5) + 0.5)


	def new_event(self):
		"""
		returns the structure that defines an event
		"""
		return {
			'num_sent': 0,
			'time_sent': None,
			'preferred': None,
			'candidates': deque(maxlen = self.num_jobs)
		}


	def process_events(self):
		"""
		process events stored, selecting the best candidate.
		upload if a new preferred event is found
		"""
		for key, event in sorted(self.events.items(), reverse=True):
			if event['num_sent'] == 0:
				updated, event = self.process_candidates(event)
				assert updated
				logging.info(
					'uploading {} event with far = {:.3E} for time window: '
					'[{:.1f}, {:.1f}]'.format(
						self.to_ordinal(1),
						event['preferred']['far'],
						key[0], key[1],
					)
				)
				self.upload_event(event)

			elif event['candidates'] and utils.gps_now() >= self.next_event_upload(event):
				updated, event = self.process_candidates(event)
				if updated:
					logging.info(
						'uploading {} event with far = {:.3E} for time window: '
						'[{:.1f}, {:.1f}]'.format(
							self.to_ordinal(event['num_sent'] + 1),
							event['preferred']['far'],
							key[0], key[1]
						)
					)
					self.upload_event(event)

		# clean out old events
		current_time = utils.gps_now()
		for key in list(self.events.keys()):
			if current_time - key[0] >= self.max_event_time:
				logging.info('removing stale event [{:.1f}, {:.1f}]'.format(*key))
				self.events.pop(key)


	def process_candidates(self, event):
		"""
		process candidates and update the preferred event
		if needed

		returns event and whether the preferred event was updated
		"""
		preferred = self.select_best(event['candidates'])
		event['candidates'].clear()

		### no preferred event yet
		if not event['preferred']:
			logging.info('found event with far: {:.3E}'.format(preferred['far']))
			event['preferred'] = preferred
			return True, event

		### preferred event is more significant
		if preferred['far'] < event['preferred']['far']:
			logging.info(
				'found new preferred event with far: '
				'{:.3E}, previous far: {:.3E}'.format(preferred['far'], event['preferred']['far'])
			)
			event['preferred'] = preferred
			return True, event

		### preferred far is better:
		### update far in coinc, far in event, keep rest
		### FIXME: turned off for now
		#elif preferred['far'] < event['preferred']['far']:
		#	event['preferred']['far'] = preferred['far']
		#	self.update_coinc_far(event['preferred']['coinc'], preferred['far'])
		#	return True, event

		### preferred snr is better:
		### update coinc except far
		### FIXME: turned off for now
		#elif preferred['snr'] > event['preferred']['snr']:
		#	far = event['preferred']['far']
		#	event['preferred']['coinc'] = preferred['coinc']
		#	self.update_coinc_far(event['preferred']['coinc'], far)
		#	return True, event

		### previous preferred is better
		else:
			return False, event


	def select_best(self, candidates):
		"""
		select the best event out of the candidates
		"""
		candidates = list(candidates)
		far_idx = numpy.argmin([c['far'] for c in candidates])
		best = candidates[far_idx]
		### FIXME: selecting best snr independently turned off for now
		#min_far = min(c['far'] for c in candidates)
		#snr_idx = numpy.argmax([c['snr'] for c in candidates])
		#best = candidates[snr_idx]
		#best['far'] = min_far
		return best


	def upload_event(self, event):
		"""
		upload a new event + auxiliary files
		"""
		event['num_sent'] += 1
		event['time_sent'] = utils.gps_now()
		for attempt in range(1, self.retries + 1):
			try:
				resp = self.client.createEvent(
					self.gracedb_group,
					self.gracedb_pipeline,
					'coinc.xml',
					filecontents = event['preferred']['coinc'],
					search = self.gracedb_search
				)
			except HTTPError as resp:
				logging.warning(resp)
			else:
				resp_json = resp.json()
				if resp.status == httplib.CREATED:
					graceid = resp_json['graceid']
					logging.info("event assigned grace ID {}".format(graceid))
					self.upload_file(
						"GstLAL internally computed p-astro",
						"p_astro.json",
						"p_astro",
						event['preferred']['p_astro'],
						graceid
					)
					try:
						resp = self.client.writeLabel(graceid, 'PASTRO_READY')
					except HTTPError as resp:
						logging.warning(resp)
					break
			logging.warning(
				"gracedb upload of {} "
				"failed on attempt {:d}/{:d}".format(filename, attempt, self.retries)
			)
			time.sleep(random.lognormal(math.log(self.retry_delay), .5))
		else:
			logging.warning("gracedb upload of {} failed".format(filename))
		self.upload_file("strain PSDs", "psd.xml", "psd", event['preferred']['psd'], graceid)


	def upload_file(self, message, filename, tag, contents, graceid):
		"""
		upload a file to gracedb
		"""
		logging.info("posting '{}' to gracedb ID {}".format(filename, graceid))
		for attempt in range(1, self.retries + 1):
			try:
				resp = self.client.writeLog(
					graceid,
					message,
					filename = filename,
					filecontents = contents,
					tagname = tag
				)
			except HTTPError as resp:
				logging.warning(resp)
			else:
				if resp.status == httplib.CREATED:
					break
			logging.info(
				"gracedb upload of {} for ID {} "
				"failed on attempt {:d}/{:d}".format(filename, graceid, attempt, self.retries)
			)
			time.sleep(random.lognormal(math.log(self.retry_delay), .5))
		else:
			logging.warning("gracedb upload of {} for ID {} failed".format(filename, graceid))


	def next_event_upload(self, event):
		"""
		check whether enough time has elapsed to send an updated event
		"""
		return event['time_sent'] + numpy.power(4, event['num_sent'])


	def update_coinc_far(self, coinc, far):
		"""
		update the far in the coinc.xml file
		"""
		### FIXME: actually update the coinc
		return coinc


	def process(self):
		"""
		fetch events and process them at the specified cadence
		"""
		while self.is_running:
			self.fetch_data()
			self.process_events()
			time.sleep(self.processing_cadence)


	def start(self):
		"""
		start the event loop
		"""
		logging.info('starting event uploader for {:d} inspiral jobs...'.format(self.num_jobs))
		self.is_running = True
		self.process()


	def stop(self):
		"""
		stop the event loop
		"""
		logging.info('shutting down...')
		### FIXME: should also handle pushing rest of data in buffer
		self.is_running = False


	@staticmethod
	def to_ordinal(n):
		"""
		given an integer, returns the ordinal number
		representation.

		this black magic is taken from
		https://stackoverflow.com/a/20007730
		"""
		return "%d%s" % (n,"tsnrhtdd"[(n/10%10!=1)*(n%10<4)*n%10::4])


class SignalHandler(object):
	"""
	helper class to shut down the event uploader gracefully before exiting
	"""
	def __init__(self, event_uploader, signals = [signal.SIGINT, signal.SIGTERM]):
		self.event_uploader = event_uploader
		for sig in signals:
			signal.signal(sig, self)

	def __call__(self, signum, frame):
		logging.info("SIG {:d} received, attempting graceful shutdown...".format(signum))
		self.event_uploader.stop()
		sys.exit(0)


#-------------------------------------------------
#					 Main
#-------------------------------------------------

if __name__ == '__main__':
	# parse arguments
	options, args = parse_command_line()

	# set up logging
	log_level = logging.DEBUG if options.verbose else logging.INFO
	logging.basicConfig(format = '%(asctime)s | event_uploader : %(levelname)s : %(message)s')
	logging.getLogger().setLevel(log_level)

	# create event uploader instance
	event_uploader = EventUploader(options)

	# install signal handler
	SignalHandler(event_uploader)

	# start up
	event_uploader.start()
