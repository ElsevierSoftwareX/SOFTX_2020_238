#!/usr/bin/env python
#
# Copyright (C) 2009-2013  Kipp Cannon, Chad Hanna, Drew Keppel
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

##@file gstlal_inspiral_marginalize_likelihood
# A program to marginalize the likelihood pdfs in noise across mass bins for a gstlal inspiral analysis.
#
# ### Command line interface
#
#	+ `--ignore-missing`: Ignore and skip missing input documents.
#	+ `--output` [filename]: Set the output file name (default = write to stdout).
#	+ `--likelihood-cache` [filename]: Set the cache file name from which to read likelihood files
#	+ `--verbose`: Be verbose.
#
# ### Review status
#
# | Names 	                    | Hash 					                   | Date       | Diff to Head of Master      |
# | --------------------------- | ---------------------------------------- | ---------- | --------------------------- |
# | Florent, Jolien, Kipp, Chad | 1dbbbd963c9dc076e1f7f5f659f936e44005f33b | 2015-05-14 | <a href="@gstlal_inspiral_cgit_diff/bin/gstlal_inspiral_marginalize_likelihood?id=HEAD&id2=1dbbbd963c9dc076e1f7f5f659f936e44005f33b">gstlal_inspiral_marginalize_likelihood</a> |
#
# #### Action
# - none
#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


from optparse import OptionParser
import sys


from glue.ligolw import ligolw
from glue.ligolw import utils as ligolw_utils
from glue.ligolw.utils import process as ligolw_process
from glue.ligolw.utils import search_summary as ligolw_search_summary
from glue import segments
from glue import lal


from gstlal import far


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
	)
	parser.add_option("--ignore-missing", action = "store_true", help = "Ignore and skip missing input documents.")
	parser.add_option("--require-coinc-param-data", action = "store_true", help = "Require input documents to contain candidate parameter PDF data (default = data is optional).")
	parser.add_option("--require-ranking-stat-data", action = "store_true", help = "Require input documents to contain ranking statistic PDF data (default = data is optional).")
	parser.add_option("--add-zerolag-to-background", action = "store_true", help = "Add zerolag events to background before populating coincident parameter PDF histograms")
	parser.add_option("-o", "--output", metavar = "filename", help = "Set the output file name (default = write to stdout).")
	parser.add_option("--likelihood-cache", metavar = "filename", help = "Set the cache file name from which to read likelihood files.")
	parser.add_option("--verbose", action = "store_true", help = "Be verbose.")

	options, urls = parser.parse_args()

	if options.likelihood_cache:
		urls += [lal.CacheEntry(line).url for line in open(options.likelihood_cache)]
	if not urls and not options.ignore_missing:
		raise ValueError("no input documents")

	return options, urls


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


#
# parse command line
#


options, urls = parse_command_line()


#
# initialize output document
#


xmldoc = ligolw.Document()
xmldoc.appendChild(ligolw.LIGO_LW())
process = ligolw_process.register_to_xmldoc(xmldoc, u"gstlal_inspiral_marginalize_likelihood", options.__dict__)
search_summary = ligolw_search_summary.append_search_summary(xmldoc, process)


#
# loop over input documents
#


distributions, ranking_data, seglists = far.marginalize_pdf_urls(
	urls,
	options.require_coinc_param_data,
	options.require_ranking_stat_data,
	ignore_missing_files = options.ignore_missing,
	verbose = options.verbose
)


#
# regenerate event parameter PDFs.  += method doesn't compute these
# correctly.
#
# NOTE:  the FAP/FAR code doesn't actually use any of the information in
# the parameter PDF file.  that code does need to know the "count above
# threshold" but in the offline case that count isn't known until after the
# coincs are ranked (the files loaded here have the count of all coincs,
# not just coincs above the ranking stat FAP/FAR normalization threshold).
# the compute_far_from_snr_chisq_histograms program will count the
# above-threshold events itself and replace the coinc counts in the
# parameter PDF file with its counts and write a new file.  we wouldn't
# need to load the parameter PDF files here at all except that the Farr et
# al. rate estimation code needs the marginalized numerator and denominator
# parameter PDFs.
#
# also re-generate likelihood ratio (ranking data) PDFs from the combined
# bin counts.  this shouldn't be needed but it's fast and maybe more
# accurate than relying on the weighted sum of the PDFs to have been
# numerically stable
#


if options.add_zerolag_to_background:
	distributions.zero_lag_in_background = True
if distributions is not None:
	distributions.finish(segs = seglists, verbose = options.verbose)
if ranking_data is not None:
	ranking_data.finish(verbose = options.verbose)


#
# write output document
#


if distributions is not None:
	distributions.process_id = process.process_id
if ranking_data is not None:
	ranking_data.process_id = process.process_id
far.gen_likelihood_control_doc(xmldoc, process, distributions, ranking_data, seglists)
search_summary.instruments = seglists.keys()
search_summary.in_segment = search_summary.out_segment = seglists.extent_all()
ligolw_process.set_process_end_time(process)
ligolw_utils.write_filename(xmldoc, options.output, gz = (options.output or "stdout").endswith(".gz"), verbose = options.verbose)
