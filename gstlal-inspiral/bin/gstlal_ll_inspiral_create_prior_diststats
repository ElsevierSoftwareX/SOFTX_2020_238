#!/usr/bin/env python
#
# Copyright (C) 2010--2014  Kipp Cannon, Chad Hanna
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#
## @file gstlal_ll_inspiral_create_prior_diststats
# A program to create some prior likelihood data to seed an online analysis
#
# ### Command line interface
#
#		--verbose, action = "store_true", help = "Be verbose."
#		--coincidence-threshold, metavar = "value", type = "float", default = 0.005, help = "Set the coincidence window in seconds (default = 0.005).  The light-travel time between instruments will be added automatically in the coincidence test."
#		--min-instruments, metavar = "count", type = "int", default = 2, help = "Set the minimum number of instruments that must contribute triggers to form a candidate (default = 2).
#		--num-templates, metavar = "N", default = 1000, type = "int", help = "Set the number of templates per bank. default 1000")
#		--num-banks, metavar = "N", default = 1, type = "int", help = "Set the number of banks. default 1")
#		--write-likelihood-basename, metavar = "string", default = "prior.xml.gz", help = "Write likelihood files to disk with this basename: default prior.xml.gz.")
#		--write-likelihood-cache, metavar = "filename", help = "Write likelihood files to disk and include the names in this cachefile file.")
#		--write-zerolag-likelihood-cache, metavar = "filename", help = "Write zerolag likelihood files to disk and include the names in this cachefile file.")
#		--segment-and-horizon, action = "append", help = "Append to a list of segments and horizon distances for a given instrument.  Argument specified as IFO:start:end:horizon, e.g., H1:1000000000:1000000100:120 ")
#		--override-background-prior, metavar = "N", type = "float", help = "Override the count in the SNR/chisquared bins to be this value")
#		--injection-prefactors, metavar = "s,e", default = "0.01,0.25", help = "Set the range of prefactors on the chi-squared distribution for the signal model: default 0.01,0.25")

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


import sys
import os
import shutil
from gstlal import far
from gstlal import inspiral_pipe
from gstlal.stats import horizonhistory
from gstlal import paths as gstlal_config_paths
from glue.ligolw import utils as ligolw_utils
from glue.ligolw.utils import process as ligolw_process
from glue.ligolw import ligolw
from glue import segments
from glue import iterutils
from glue.lal import CacheEntry
from lal import LIGOTimeGPS
from pylal import rate
from optparse import OptionParser


def parse_command_line():
	parser = OptionParser(
		version = "Name: %%prog\n%s" % "" # FIXME
	)
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	# FIXME:  default must be kept identical to gstlal_inspiral's
	parser.add_option("--coincidence-threshold", metavar = "value", type = "float", default = 0.005, help = "Set the coincidence window in seconds (default = 0.005).  The light-travel time between instruments will be added automatically in the coincidence test.")
	# FIXME:  default must be kept identical to gstlal_inspiral's
	parser.add_option("--min-instruments", metavar = "count", type = "int", default = 2, help = "Set the minimum number of instruments that must contribute triggers to form a candidate (default = 2).")
	parser.add_option("--num-templates", metavar = "N", default = 1000, type = "int", help = "Set the number of templates per bank. default 1000")
	parser.add_option("--num-banks", metavar = "N", default = 1, type = "int", help = "Set the number of banks. default 1")
	parser.add_option("--write-likelihood-basename", metavar = "string", default = "prior.xml.gz", help = "Write likelihood files to disk with this basename: default prior.xml.gz.")
	parser.add_option("--write-likelihood-cache", metavar = "filename", help = "Write likelihood files to disk and include the names in this cachefile file.")
	parser.add_option("--write-zerolag-likelihood-cache", metavar = "filename", help = "Write zerolag likelihood files to disk and include the names in this cachefile file.")
	parser.add_option("--segment-and-horizon", action = "append", help = "Append to a list of segments and horizon distances for a given instrument.  Argument specified as IFO:start:end:horizon, e.g., H1:1000000000:1000000100:120 ")
	parser.add_option("--override-background-prior", metavar = "N", type = "float", help = "Override the count in the SNR/chisquared bins to be this value")
	parser.add_option("--injection-prefactors", metavar = "s,e", default = "0.01,0.25", help = "Set the range of prefactors on the chi-squared distribution for the signal model: default 0.01,0.25")
	options, filenames = parser.parse_args()

	process_params = dict(options.__dict__)

	def parse_segment_and_horizon(options = options):
		seglistdict = segments.segmentlistdict()
		horizon_history = {}
		for x in options.segment_and_horizon:
			ifo, start, stop, horizon = x.split(":")
			seglistdict.setdefault(ifo, segments.segmentlist()).append(segments.segment([LIGOTimeGPS(start), LIGOTimeGPS(stop)]))
			horizon_history.setdefault(ifo, horizonhistory.NearestLeafTree())[float(start)] = float(horizon)
			horizon_history.setdefault(ifo, horizonhistory.NearestLeafTree())[float(stop)] = float(horizon)
		return seglistdict, horizon_history

	if options.segment_and_horizon is None:
		raise ValueError("--segment-and-horizon required")
	seglistdict, horizon_history = parse_segment_and_horizon(options)

	options.instruments = frozenset(seglistdict)
	if not options.instruments:
		raise ValueError("must specify at least one instrument")
	if options.min_instruments > len(options.instruments):
		raise ValueError("--min-instruments is greater than number of available instruments")

        bankcachedict = None #inspiral_pipe.parse_cache_str(options.bank_cache)

	return options, process_params, seglistdict, horizon_history, bankcachedict


options, process_params, segs, horizon_history, bankcachedict = parse_command_line()

if options.verbose:
	print >> sys.stderr, "Livetime: ", abs(segs)
	print >> sys.stderr, "Extent: ", segs.extent_all()

#
# quantities derived from input
#

#
# Number of background events in each detector
#
# This is calculated assuming the following
# 1) There are options.num_templates in the analysis
# 2) Each template produces exactly 1 trigger for every second that a given
# detector is on according to the user provided segments
#
# FIXME what should zero lag be? Right now we set them to be the same
#

n_background = dict(((ifo, float(abs(seg)) * options.num_templates) for ifo, seg in segs.items())) 
n_zerolag    = dict(((ifo, float(abs(seg)) * options.num_templates) for ifo, seg in segs.items()))

#
# Initialize an empty ThincaCoincParamsDistributions class
#

diststats = far.ThincaCoincParamsDistributions(instruments = set(segs), delta_t = options.coincidence_threshold, min_instruments = options.min_instruments)

#
# Add background, zero_lag and injection prior distributions in the SNR and chi-squared plane
# 

# pull out the all O1 marginalized snr / chisq as a good prior guess
background_pdf_file = os.path.join(gstlal_config_paths["pkgdatadir"], "O1_snr_chi_pdf_reference.xml.gz")
background_xml = ligolw_utils.load_filename(background_pdf_file, contenthandler = far.ThincaCoincParamsDistributions.LIGOLWContentHandler, verbose = options.verbose)
background_binned_array = rate.BinnedArray.from_xml(background_xml, name = "background:H1_snr_chi")

# fill in the background to the requested amount
for instrument, number_of_events in n_background.items():
	binarr = diststats.background_rates["%s_snr_chi" % instrument]
	binarr.array[:] = background_binned_array.array[:]
	binarr.array *= number_of_events / binarr.array.sum()
	# FIXME is this right? Assuming that the events are produced at a rate
	# of 1 per template per second, the total rate in this bin is just
	# equal to the number of templates
	diststats.background_rates["singles"][instrument,] = number_of_events
	if options.verbose:
		print "setting number of background events in %s to %d" % (instrument, number_of_events)

# fill in the zerolag to the requested amount
for instrument, number_of_events in n_zerolag.items():
	binarr = diststats.zero_lag_rates["%s_snr_chi" % instrument]
	binarr.array[:] = background_binned_array.array[:]
	binarr.array *= number_of_events / binarr.array.sum()
	# FIXME is this right? Assuming that the events are produced at a rate
	# of 1 per template per second, the total rate in this bin is just
	# equal to the number of templates
	diststats.zero_lag_rates["singles"][instrument,] = number_of_events
	if options.verbose:
		print "setting number of zero events in %s to %d" % (instrument, number_of_events)

diststats.add_foreground_snrchi_prior(n = dict(((ifo, 1e8) for ifo, seg in segs.items())), prefactors_range = tuple(float(x) for x in options.injection_prefactors.split(",")), verbose = options.verbose)

#
# Update the horizon distance history with our fake, user provided horizon history
#

diststats.horizon_history.update(horizon_history)

#
# Fake a set of coincident triggers for all possible combinations assuming each coincidence level reduces the rate by 50.
# FIXME How is this related to the background_rates["singles"] ???
#

for i in range(options.min_instruments, len(options.instruments) + 1):
	for ifos in [frozenset(x) for x in iterutils.choices(tuple(options.instruments), i)]:
		diststats.zero_lag_rates["instruments"][ifos,] = min(n_zerolag[ifo] for ifo in ifos) / 50.**(i-1)
		if options.verbose:
			print "setting number of zerolag events for %s to %d" % (ifos, diststats.zero_lag_rates["instruments"][ifos,])


#
# Wrap it up
#

diststats.finish(segs = segs, verbose = options.verbose)

#
# Prep an output XML file
#

xmldoc = ligolw.Document()
xmldoc.appendChild(ligolw.LIGO_LW())
process = ligolw_process.register_to_xmldoc(xmldoc, u"gstlal_ll_inspiral_create_prior_diststats", ifos = options.instruments, paramdict = process_params)

#
# Instantiate a new RankingData class from our distribution stats
#

ranking_data = far.RankingData(diststats, process_id = process.process_id, nsamples = 100000, verbose = options.verbose)

#
# Simulate a measured coincident trigger likelihood histogram by just using the background one with a lower normalized count
#

for instruments in ranking_data.background_likelihood_rates:
	if instruments is None:
		continue
	else:
		ranking_data.zero_lag_likelihood_rates[instruments].array[:] = ranking_data.background_likelihood_rates[instruments].array[:] / ranking_data.background_likelihood_rates[instruments].array.sum() * diststats.zero_lag_rates["instruments"][instruments,]
		ranking_data.zero_lag_likelihood_rates[instruments].array[:] = ranking_data.zero_lag_likelihood_rates[instruments].array.round()

ranking_data._compute_combined_rates()
ranking_data.finish()

#
# Override the background SNR/chisq priors if requested
# This can help the ranking to converge more quickly
# 

if options.override_background_prior is not None:
	prior = dict(((ifo, options.override_background_prior) for ifo, seg in segs.items()))
	for ba in ("background_rates", "zero_lag_rates"):
		for instrument, number_of_events in prior.items():
			binarr = getattr(diststats, ba)["%s_snr_chi" % instrument]
			binarr.array *= number_of_events / binarr.array.sum()
			if options.verbose:
				print >> sys.stderr, "Overriding the %s background snr / chisq priors to: %.2f" % (instrument, binarr.array.sum())

#
# done
#

ligolw_process.set_process_end_time(process)

#
# record background results in output file
#

far.gen_likelihood_control_doc(xmldoc, process, diststats, ranking_data, segs, comment = u"background and signal priors (no real data)")
cachefile = open(options.write_likelihood_cache, "w")
for n in range(options.num_banks):
	this_name = "%04d_%s" % (n, options.write_likelihood_basename)
	c = CacheEntry("".join(sorted(segs.keys())), "PRIORS", segs.extent_all(), this_name)
	if n == 0:
		ligolw_utils.write_filename(xmldoc, this_name, gz = this_name.endswith(".gz"), verbose = options.verbose)
		original = this_name
	else:
		shutil.copyfile(original, this_name)
	print >> cachefile, str(c)
cachefile.close()


#
# create all-zero ranking statistic histograms to seed zero-lag counts
#


xmldoc = ligolw.Document()
xmldoc.appendChild(ligolw.LIGO_LW())
process = ligolw_process.register_to_xmldoc(xmldoc, u"gstlal_ll_inspiral_create_prior_diststats", ifos = options.instruments, paramdict = process_params)
# re-use ranking_data from above so we've got the right instrument
# combinations, but wipe the arrays of the other rates
for instruments in ranking_data.background_likelihood_rates:
	if instruments is not None:
		ranking_data.background_likelihood_rates[instruments].array[:] = 0
		ranking_data.background_likelihood_pdfs[instruments].array[:] = 0
		ranking_data.signal_likelihood_rates[instruments].array[:] = 0
		ranking_data.signal_likelihood_pdfs[instruments].array[:] = 0
		if options.verbose:
			print "setting %f counts for zero lag in instrument combo %s" % (ranking_data.zero_lag_likelihood_rates[instruments].array.sum(), instruments)

far.gen_likelihood_control_doc(xmldoc, process, None, ranking_data, segs, comment = u"zerolag priors (no real data)")
cachefile = open(options.write_zerolag_likelihood_cache, "w")
for n in range(options.num_banks):
	this_name = "%04d_zerolag_%s" % (n, options.write_likelihood_basename)
	c = CacheEntry("".join(sorted(segs.keys())), "PRIORS", segs.extent_all(), this_name)
	if n == 0:
		ligolw_utils.write_filename(xmldoc, this_name, gz = this_name.endswith(".gz"), verbose = options.verbose)
		original = this_name
	else:
		shutil.copyfile(original, this_name)
	print >> cachefile, str(c)
cachefile.close()
