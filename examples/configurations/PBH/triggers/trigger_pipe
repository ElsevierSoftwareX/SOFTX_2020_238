#!/usr/bin/python
"""
This program makes a dag to generate svd banks
"""

__author__ = 'Chad Hanna <channa@caltech.edu>'

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import subprocess, socket, tempfile

##############################################################################
# import the modules we need to build the pipeline
from glue import iterutils
from glue import pipeline
from glue import lal
from glue.ligolw import lsctables
from glue import segments
import glue.ligolw.utils as utils
import glue.ligolw.utils.segments as ligolw_segments
from optparse import OptionParser

def which(prog):
	which = subprocess.Popen(['which',prog], stdout=subprocess.PIPE)
	out = which.stdout.read().strip()
	if not out: 
		print >>sys.stderr, "ERROR: could not find %s in your path, have you built the proper software and source the proper env. scripts?" % (prog,prog)
		raise ValueError 
	return out

def log_path():
	host = socket.getfqdn()
	#FIXME add more hosts as you need them
	if 'caltech.edu' in host: return '/usr1/' + os.environ['USER']
	if 'phys.uwm.edu' in host: return '/localscratch/' + os.environ['USER']
	if 'aei.uni-hannover.de' in host: return '/local/user/' + os.environ['USER']
	if 'phy.syr.edu' in host: return '/usr1/' + os.environ['USER']


class bank_DAG(pipeline.CondorDAG):

	def __init__(self, name, logpath = log_path()):
		self.basename = name
		tempfile.tempdir = logpath
		tempfile.template = self.basename + '.dag.log.'
		logfile = tempfile.mktemp()
		fh = open( logfile, "w" )
		fh.close()
		pipeline.CondorDAG.__init__(self,logfile)
		self.set_dag_file(self.basename)
		self.jobsDict = {}
		self.node_id = 0
		self.output_cache = []

	def add_node(self, node):
		node.set_retry(3)
		self.node_id += 1
		node.add_macro("macroid", self.node_id)
		pipeline.CondorDAG.add_node(self, node)

	def write_cache(self):
		out = self.basename + ".cache"
		f = open(out,"w")
		for c in self.output_cache:
			f.write(str(c)+"\n")
		f.close()

class gstlal_inspiral_job(pipeline.CondorDAGJob):
	"""
	A gstlal_inspiral job
	"""
	def __init__(self, executable=which('gstlal_inspiral'), tag_base='gstlal_inspiral'):
		"""
		"""
		self.__prog__ = 'gstlal_inspiral'
		self.__executable = executable
		self.__universe = 'vanilla'
		pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
		self.add_condor_cmd('getenv','True')
		self.add_condor_cmd('requirements', 'Memory > 3000') #FIXME is this enough?
		self.tag_base = tag_base
		self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
		self.set_sub_file(tag_base+'.sub')
		self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(process).out')
		self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(process).err')


class gstlal_inspiral_node(pipeline.CondorDAGNode):
	"""
	A gstlal_inspiral node
	"""
	def __init__(self, job, dag, frame_cache, gps_start_time, gps_end_time, instrument, channel, reference_psd, svd_bank, tmp_space=log_path(), injections=None, flow=40, p_node=[]):

		pipeline.CondorDAGNode.__init__(self,job)
		self.add_var_opt("frame-cache", frame_cache)
		self.add_var_opt("gps-start-time",gps_start_time)
		self.add_var_opt("gps-end-time",gps_end_time)
		self.add_var_opt("instrument", instrument)
		self.add_var_opt("channel-name", channel)
		self.add_var_opt("reference-psd", reference_psd)
		self.add_var_opt("svd_bank", svd_bank)
		self.add_var_opt("tmp-space", tmp_space)
		self.add_var_opt("flow", flow)
		if injections: self.add_var_opt("injections", injections)
		path = os.getcwd()
		output_name = '%s/%s-%d-%d-triggers.sqlite' % (path, svd_bank, gps_start_time, gps_end_time)
		self.add_var_opt("output",output_name)
		dag.output_cache.append(lal.CacheEntry(instrument, "-", segments.segment(gps_start_time, gps_end_time), "file://localhost/%s" % (output_name,)))
		for p in p_node:
			self.add_parent(p)
		dag.add_node(self)

# FIXME surely this is in glue
def parse_cache_str(instr):
	dictcache = {}
	for c in instr.split(','):
		ifo = c.split("=")[0]
		cache = c.replace(ifo+"=","")
		dictcache[ifo] = cache
	return dictcache

def cache_to_dict(cachefile):
	out  = {}
	for l in open(cachefile):
		c = lal.CacheEntry(l)
		out.setdefault(c.observatory, []).append(c)
	return out

def parse_command_line():
	parser = OptionParser(description = __doc__)
	parser.add_option("--injections", metavar = "filename", help = "Set the name of the xml file for injections")
	parser.add_option("--frame-cache", metavar = "filenames", help = "Set the frame cache files in format H1=H1.cache,H2=H2.cache, etc..")
	parser.add_option("--psd-cache", metavar = "filenames", help = "Set the psd cache file. (just one)")
	parser.add_option("--bank-cache", metavar = "filenames", help = "Set the bank cache files in format H1=H1.cache,H2=H2.cache, etc..")
	#FIXME get this from the cache?
	parser.add_option("--channel", metavar = "name", help = "Set the channel name (default=LSC-STRAIN)", default="LSC-STRAIN")
	
	options, filenames = parser.parse_args()

	fail = ""
	for option in ("frame_cache", "psd_cache", "bank_cache"):
		if getattr(options, option) is None:
			fail += "must provide option %s\n" % (option)
	if fail: raise ValueError, fail

	#FIXME add consistency check?
	framecache = parse_cache_str(options.frame_cache)
	bankcache = parse_cache_str(options.bank_cache)
	
	return options, filenames, framecache, bankcache#, process_params


options, filenames, frame_cache, bank_cache = parse_command_line()

try: os.mkdir("logs")
except: pass
dag = bank_DAG("trigger_pipe")

inspJob = gstlal_inspiral_job()

for instrument, centries in cache_to_dict(options.psd_cache).items():
	for centry in centries:
		seg = centry.segment
		for bank in cache_to_dict(bank_cache[instrument])[instrument]:
			#FIXME if there are segments without frame caches this will barf
			gstlal_inspiral_node(inspJob, dag, frame_cache[instrument], int(seg[0]), int(seg[1]), instrument, options.channel, reference_psd=centry.path(), svd_bank=bank.path(), injections=options.injections, flow=40)
		
dag.write_sub_files()
dag.write_dag()
dag.write_script()
dag.write_cache()
