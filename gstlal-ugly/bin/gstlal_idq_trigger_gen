#!/usr/bin/env python

# Copyright (C) 2017 Sydney J. Chamberlin, Patrick Godwin, Chad Hanna, Duncan Meacher
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.



####################
# 
#     preamble
#
####################   


from optparse import OptionParser
from collections import deque
import os
import sys
import socket
import resource
import StringIO
import threading
import shutil
import traceback

import gi
gi.require_version('Gst', '1.0')
from gi.repository import GObject, Gst
GObject.threads_init()
Gst.init(None)
import lal

import numpy
import pandas

from gstlal import pipeio
from gstlal import datasource
from gstlal import idq_multirate_datasource
from gstlal import multichannel_datasource
from gstlal import sngltriggertable
from gstlal import pipeparts
from gstlal import simplehandler
from gstlal import aggregator
from gstlal import httpinterface
from gstlal import bottle
from glue.ligolw import utils as ligolw_utils
from glue import iterutils
from glue.ligolw import ligolw
from glue.ligolw import utils as ligolw_utils
from glue.ligolw.utils import process as ligolw_process

#
# Make sure we have sufficient resources
# We allocate far more memory than we need, so this is okay
#

def setrlimit(res, lim):
	hard_lim = resource.getrlimit(res)[1]
	resource.setrlimit(res, (lim if lim is not None else hard_lim, hard_lim))

# set the number of processes and total set size up to hard limit and
# shrink the per-thread stack size (default is 10 MiB)
setrlimit(resource.RLIMIT_NPROC, None)
setrlimit(resource.RLIMIT_AS, None)
setrlimit(resource.RLIMIT_RSS, None)
# FIXME:  tests at CIT show that this next tweak has no effect.  it's
# possible that SL7 has lowered the default stack size from SL6 and we
# don't need to do this anymore.  remove?
setrlimit(resource.RLIMIT_STACK, 1024 * 1024) # 1 MiB per thread

####################
# 
#    functions
#
####################   

#
# construct sine gaussian waveforms that taper to 1e-7 at edges of window
#
def duration(phi, q, tolerance = 1e-7):
        # return the duration of the waveform such that its edges will die out to tolerance of the peak.
	# phi is the central frequency of the frequency band
	return 2.*q/(2.*numpy.pi*phi)*numpy.log(1./tolerance)

def sine_gaussian(phi, phi_0, q, time_arr):
        # edges should be 1e-7 times peak
	dt = time_arr[1]-time_arr[0]
	rate = 1./dt
	assert phi < rate/2. 

	# phi is the central frequency of the sine gaussian
	dur = duration(phi,q)
	tau = q/(2.*numpy.pi*phi)
	sg_vals = numpy.cos(2.*numpy.pi*phi*time_arr + phi_0)*numpy.exp(-1.*time_arr**2./tau**2.)

	# normalize sine gaussians to have unit length in their vector space
	inner_product = numpy.sum(sg_vals*sg_vals)
	norm_factor = 1./(inner_product**0.5)

	return norm_factor*sg_vals 

def half_sine_gaussian(sg_arr):
	samples = sg_arr.size/2 + 1

	# only take first half of sine gaussian + peak
	hsg_vals = sg_arr[:samples]
	
	# renormalize
	inner_product = numpy.sum(hsg_vals*hsg_vals)
	norm_factor = 1./(inner_product**0.5)

	return norm_factor*hsg_vals

#
# number of tiles in frequency and Q
#
def N_Q(q_min, q_max, mismatch = 0.2):
        return numpy.ceil(1./(2.*numpy.sqrt(mismatch/3.))*1./numpy.sqrt(2)*numpy.log(q_max/q_min))

def N_phi(phi_min, phi_max, q,  mismatch = 0.2):
       return numpy.ceil(1./(2.*numpy.sqrt(mismatch/3.))*(numpy.sqrt(2.+q**2.)/2.)*numpy.log(phi_max/phi_min))

def Qq(q_min, q_max, mismatch = 0.2):
	N_q = numpy.ceil(N_Q(q_min, q_max, mismatch = mismatch))
	return [q_min*(q_max/q_min)**((0.5+q)/N_q) for q in range(int(N_q))]

def phi_ql(phi_min, phi_max, q_min, q_max, mismatch = 0.2):
	for q in Qq(q_min, q_max, mismatch = mismatch):
		nphi = N_phi(phi_min, phi_max, q, mismatch = mismatch)
		for l in range(int(nphi)):
			yield (phi_min*(phi_max/phi_min)**((0.5+l)/nphi), q)

def parse_command_line():

	parser = OptionParser(description = __doc__)

	#
	# First append the datasource common options
	#

	multichannel_datasource.append_options(parser)
	parser.add_option("--out-path", metavar = "path", default = ".", help = "Write to this path. Default = .")
	parser.add_option("--description", metavar = "string", default = "GSTLAL_IDQ_TRIGGERS", help = "Set the filename description in which to save the output.")
	parser.add_option("--cadence", type = "int", default = 32, help = "Rate at which to write trigger files to disk. Default = 32 seconds.")
	parser.add_option("--disable-web-service", action = "store_true", help = "If set, disables web service that allows monitoring of PSDS of aux channels.")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	parser.add_option("--triggers-from-dataframe", action = "store_true", default = False, help = "If set, will output iDQ-compatible triggers to disk straight from dataframe once every cadence")
	parser.add_option("-m", "--mismatch", type = "float", default = 0.2, help = "Mismatch between templates, mismatch = 1 - minimal match. Default = 0.2.")
	parser.add_option("-q", "--qhigh", type = "float", default = 20, help = "Q high value for half sine-gaussian waveforms. Default = 20.")

	#
	# parse the arguments and sanity check
	#

	options, filenames = parser.parse_args()

	return options, filenames

####################
# 
#      classes
#
####################   


class MultiChannelHandler(simplehandler.Handler):
	"""
	A subclass of simplehandler.Handler to be used with 
	multiple channels.

	Implements additional message handling for dealing with spectrum
	messages and creates trigger files for use in iDQ.
	"""
	def __init__(self, *args, **kwargs):
		self.lock = threading.Lock()
		self.basis_params = kwargs.pop("basis_params")
		self.description = kwargs.pop("description")
		self.out_path = kwargs.pop("out_path")
		self.instrument = kwargs.pop("instrument")
		self.keys = kwargs.pop("keys")

		# iDQ saving properties
		self.last_save_time = None
		self.cadence = options.cadence
		# create header for trigger file
		# header for string output from dataframe
		if options.triggers_from_dataframe:
			self.header = "# %18s\t%17s\t%14s\t%9s\t%1s\t%1s\t%2s\t%7s\t%s\n" % ("start_time", "stop_time", "trigger_time", "freq", "phase", "sigmasq", "chisq", "snr", "channel")
		# header for standard output straight to string
		else:
			#self.header = "# %18s\t%20s\t%20s\t%6s\t%8s\t%8s\t%8s\t%10s\t%10s\t%9s\t%8s\t%s\n" % ("start_time", "stop_time", "trigger_time", "phase", "snr", "chisq", "sigmasq", "frequency", "Q", "latency", "rate", "channel")
			self.header = "# %18s\t%20s\t%20s\t%10s\t%8s\t%8s\t%8s\t%10s\t%s\n" % ("start_time", "stop_time", "trigger_time", "frequency", "phase", "sigmasq", "chisq", "snr", "channel")
		self.fdata = ""

		# dataframe/hdf saving properties
		self.last_hdf_save_time = dict.fromkeys(self.keys, None)
		self.hdf_cadence = 10
		self.reduce_cadence = 100
		self.init_gps_time = int(aggregator.now())

		# dataframe properties
		columns = ['start_time', 'stop_time', 'trigger_time', 'frequency', 'phase', 'sigmasq', 'chisq', 'snr']
		self.dataframes = {}
		for (channel, rate) in self.keys:
			self.dataframes[(channel, rate)] = pandas.DataFrame(numpy.nan, index = self.to_dataframe_index(self.init_gps_time), columns = columns)
		
		# set up bottle routes for spectra by each whitener
		self.psds = {}
		if not options.disable_web_service:
			bottle.route("/psds.xml")(self.web_get_psd_xml)

		super(MultiChannelHandler, self).__init__(*args, **kwargs)

	def do_on_message(self, bus, message):
		"""!
		Handle application-specific message types, 
		e.g., spectrum messages.
		
		@param bus A reference to the pipeline's bus
		@param message A reference to the incoming message
		"""
		#
		# return value of True tells parent class that we have done
		# all that is needed in response to the message, and that
		# it should ignore it.  a return value of False means the
		# parent class should do what it thinks should be done
		#
		if message.type == Gst.MessageType.ELEMENT:
			if message.get_structure().get_name() == "spectrum":
				# get the channel name & psd.
				instrument, info = message.src.get_name().split("_", 1)
				channel, _ = info.rsplit("_", 1)
				psd = pipeio.parse_spectrum_message(message)
				# save psd
				self.psds[channel] = psd
				return True		
		return False

	def bufhandler(self, elem, sink_dict):
		with self.lock:
			buf = elem.emit("pull-sample").get_buffer()
			buftime = int(buf.pts / 1e9)
			channel, rate  = sink_dict[elem]

			# set save times appropriately
			if self.last_save_time is None:
				self.last_save_time = buftime
			if self.last_hdf_save_time[(channel, int(rate))] is None:
				self.last_hdf_save_time[(channel, int(rate))] = buftime
				# check if dataframe needs to be reindexed with new gps times before first buffer
				if (buftime - self.truncate_int(self.init_gps_time, aggregator.MIN_TIME_QUANTA)) >= aggregator.MIN_TIME_QUANTA:
					self.dataframes[(channel, int(rate))] = self.dataframes[(channel, int(rate))].reindex(self.to_dataframe_index(buftime))
			# create path if it doesn't already exist
			if not os.path.exists(self.to_agg_path(buftime, channel = channel, rate = rate)):
				aggregator.makedir(self.to_agg_path(buftime, channel = channel, rate = rate))
			
			# Save triggers once every cadence
			# iDQ file saving
			if ((buftime - self.truncate_int(self.last_save_time, self.cadence)) >= self.cadence):
				if options.triggers_from_dataframe:
					self.to_trigger_file(self.to_dataframe_string(buftime))
				else:
					self.to_trigger_file()
					self.fdata = ""
				self.last_save_time = buftime

			# hdf file saving
			if options.triggers_from_dataframe:
				if (buftime - self.truncate_int(self.last_hdf_save_time[(channel, int(rate))], self.hdf_cadence)) >= self.hdf_cadence:
					self.to_hdf5(channel, int(rate), buftime)
					self.last_hdf_save_time[(channel, int(rate))] = buftime

			# read buffer contents
			for i in range(buf.n_memory()):
				memory = buf.peek_memory(i)
				result, mapinfo = memory.map(Gst.MapFlags.READ)
				assert result
				# NOTE NOTE NOTE NOTE
				# It is critical that the correct class'
				# .from_buffer() method be used here.  This
				# code is interpreting the buffer's
				# contents as an array of C structures and
				# building instances of python wrappers of
				# those structures but if the python
				# wrappers are for the wrong structure
				# declaration then terrible terrible things
				# will happen
				if mapinfo.data:
					for row in sngltriggertable.GSTLALSnglTrigger.from_buffer(mapinfo.data):
						trigger_time = row.end_time + row.end_time_ns * 1e-9
						latency = numpy.round(int(aggregator.now()) - buftime)
						freq, q, duration = self.basis_params[(channel, int(rate))][row.channel_index]
						start_time = trigger_time - duration
						channel_tag = ('%s_%i_%i' %(channel, int(rate)/4, int(rate)/2)).replace(":","_",1)
						# NOTE
						# Setting stop time to trigger time for use with half sine gaussians
						stop_time = trigger_time
						# save iDQ compatible data
						if not options.triggers_from_dataframe:
							#self.fdata += "%20.9f\t%20.9f\t%20.9f\t%6.3f\t%8.3f\t%8.3f\t%8.3f\t%10.3f\t%10.3f\t%9d\t%8.1f\t%s\n" % (start_time, stop_time, trigger_time, phase, snr, chisq, sigmasq, freq, q, latency, int(rate), channel)
							self.fdata += "%20.9f\t%20.9f\t%20.9f\t%10.3f\t%8.3f\t%8.3f\t%8.3f\t%10.3f\t%s\n" % (start_time, stop_time, trigger_time, freq, row.phase, row.sigmasq, row.chisq, row.snr, channel_tag)
						# save dataframe compatible data
						buftime_index = self.truncate_int(buftime, self.hdf_cadence)
						if options.triggers_from_dataframe:
							try:
								self.dataframes[(channel, int(rate))].loc[buftime_index, buftime] = numpy.array([start_time, stop_time, trigger_time, freq, row.phase, row.sigmasq, row.chisq, row.snr], dtype=float)
							except ValueError:
								print >>sys.stderr, "Error saving buffer contents to dataframe at buffer time = %d for channel = %s, rate = %d." % (buftime, channel, int(rate))
								traceback.print_exc()

				memory.unmap(mapinfo)

			del buf
			return Gst.FlowReturn.OK

	def to_hdf5(self, channel, rate, buftime):
		last_save_index = self.truncate_int(self.last_hdf_save_time[(channel, rate)], self.hdf_cadence) - self.hdf_cadence
		# check to make sure saving index only covers current dataframe index
		if (last_save_index + self.hdf_cadence) % aggregator.MIN_TIME_QUANTA == 0:
			last_save_index = last_save_index + self.hdf_cadence
		# case 1: save current triggers from prev leaf directory, aggregate, reindex and clean out dataframe, move to new leaf directory
		# FIXME: Doesn't save last 10 second chunk of triggers,
		#        and also has an indexing error in first 10 sec,
		#        probably an 'off by one' implementation, will
		#        need to look into this, but have workaround
		#        for now.
		if (buftime - self.truncate_int(self.last_hdf_save_time[(channel, rate)], aggregator.MIN_TIME_QUANTA)) >= aggregator.MIN_TIME_QUANTA:
			current_save_index = self.truncate_int(buftime, aggregator.MIN_TIME_QUANTA) - self.hdf_cadence
			for save_index in range(last_save_index, current_save_index, self.hdf_cadence):
				try:
					self.dataframes[(channel, rate)].loc[save_index].to_hdf(os.path.join(self.to_agg_path(save_index, channel, rate), '%d.h5' % save_index), 'data', format = 'table', mode = 'w')
				except KeyError:
					traceback.print_exc()
			# find gps times of max snr triggers per cadence and save to file
			last_reduce_index = self.truncate_int(buftime, self.reduce_cadence) - self.hdf_cadence
			max_index = self.dataframes[(channel, rate)].loc[last_reduce_index:current_save_index].groupby(level=0)['snr'].idxmax().dropna().values
			if max_index.size > 0:
				try:
					self.dataframes[(channel, rate)].loc[max_index].to_hdf(os.path.join(self.to_agg_path(current_save_index, channel, rate), 'aggregates.h5'), 'max', format = 'table', mode = 'a', append = True)
				except KeyError:
					traceback.print_exc()
			# reindex to clean out dataframe and save new triggers
			self.dataframes[(channel, rate)] = self.dataframes[(channel, rate)].reindex(self.to_dataframe_index(self.truncate_int(self.last_hdf_save_time[(channel, rate)], aggregator.MIN_TIME_QUANTA) + aggregator.MIN_TIME_QUANTA))
		else:
			# case 2: save current triggers
			current_save_index = self.truncate_int(buftime, self.hdf_cadence) - self.hdf_cadence
			for save_index in range(last_save_index, current_save_index, self.hdf_cadence):
				try:
					self.dataframes[(channel, rate)].loc[save_index].to_hdf(os.path.join(self.to_agg_path(save_index, channel, rate), '%d.h5' % save_index), 'data', format = 'table', mode = 'w')
				except KeyError:
					traceback.print_exc()
			# case 3: save current triggers from current directory and aggregate
			if (buftime - self.truncate_int(self.last_hdf_save_time[(channel, rate)], self.reduce_cadence)) >= self.reduce_cadence:
				# find gps times of max snr triggers per cadence and save to file
				last_reduce_index = self.truncate_int(buftime, self.reduce_cadence) - self.hdf_cadence
				max_index = self.dataframes[(channel, rate)].loc[last_reduce_index:current_save_index].groupby(level=0)['snr'].idxmax().dropna().values
				if max_index.size > 0:
					try:
						self.dataframes[(channel, rate)].loc[max_index].to_hdf(os.path.join(self.to_agg_path(current_save_index, channel, rate), 'aggregates.h5'), 'max', format = 'table', mode = 'a', append = True)
					except KeyError:
						traceback.print_exc()

	def to_dataframe_string(self, gps_time):
		"""
		Given a gps time, will write contents of
		dataframe from last save time to gps time
		to a string.
		"""
		df_data = ""
		idx = pandas.IndexSlice
		time_format = lambda x: '%20.9f' % x
		col1_format = lambda x: '%10.3f' % x
		col2_format = lambda x: '%8.3f' % x
		trigger_format = {'start_time': time_format, 'stop_time': time_format, 'trigger_time': time_format, 'frequency': col1_format, 'phase': col2_format, 'sigmasq': col2_format, 'chisq': col2_format, 'snr': col1_format}
		for (channel, rate) in self.keys:
			channel_tag_str = ('%s_%i_%i' %(channel, int(rate)/4, int(rate)/2)).replace(":","_",1)
			channel_tag = pandas.Series(numpy.array([channel_tag_str for i in range(self.last_save_time,gps_time + 1)]))
			gps_index = self.truncate_int(gps_time, self.hdf_cadence)
			if not self.dataframes[(channel, int(rate))].loc[idx[:, self.last_save_time:gps_time], :].dropna().empty:
				df_data += self.dataframes[(channel, int(rate))].loc[idx[:, self.last_save_time:gps_time], :].assign(channel_tag = channel_tag.values).dropna().to_string(col_space = 3, header = False, index = False, formatters = trigger_format) + "\n"
		return df_data

	def to_dataframe_index(self, gps_time):
		"""
		Returns a two level index based on gps times
		per minimum aggregator quanta.
		"""
		index_t = numpy.arange(self.truncate_int(gps_time, aggregator.MIN_TIME_QUANTA), self.truncate_int(gps_time, aggregator.MIN_TIME_QUANTA) + aggregator.MIN_TIME_QUANTA, dtype = numpy.int)
		index_cadence = numpy.fromiter(( self.truncate_int(x, self.hdf_cadence) for x in index_t), dtype = numpy.int)
		return pandas.MultiIndex.from_tuples(list(zip(index_cadence, index_t)), names = ['gps_time_cadence', 'gps_time'])

	def to_agg_path(self, gps_time, channel, rate, level = 0):
		"""
		Returns a hierarchical gps time path based on
		channel rate and level in hierarchy.
		e.g. level 0: out-path/description/channel/1/2/3/4/5/6/rate/
		e.g. level 2: out-path/description/channel/1/2/3/4/rate/
		"""
		tag = '%s-IDQ_TRIGGERS_BY_CHANNEL' % self.instrument[:1]
		path = os.path.join(self.out_path, tag)
		if channel is not None:
			path = os.path.join(path, channel)
		path = os.path.join(path, aggregator.gps_to_leaf_directory(gps_time, level = level))
		if rate is not None:
			path = os.path.join(path, str(rate).zfill(5))
		return path
	
	def truncate_int(self, x, n):
		"""
		Truncates an integer by removing its remainder
		from integer division by another integer n.
		e.g. truncate_int(163, 10) = 160
		"""
		assert n > 0
		return (x / n) * n

	def to_trigger_file(self, framedata = None):
		# NOTE
		# This method should only be called by an instance that is locked.
		# Use T050017 filenaming convention.
		tag = '%s-%s' % (self.instrument[:1], self.description)
		fname = '%s-%d-%d.%s' % (tag, self.truncate_int(self.last_save_time, self.cadence), self.cadence, "trg")
		path = os.path.join(self.out_path, tag, tag+"-"+str(fname.split("-")[2])[:5])
		fpath = os.path.join(path, fname)
		tmpfile = fpath+"~"
		try:
			os.makedirs(path)
		except OSError:
			pass
		if framedata is not None:
			data = self.header + framedata
		else:
			data = self.header + self.fdata
		with open(tmpfile, 'w') as f:
 			f.write(data)
		shutil.move(tmpfile, fpath)

	def gen_psd_xmldoc(self):
		xmldoc = lal.series.make_psd_xmldoc(self.psds)
		process = ligolw_process.register_to_xmldoc(xmldoc, "gstlal_idq", {})
		ligolw_process.set_process_end_time(process)
		return xmldoc

	def web_get_psd_xml(self):
		with self.lock:
			output = StringIO.StringIO()
			ligolw_utils.write_fileobj(self.gen_psd_xmldoc(), output)
			outstr = output.getvalue()
			output.close()
		return outstr


class LinkedAppSync(pipeparts.AppSync):
	def __init__(self, appsink_new_buffer, sink_dict = {}):
		super(LinkedAppSync, self).__init__(appsink_new_buffer, sink_dict.keys())
		self.sink_dict = sink_dict	
		self.time_ordering = 'full'
	
	def attach(self, appsink):
		"""
		connect this AppSync's signal handlers to the given appsink
		element.  the element's max-buffers property will be set to
		1 (required for AppSync to work).
		"""
		if appsink in self.appsinks:
			raise ValueError("duplicate appsinks %s" % repr(appsink))
		appsink.set_property("max-buffers", 1)
		handler_id = appsink.connect("new-preroll", self.new_preroll_handler)
		assert handler_id > 0
		handler_id = appsink.connect("new-sample", self.new_sample_handler)
		assert handler_id > 0
		handler_id = appsink.connect("eos", self.eos_handler)
		assert handler_id > 0
		self.appsinks[appsink] = None
		_, rate, channel = appsink.name.split("_", 2)
		self.sink_dict.setdefault(appsink, (channel, rate))
		return appsink
	
	def pull_buffers(self, elem):
		"""
		for internal use.  must be called with lock held.
		"""

		while 1:
			if self.time_ordering == 'full':
				# retrieve the timestamps of all elements that
				# aren't at eos and all elements at eos that still
				# have buffers in them
				timestamps = [(t, e) for e, t in self.appsinks.items() if e not in self.at_eos or t is not None]
				# if all elements are at eos and none have buffers,
				# then we're at eos
				if not timestamps:
					return Gst.FlowReturn.EOS
				# find the element with the oldest timestamp.  None
				# compares as less than everything, so we'll find
				# any element (that isn't at eos) that doesn't yet
				# have a buffer (elements at eos and that are
				# without buffers aren't in the list)
				timestamp, elem_with_oldest = min(timestamps)
				# if there's an element without a buffer, quit for
				# now --- we require all non-eos elements to have
				# buffers before proceding
				if timestamp is None:
					return Gst.FlowReturn.OK
				# clear timestamp and pass element to handler func.
				# function call is done last so that all of our
				# book-keeping has been taken care of in case an
				# exception gets raised
				self.appsinks[elem_with_oldest] = None
				self.appsink_new_buffer(elem_with_oldest, self.sink_dict)
	
			elif self.time_ordering == 'partial':
				# retrieve the timestamps of elements of a given channel
				# that aren't at eos and all elements at eos that still
				# have buffers in them
				channel = self.sink_dict[elem][0]
				timestamps = [(t, e) for e, t in self.appsinks.items() if self.sink_dict[e][0] == channel and (e not in self.at_eos or t is not None)]
				# if all elements are at eos and none have buffers,
				# then we're at eos
				if not timestamps:
					return Gst.FlowReturn.EOS
				# find the element with the oldest timestamp.  None
				# compares as less than everything, so we'll find
				# any element (that isn't at eos) that doesn't yet
				# have a buffer (elements at eos and that are
				# without buffers aren't in the list)
				timestamp, elem_with_oldest = min(timestamps)
				# if there's an element without a buffer, quit for
				# now --- we require all non-eos elements to have
				# buffers before proceding
				if timestamp is None:
					return Gst.FlowReturn.OK
				# clear timestamp and pass element to handler func.
				# function call is done last so that all of our
				# book-keeping has been taken care of in case an
				# exception gets raised
				self.appsinks[elem_with_oldest] = None
				self.appsink_new_buffer(elem_with_oldest, self.sink_dict)
			
			elif self.time_ordering == 'none':
				if not elem in self.appsinks:
					return Gst.FlowReturn.EOS
				if self.appsinks[elem] is None:
					return Gst.FlowReturn.OK
				self.appsinks[elem] = None
				self.appsink_new_buffer(elem, self.sink_dict)


####################
# 
#       main
#
####################   

#  
# parsing and setting up some core structures
#

options, filenames = parse_command_line()

data_source_info = multichannel_datasource.DataSourceInfo(options)
instrument = data_source_info.instrument
channels = data_source_info.channel_dict.keys()

# dictionary of basis parameters keyed by ifo, rate
basis_params = {}

if not options.disable_web_service:	
	#
	# create a new, empty, Bottle application and make it the current
	# default, then start http server to serve it up
	#

	bottle.default_app.push()
	# uncomment the next line to show tracebacks when something fails
	# in the web server
	#bottle.app().catchall = False
	import base64, uuid	# FIXME:  don't import when the uniquification scheme is fixed
	httpservers = httpinterface.HTTPServers(
		# FIXME:  either switch to using avahi's native name
		# uniquification system or adopt a naturally unique naming
		# scheme (e.g., include a search identifier and job
		# number).
		service_name = "gstlal_idq (%s)" % base64.urlsafe_b64encode(uuid.uuid4().bytes),
		service_properties = {},
		verbose = options.verbose
	)

	#
	# Set up a registry of the resources that this job provides
	#
	
	@bottle.route("/")
	@bottle.route("/index.html")
	def index(channel_list = channels):
		# get the host and port to report in the links from the
		# request we've received so that the URLs contain the IP
		# address by which client has contacted us
		netloc = bottle.request.urlparts[1]
		server_address = "http://%s" % netloc
		yield "<html><body>\n<h3>%s %s</h3>\n<p>\n" % (netloc, " ".join(sorted(channel_list)))
		for route in sorted(bottle.default_app().routes, key = lambda route: route.rule):
			# don't create links back to this page
			if route.rule in ("/", "/index.html"):
				continue
			# only create links for GET methods
			if route.method != "GET":
				continue
			yield "<a href=\"%s%s\">%s</a><br>\n" % (server_address, route.rule, route.rule)
		yield "</p>\n</body></html>"
	# FIXME:  get service-discovery working, then don't do this
	open("registry.txt", "w").write("http://%s:%s/\n" % (socket.gethostname(), httpservers[0][0].port))

#
# building the event loop and pipeline
#

if options.verbose:
	print >>sys.stderr, "assembling pipeline..."

mainloop = GObject.MainLoop()
pipeline = Gst.Pipeline(sys.argv[0])

# multiple channel src
head = multichannel_datasource.mkbasicmultisrc(pipeline, data_source_info, instrument, verbose = options.verbose)
src = {}
for channel in channels:
	samp_rate = data_source_info.channel_dict[channel]['fsamp']  
	max_samp_rate = min(2048, int(samp_rate))
	min_samp_rate = min(32, max_samp_rate)
	n_rates = int(numpy.log2(max_samp_rate/min_samp_rate) + 1)
	if data_source_info.latency_output:
		head[channel] = pipeparts.mklatency(pipeline, head[channel], name = 'stage2_beforeWhitening_%s' % channel)
	for rate, thishead in idq_multirate_datasource.mkwhitened_multirate_src(pipeline, head[channel], [min_samp_rate*2**i for i in range(n_rates)], int(samp_rate), instrument, channel_name = channel, width=32, quality=1, cutoff=None).items():
		if data_source_info.latency_output:
			thishead = pipeparts.mklatency(pipeline, thishead, name = 'stage3_afterWhitening_%s_%s' % (str(rate).zfill(5), channel))
		if data_source_info.extension == 'ini':
			# use omicron params with .ini files
			flow = data_source_info.channel_dict[channel]['flow']  
			fhigh = data_source_info.channel_dict[channel]['fhigh']  
			qhigh = data_source_info.channel_dict[channel]['qhigh']  
		else:
			# FIXME: don't hardcode q and frequency
			# NOTE: as long as this fudge factor (0.8) for the high frequency cutoff is
			#       less than the factor in the chebychev low pass cutoff in the downsampler
			#       this should be fine
			flow = rate/4.*0.8
			fhigh = rate/2.*0.8
			qhigh = options.qhigh
		qlow = 4
		dur = max([duration(phi, q, 1e-3) for (phi, q) in phi_ql(flow, fhigh, qlow, qhigh, mismatch = options.mismatch)])
		t_arr = numpy.linspace(-dur/2., dur/2., int(dur*rate))
		phase = [0, numpy.pi/2.]
		basis_params[(channel, rate)] = [(phi, q, duration(phi, q, 1e-3)/2.) for (phi, q) in phi_ql(flow, fhigh, qlow, qhigh, mismatch = options.mismatch)]
		thishead = pipeparts.mkqueue(pipeline, thishead, max_size_buffers = 0, max_size_bytes = 0, max_size_time = Gst.SECOND * 30)
		time_domain = (dur*rate**2) < (5*dur*rate*numpy.log2(rate))
		#print >>sys.stderr, len(list(half_sine_gaussian(sine_gaussian(phi, 0, q, t_arr)) for (phi, q) in phi_ql(flow, fhigh, qlow, qhigh, mismatch = 0.2)))
		thishead = pipeparts.mkfirbank(pipeline, thishead, fir_matrix = numpy.array([half_sine_gaussian(sine_gaussian(phi, phi_0, q, t_arr)) for (phi, q) in phi_ql(flow, fhigh, qlow, qhigh, mismatch = options.mismatch) for phi_0 in phase]), time_domain = time_domain, block_stride = int(rate), latency = 0)
		if data_source_info.latency_output:
			thishead = pipeparts.mklatency(pipeline, thishead, name = 'stage4_afterFIRbank_%s_%s' % (str(rate).zfill(5), channel))
		thishead = pipeparts.mkqueue(pipeline, thishead, max_size_buffers = 1, max_size_bytes = 0, max_size_time = 0)
		thishead = pipeparts.mktogglecomplex(pipeline, thishead)
		thishead = pipeparts.mkcapsfilter(pipeline, thishead, caps = "audio/x-raw, format=Z64LE, rate=%i" % rate)
		thishead = pipeparts.mktaginject(pipeline, thishead, "instrument=%s,channel-name=%s" %( instrument, channel))
		thishead = pipeparts.mktrigger(pipeline, thishead, rate, max_snr = True)
		if data_source_info.latency_output:
			thishead = pipeparts.mklatency(pipeline, thishead, name = 'stage5_afterTrigger_%s_%s' % (str(rate).zfill(5), channel))
		src[(channel, rate)] = thishead	

if options.verbose:
	print >>sys.stderr, "attaching appsinks to pipeline..."

handler = MultiChannelHandler(mainloop, pipeline, basis_params = basis_params, description = options.description, out_path = options.out_path, instrument = instrument, keys = src.keys())
appsync = LinkedAppSync(appsink_new_buffer = handler.bufhandler)
appsinks = set(appsync.add_sink(pipeline, src[(channel, rate)], name = "sink_%s_%s" % (rate, channel)) for (channel, rate) in src.keys()) 
  
# Allow Ctrl+C or sig term to gracefully shut down the program for online
# sources, otherwise it will just kill it
if data_source_info.data_source in ("lvshm", "framexmit"):# what about nds online?
	simplehandler.OneTimeSignalHandler(pipeline)

# Seek
if pipeline.set_state(Gst.State.READY) == Gst.StateChangeReturn.FAILURE:
	raise RuntimeError("pipeline failed to enter READY state")
if data_source_info.data_source not in ("lvshm", "framexmit"):# what about nds online?
	datasource.pipeline_seek_for_gps(pipeline, options.gps_start_time, options.gps_end_time)

# run
if pipeline.set_state(Gst.State.PLAYING) == Gst.StateChangeReturn.FAILURE:
	raise RuntimeError("pipeline failed to enter PLAYING state")

if options.verbose:
	print >>sys.stderr, "running pipeline..."

mainloop.run()

if options.verbose:
	print >>sys.stderr, "shutting down pipeline..."

if not options.disable_web_service:
	#
	# Shutdown the web interface servers and garbage collect the Bottle
	# app.  This should release the references the Bottle app's routes
	# hold to the pipeline's data (like template banks and so on).
	#
	
	
	del httpservers
	bottle.default_app.pop()
	

#
# Set pipeline state to NULL and garbage collect the handler
#


if pipeline.set_state(Gst.State.NULL) != Gst.StateChangeReturn.SUCCESS:
	raise RuntimeError("pipeline could not be set to NULL")
del handler.pipeline
del handler

#
# close program manually if data source is live
#


if options.data_source in ("lvshm", "framexmit"):
        sys.exit(0)
