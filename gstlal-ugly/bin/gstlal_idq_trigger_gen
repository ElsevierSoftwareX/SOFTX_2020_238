#!/usr/bin/env python

# Copyright (C) 2017 Sydney J. Chamberlin, Patrick Godwin, Chad Hanna, Duncan Meacher
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.



####################
# 
#     preamble
#
####################   


from optparse import OptionParser
from collections import deque
import os
import sys
import resource
import StringIO

import gi
gi.require_version('Gst', '1.0')
from gi.repository import GObject, Gst
GObject.threads_init()
Gst.init(None)
import lal

import numpy
from gstlal import datasource
from gstlal import idq_multirate_datasource
from gstlal import multichannel_datasource
from gstlal import sngltriggertable
from gstlal import pipeparts
from gstlal import simplehandler
from glue.ligolw import utils as ligolw_utils
from lal import gpstime

#
# Make sure we have sufficient resources
# We allocate far more memory than we need, so this is okay
#

def setrlimit(res, lim):
	hard_lim = resource.getrlimit(res)[1]
	resource.setrlimit(res, (lim if lim is not None else hard_lim, hard_lim))

# set the number of processes and total set size up to hard limit and
# shrink the per-thread stack size (default is 10 MiB)
setrlimit(resource.RLIMIT_NPROC, None)
setrlimit(resource.RLIMIT_AS, None)
setrlimit(resource.RLIMIT_RSS, None)
# FIXME:  tests at CIT show that this next tweak has no effect.  it's
# possible that SL7 has lowered the default stack size from SL6 and we
# don't need to do this anymore.  remove?
setrlimit(resource.RLIMIT_STACK, 1024 * 1024) # 1 MiB per thread

####################
# 
#    functions
#
####################   

#
# construct sine gaussian waveforms that taper to 1e-7 at edges of window
#
def duration(phi, q, tolerance = 1e-7):
        # return the duration of the waveform such that its edges will die out to tolerance of the peak.
	# phi is the central frequency of the frequency band
	return 2.*q/(2.*numpy.pi*phi)*numpy.log(1./tolerance)

def sine_gaussian(phi, phi_0, q, time_arr):
        # edges should be 1e-7 times peak
	dt = time_arr[1]-time_arr[0]
	rate = 1./dt
	assert phi < rate/2. 

	# phi is the central frequency of the sine gaussian
	dur = duration(phi,q)
	tau = q/(2.*numpy.pi*phi)
	sg_vals = numpy.cos(2.*numpy.pi*phi*time_arr + phi_0)*numpy.exp(-1.*time_arr**2./tau**2.)

	# normalize sine gaussians to have unit length in their vector space
	inner_product = numpy.sum(sg_vals*sg_vals)
	norm_factor = 1./(inner_product**0.5)

	return norm_factor*sg_vals 

#
# number of tiles in frequency and Q
#
def N_Q(q_min, q_max, mismatch = 0.05):
        return numpy.ceil(1./(2.*numpy.sqrt(mismatch/3.))*1./numpy.sqrt(2)*numpy.log(q_max/q_min))

def N_phi(phi_min, phi_max, q,  mismatch = 0.05):
       return numpy.ceil(1./(2.*numpy.sqrt(mismatch/3.))*(numpy.sqrt(2.+q**2.)/2.)*numpy.log(phi_max/phi_min))

def Qq(q_min, q_max, mismatch = 0.05):
	N_q = numpy.ceil(N_Q(q_min, q_max, mismatch))
	return [q_min*(q_max/q_min)**((0.5+q)/N_q) for q in range(int(N_q))]

def phi_ql(phi_min, phi_max, q_min, q_max, mismatch = 0.05):
	for q in Qq(q_min, q_max):
		nphi = N_phi(phi_min, phi_max, q)
		for l in range(int(nphi)):
			yield (phi_min*(phi_max/phi_min)**((0.5+l)/nphi), q)

def parse_command_line():

	parser = OptionParser(description = __doc__)

	#
	# First append the datasource common options
	#

	multichannel_datasource.append_options(parser)
	parser.add_option("--out-path", metavar = "path", default = ".", help = "Write to this path. Default = .")
	parser.add_option("--out-file", metavar = "filename", default = "output_triggers.trg", help = "Set the filename in which to save the output.")
	parser.add_option("--cadence", type = "int", default = 32, help = "Rate at which to write trigger files to disk. Default = 32 seconds.")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")

	#
	# parse the arguments and sanity check
	#

	options, filenames = parser.parse_args()

	return options, filenames

####################
# 
#      classes
#
####################   


class MultiChannelHandler(simplehandler.Handler):
	def __init__(self, *args, **kwargs):
		self.durations = kwargs.pop("durations")
		self.out_file = kwargs.pop("out_file")
		self.out_path = kwargs.pop("out_path")
		self.last_save_time = None
		self.cadence = options.cadence
                # create header for trigger file
                #self.header = "# start_time stop_time time frequency unnormalized_energy normalized_energy chisqdof significance channel\n"
                self.header = "# %18s\t%20s\t%20s\t%6s\t%8s\t%8s\t%8s\t%9s\t%8s\t%s\n" % ("start_time", "stop_time", "trigger_time", "phase", "snr", "chisq", "sigmasq", "latency", "rate", "channel")
		super(MultiChannelHandler, self).__init__(*args, **kwargs)

	def do_on_message(self, bus, message):
		return False

	def bufhandler(self, elem, sink_dict):
		buf = elem.emit("pull-sample").get_buffer()
		buftime = int(buf.pts / 1e9)
		if self.last_save_time is None:
			self.last_save_time = int(buftime)
		channel, rate  = sink_dict[elem]
		duration = self.durations[(channel, float(rate))]
		fdata = ""	
		for i in range(buf.n_memory()):
			memory = buf.peek_memory(i)
			result, mapinfo = memory.map(Gst.MapFlags.READ)
			assert result
			# NOTE NOTE NOTE NOTE
			# It is critical that the correct class'
			# .from_buffer() method be used here.  This
			# code is interpreting the buffer's
			# contents as an array of C structures and
			# building instances of python wrappers of
			# those structures but if the python
			# wrappers are for the wrong structure
			# declaration then terrible terrible things
			# will happen
			if mapinfo.data:
				for row in sngltriggertable.GSTLALSnglTrigger.from_buffer(mapinfo.data):
					trigger_time = row.end_time + row.end_time_ns * 1e-9
					current_time = gpstime.gps_time_now().gpsSeconds + gpstime.gps_time_now().gpsNanoSeconds * 1e-9
					latency = numpy.ceil(current_time - buftime)
					start_time = trigger_time - duration/2
					stop_time = trigger_time + duration/2
					fdata += "%20.9f\t%20.9f\t%20.9f\t%6.3f\t%8.3f\t%8.3f\t%8.3f\t%9d\t%8.1f\t%s\n" % (start_time, stop_time, trigger_time, row.phase, row.snr, row.chisq, row.sigmasq, latency, float(rate), channel)
			memory.unmap(mapinfo)

		# Save a "latest"
		if (buftime - self.last_save_time) >= self.cadence:
			self.last_save_time = int(buftime)
		self.to_trigger_file(os.path.join(self.out_path, "%d_%s" % (self.last_save_time, self.out_file)), fdata)

		del buf
		return Gst.FlowReturn.OK

	def to_trigger_file(self, path, data):
		if not os.path.isfile(path):
			data = self.header + data 
		with open(path, 'a') as f:
 			f.write(data)


class LinkedAppSync(pipeparts.AppSync):
	def __init__(self, appsink_new_buffer, sink_dict = {}):
		super(LinkedAppSync, self).__init__(appsink_new_buffer, sink_dict.keys())
		self.sink_dict = sink_dict	
		self.time_ordering = 'full'
	
	def attach(self, appsink):
		"""
		connect this AppSync's signal handlers to the given appsink
		element.  the element's max-buffers property will be set to
		1 (required for AppSync to work).
		"""
		if appsink in self.appsinks:
			raise ValueError("duplicate appsinks %s" % repr(appsink))
		appsink.set_property("max-buffers", 1)
		handler_id = appsink.connect("new-preroll", self.new_preroll_handler)
		assert handler_id > 0
		handler_id = appsink.connect("new-sample", self.new_sample_handler)
		assert handler_id > 0
		handler_id = appsink.connect("eos", self.eos_handler)
		assert handler_id > 0
		self.appsinks[appsink] = None
		_, rate, channel = appsink.name.split("_", 2)
		self.sink_dict.setdefault(appsink, (channel, rate))
		return appsink
	
	def pull_buffers(self, elem):
		"""
		for internal use.  must be called with lock held.
		"""

		while 1:
			if self.time_ordering == 'full':
				# retrieve the timestamps of all elements that
				# aren't at eos and all elements at eos that still
				# have buffers in them
				timestamps = [(t, e) for e, t in self.appsinks.items() if e not in self.at_eos or t is not None]
				# if all elements are at eos and none have buffers,
				# then we're at eos
				if not timestamps:
					return Gst.FlowReturn.EOS
				# find the element with the oldest timestamp.  None
				# compares as less than everything, so we'll find
				# any element (that isn't at eos) that doesn't yet
				# have a buffer (elements at eos and that are
				# without buffers aren't in the list)
				timestamp, elem_with_oldest = min(timestamps)
				# if there's an element without a buffer, quit for
				# now --- we require all non-eos elements to have
				# buffers before proceding
				if timestamp is None:
					return Gst.FlowReturn.OK
				# clear timestamp and pass element to handler func.
				# function call is done last so that all of our
				# book-keeping has been taken care of in case an
				# exception gets raised
				self.appsinks[elem_with_oldest] = None
				self.appsink_new_buffer(elem_with_oldest, self.sink_dict)
	
			elif self.time_ordering == 'partial':
				# retrieve the timestamps of elements of a given channel
				# that aren't at eos and all elements at eos that still
				# have buffers in them
				channel = self.sink_dict[elem][0]
				timestamps = [(t, e) for e, t in self.appsinks.items() if self.sink_dict[e][0] == channel and (e not in self.at_eos or t is not None)]
				# if all elements are at eos and none have buffers,
				# then we're at eos
				if not timestamps:
					return Gst.FlowReturn.EOS
				# find the element with the oldest timestamp.  None
				# compares as less than everything, so we'll find
				# any element (that isn't at eos) that doesn't yet
				# have a buffer (elements at eos and that are
				# without buffers aren't in the list)
				timestamp, elem_with_oldest = min(timestamps)
				# if there's an element without a buffer, quit for
				# now --- we require all non-eos elements to have
				# buffers before proceding
				if timestamp is None:
					return Gst.FlowReturn.OK
				# clear timestamp and pass element to handler func.
				# function call is done last so that all of our
				# book-keeping has been taken care of in case an
				# exception gets raised
				self.appsinks[elem_with_oldest] = None
				self.appsink_new_buffer(elem_with_oldest, self.sink_dict)
			
			elif self.time_ordering == 'none':
				if not elem in self.appsinks:
					return Gst.FlowReturn.EOS
				if self.appsinks[elem] is None:
					return Gst.FlowReturn.OK
				self.appsinks[elem] = None
				self.appsink_new_buffer(elem, self.sink_dict)


####################
# 
#       main
#
####################   

  
# parsing and setting up some core structures
options, filenames = parse_command_line()

data_source_info = multichannel_datasource.DataSourceInfo(options)
instrument = data_source_info.instrument
channels = data_source_info.channel_dict.keys()

# dictionary of durations keyed by ifo, rate
durations = {}

# building the event loop and pipeline
mainloop = GObject.MainLoop()
pipeline = Gst.Pipeline(sys.argv[0])
handler = MultiChannelHandler(mainloop, pipeline, durations = durations, out_file = options.out_file, out_path = options.out_path)

# multiple channel src
head = multichannel_datasource.mkbasicmultisrc(pipeline, data_source_info, instrument, verbose = options.verbose)
src = {}
for channel in channels:
	samp_rate = data_source_info.channel_dict[channel]['fsamp']  
	max_samp_rate = int(samp_rate) 
	min_samp_rate = min(32, max_samp_rate)
	n_rates = int(numpy.log2(max_samp_rate/min_samp_rate) + 1)
	for rate, thishead in idq_multirate_datasource.mkwhitened_multirate_src(pipeline, head[channel], [min_samp_rate*2**i for i in range(n_rates)], instrument, channel_name = channel, width=32, quality=1, cutoff=None).items():
		# FIXME: don't hardcode q and frequency
		flow = rate/4.*0.8
		fhigh = rate/2.*0.8
		qlow = 2
		qhigh = 8
		# omicron params, make sure to use .INI file before uncommenting
		#flow = data_source_info.channel_dict[channel]['flow']  
		#fhigh = data_source_info.channel_dict[channel]['fhigh']  
		#qhigh = data_source_info.channel_dict[channel]['qhigh']  
		dur = max([duration(phi, q, 1e-3) for (phi, q) in phi_ql(flow, fhigh, qlow, qhigh)])
		t_arr = numpy.linspace(-dur/2., dur/2., int(dur*rate))
		#print >>sys.stderr, "dur = %f, rate = %d" % (dur, rate)
		phase = [0, numpy.pi/2.]
		durations[(channel, rate)] = dur
		thishead = pipeparts.mkqueue(pipeline, thishead, max_size_buffers = 0, max_size_bytes = 0, max_size_time = Gst.SECOND * 3)
		thishead = pipeparts.mkfirbank(pipeline, thishead, fir_matrix = numpy.array([sine_gaussian(phi, phi_0, q, t_arr) for (phi, q) in phi_ql(flow, fhigh, qlow, qhigh) for phi_0 in phase]), time_domain = False, block_stride = int(rate), latency = int(rate*dur/2))
		thishead = pipeparts.mkqueue(pipeline, thishead, max_size_buffers = 1, max_size_bytes = 0, max_size_time = 0)
		thishead = pipeparts.mktogglecomplex(pipeline, thishead)
		thishead = pipeparts.mkcapsfilter(pipeline, thishead, caps = "audio/x-raw, format=Z64LE, rate=%i" % rate)
		thishead = pipeparts.mktaginject(pipeline, thishead, "instrument=%s,channel-name=%s" %( instrument, channel) )
		thishead = pipeparts.mktrigger(pipeline, thishead, rate, max_snr = True)
		src[(channel, rate)] = thishead	

if options.verbose:
	print >>sys.stderr, "attaching appsinks to pipeline..."
appsync = LinkedAppSync(appsink_new_buffer = handler.bufhandler)
appsinks = set(appsync.add_sink(pipeline, src[(channel, rate)], name = "sink_%s_%s" % (rate, channel)) for (channel, rate) in src.keys()) 
  
# Allow Ctrl+C or sig term to gracefully shut down the program for online
# sources, otherwise it will just kill it
if data_source_info.data_source in ("lvshm", "framexmit"):# what about nds online?
	simplehandler.OneTimeSignalHandler(pipeline)

# Seek
if pipeline.set_state(Gst.State.READY) == Gst.StateChangeReturn.FAILURE:
	raise RuntimeError("pipeline failed to enter READY state")
if data_source_info.data_source not in ("lvshm", "framexmit"):# what about nds online?
	datasource.pipeline_seek_for_gps(pipeline, options.gps_start_time, options.gps_end_time)

# run
if pipeline.set_state(Gst.State.PLAYING) == Gst.StateChangeReturn.FAILURE:
	raise RuntimeError("pipeline failed to enter PLAYING state")
mainloop.run()
  
   
   
   
   
   
   
   
   
  
 
