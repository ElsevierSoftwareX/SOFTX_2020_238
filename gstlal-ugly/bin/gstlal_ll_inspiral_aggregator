#!/usr/bin/env python
#
# Copyright (C) 2016  Kipp Cannon, Chad Hanna
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


import h5py
import numpy
import sys, os
import itertools
import argparse
import lal
from lal import LIGOTimeGPS
import time
from gstlal import servicediscovery
from gi.repository import GLib
import logging
import subprocess
import threading
from gi.repository import GLib
from gstlal import servicediscovery
import urllib2
import shutil
import collections

MIN_TIME_QUANTA = 1000


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


# Read command line options
def parse_command_line():

	parser = argparse.ArgumentParser(description="Online data aggregator")

	# directory to put everything in
	parser.add_argument("--base-dir", action="store", default="aggregator", help="Specify output path")
	parser.add_argument("--dump-period", type = float, default = 180., help = "Wait this many seconds between dumps of the URLs (default = 180., set to 0 to disable)")
	parser.add_argument("--num-jobs", action="store", type=int, default=10, help="number of running jobs")
	parser.add_argument("--job-tag", help = "Collect URLs for jobs reporting this job tag (default = collect all gstlal_inspiral URLs).")

	args = parser.parse_args()

	return args


#
# =============================================================================
#
#                                 Utility functions
#
# =============================================================================
#


def median(l):
	"""!
	Return the median of a list on nearest value
	"""
	return sorted(l)[len(l)//2]


def now():
	"""!
	A convenience function to return the current gps time
	"""
	return LIGOTimeGPS(lal.UTCToGPS(time.gmtime()), 0)


def get_url(url,d):
	"""!
	A function to pull data from @param url where @param d specifies a
	specific route.  FIXME it assumes that the routes end in .txt
	"""
	jobdata = urllib2.urlopen("%s%s.txt" % (url, d)).read().split("\n")
	jobtime = [float(x.split()[0]) for x in jobdata if x]
	jobdata = [float(x.split()[1]) for x in jobdata if x]
	assert len(jobdata) == len(jobtime)
	return jobtime, jobdata


def reduce_data(xarr, yarr, func, level = 0):
	"""!
	This function does a data reduction by powers of 10
	"""
	datadict = collections.OrderedDict()
	assert len(yarr) == len(xarr)
	for x,y in zip(xarr, yarr):
		# reduce to this level
		key = int(x) // (10**level)
		# we want to sort on y not x
		datadict.setdefault(key, []).append((y,x))
	reduced_data = [func(value) for value in datadict.values()]
	reduced_time = [x[1] for x in reduced_data]
	reduced_data = [x[0] for x in reduced_data]
	assert len(reduced_data) == len(reduced_time)
	idx = numpy.argsort(reduced_time)
	
	return list(numpy.array(reduced_time)[idx]), list(numpy.array(reduced_data)[idx])


def makedir(path):
	"""!
	A convenience function to make new directories and trap errors
	"""
	try:
		os.makedirs(path)
	except IOError:
		pass
	except OSError:
		pass


def create_new_dataset(path, base, timedata = None, data = None, tmp = False):
	"""!
	A function to create a new dataset with time @param timedata and data
	@param data.  The data will be stored in an hdf5 file at path @param path with
	base name @param base.  You can also make a temporary file.
	"""
	if tmp:
		fname = os.path.join(path, "%s.hdf5.tmp" % base)
	else:
		# A non tmp dataset should not be overwritten
		fname = os.path.join(path, "%s.hdf5" % base)
		if os.path.exists(fname):
			return fname
	f = h5py.File(fname, "w")
	if timedata is None and data is None:
		f.create_dataset("time", (0,), dtype="f8")
		f.create_dataset("data", (0,), dtype="f8")
	else:
		if len(timedata) != len(data):
			raise ValueError("time data %d data %d" % (len(timedata), len(data)))
		f.create_dataset("time", (len(timedata),), dtype="f8")
		f.create_dataset("data", (len(data),), dtype="f8")
		f["time"][...] = timedata
		f["data"][...] = data

	f.close()
	return fname


def get_dataset(path, base):
	"""!
	open a dataset at @param path with name @param base and return the data
	"""
	fname = os.path.join(path, "%s.hdf5" % base)
	f = h5py.File(fname, "r")
	x,y = list(f["time"]), list(f["data"])
	f.close()
	return fname, x,y


def gps_to_minimum_time_quanta(gpstime):
	"""!
	given a gps time return the minimum time quanta, e.g., 123456789 ->
	123456000.
	"""
	return int(gpstime) // MIN_TIME_QUANTA * MIN_TIME_QUANTA


def gps_to_leaf_directory(gpstime, level = 0):
	"""!
	get the leaf directory for a given gps time
	"""
	return "/".join(str(gps_to_minimum_time_quanta(gpstime) // MIN_TIME_QUANTA // (10**level)))


def gps_to_sub_directories(gpstime, level, basedir):
	"""!
	return the entire relevant directory structure for a given GPS time
	"""
	root = os.path.join(basedir, gps_to_leaf_directory(gpstime, level))
	out = []
	for i in range(10):
		path = os.path.join(root,str(i))
		if os.path.exists(path):
			out.append(str(i))
	return out


def setup_dirs(gpstime, types, jobs, data, base_dir, verbose = True):
	"""!
	Given a gps time, the number of jobs and data types produce an
	appropriate data structure for storing the hierarchical data.
	"""
	str_time = str(gpstime).split(".")[0]
	digits = [int(x) for x in str_time]
	directories = [numpy.array([digits[x]]) for x in range(7)]

	# Setup the directory structure and put in empty files
	for dirs in [directories[:i+1] for i in range(len(directories))]:
		for path in itertools.product(*dirs):
			cur_dir = os.path.join(base_dir, "/".join(str(x) for x in path))
			makedir(cur_dir)
			for (typ,_) in types:
				type_dir = os.path.join(cur_dir, typ)
				makedir(type_dir)
				for d in data:
					create_new_dataset(type_dir, d)
			type_dir = os.path.join(cur_dir, "by_job")
			for b in jobs:
				bin_dir = os.path.join(type_dir, b)
				for (typ,_) in types:
					type_bin_dir = os.path.join(bin_dir, typ)
					makedir(type_bin_dir)
					for d in data:
						create_new_dataset(type_bin_dir, d)


def gps_range(jobtime, dataspan):
	gpsblocks = set((gps_to_minimum_time_quanta(t) for t in jobtime))
	min_t, max_t = min(gpsblocks), max(gpsblocks)
	for gpstime in gpsblocks:
		dataspan.add(gpstime)
	return range(min_t, max_t+MIN_TIME_QUANTA, MIN_TIME_QUANTA), range(min_t+MIN_TIME_QUANTA, max_t+2*MIN_TIME_QUANTA, MIN_TIME_QUANTA)


def update_lowest_level_data(job, path, d, s, e, typ, self, jobtime, jobdata, func):
	try:
		fname, prev_times, prev_data = get_dataset(path, d)
	except:
		setup_dirs(s, self.datatypes, self.jobs, self.dataurls, self.base_dir)
		fname, prev_times, prev_data = get_dataset(path, d)
	# only get new data
	if prev_times:
		this_time_ix = [i for i,t in enumerate(jobtime) if s <= t < e and t > prev_times[-1]]
	else:
		this_time_ix = [i for i,t in enumerate(jobtime) if s <= t < e]
	this_time = [jobtime[i] for i in this_time_ix] + prev_times
	this_data = [jobdata[i] for i in this_time_ix] + prev_data
	reduced_time, reduced_data = reduce_data(this_time, this_data, func, level = 0)
	logging.info("processing job %s for data %s in span [%d,%d] of type %s: found %d" % (job, d, s, e, typ, len(reduced_time)))
	tmpfname = create_new_dataset(path, d, reduced_time, reduced_data, tmp = True)
	# copy the tmp file over the original
	shutil.move(tmpfname, fname)


def job_expanse(dataspan):
	if dataspan:
		min_t, max_t = min(dataspan), max(dataspan)
		return range(min_t, max_t+MIN_TIME_QUANTA, MIN_TIME_QUANTA), range(min_t+MIN_TIME_QUANTA, max_t+2*MIN_TIME_QUANTA, MIN_TIME_QUANTA)
	else:
		return [], []

def reduce_data_from_lower_level(level, self, this_level_dir, job, typ, d, func, s, e):

	agg_data = []
	agg_time = []
	for subdir in gps_to_sub_directories(s, level, self.base_dir):
		path = "/".join([this_level_dir, subdir, "by_job", job, typ])
		try:
			fname, x, y = get_dataset(path, d)
			agg_time += x
			agg_data += y
		except IOError as ioerr:
			logging.error(ioerr)
			pass
	reduced_time, reduced_data = reduce_data(agg_time, agg_data, func, level=level)
	path = "/".join([this_level_dir, "by_job", job, typ])
	logging.info("processing reduced data %s for job %s  in span [%d,%d] of type %s at level %d: found %d/%d" % (d, job, s, e, typ, level, len(reduced_time), len(agg_time)))
	tmpfname = create_new_dataset(path, d, reduced_time, reduced_data, tmp = True)
	# FIXME don't assume we can get the non temp file name this way
	shutil.move(tmpfname, tmpfname.replace(".tmp",""))


def reduce_across_jobs(self, this_level_dir, typ, d, func, level, s, e):
	# Process this level
	agg_data = []
	agg_time = []
	for job  in sorted(self.urls):
		path = "/".join([this_level_dir, "by_job", job, typ])
		try:
			fname, x, y = get_dataset(path, d)
			agg_time += x
			agg_data += y
		except IOError as ioerr:
			logging.error(ioerr)
			pass
	reduced_time, reduced_data = reduce_data(agg_time, agg_data, func, level=level)
	logging.info("processing reduced data %s in span [%d,%d] of type %s at level %d: found %d/%d" % (d, s, e, typ, level, len(reduced_time), len(agg_time)))
	path = "/".join([this_level_dir, typ])
	tmpfname = create_new_dataset(path, d, reduced_time, reduced_data, tmp = True)
	# FIXME don't assume we can get the non temp file name this way
	shutil.move(tmpfname, tmpfname.replace(".tmp",""))
#
# =============================================================================
#
#                               Internal Library
#
# =============================================================================
#


class Collector(servicediscovery.Listener):
	def __init__(self, mainloop, datatypes, jobs, dataurls, base_dir, job_tag = None, dump_period = 180.):
		self.datatypes = datatypes
		self.jobs = set(jobs)
		self.dataurls = dataurls
		self.base_dir = base_dir
		self.job_tag = job_tag
		self.dump_period = dump_period
		self.urls = {}
		self.lock = threading.Lock()
		# FIXME:  use glib's mainloop machinery instead, implement
		# this as a timeout or whatever it's called
		logging.info("starting wget loop thread")
		self.wget_thread = threading.Thread(target = self.wget_loop, args = (mainloop,))
		self.wget_thread.start()

	def add_service(self, sname, stype, sdomain, host, port, properties):
		if stype != "_http._tcp" or not sname.startswith("gstlal_inspiral "):
			return
		url = "http://%s:%s/" % (host, port)
		logging.info("found '%s' server at %s for job tag '%s'" % (sname, url, properties.get("job_tag")))
		if self.job_tag is not None and properties.get("job_tag") != self.job_tag:
			logging.info("server has wrong or missing job tag, discarding")
			return
		if not properties.get("GSTLAL_LL_JOB"):
			logging.info("server has no GSTLAL_LL_JOB value, discarding")
			return
		if properties.get("GSTLAL_LL_JOB") not in self.jobs:
			logging.info("server has a GSTLAL_LL_JOB value outside of requested range, discarding")
			return
		# watch for security problems:  don't let url or job ID
		# terminate the wget shell command in mid-string
		if ";" in url or ";" in properties["GSTLAL_LL_JOB"]:
			logging.warn("invalid URL and/or job ID")
			return
		logging.info("recording server at %s for GSTLAL_LL_JOB='%s'" % (url, properties["GSTLAL_LL_JOB"]))
		with self.lock:
			self.urls[properties["GSTLAL_LL_JOB"]] = url

	def wget_loop(self, mainloop):
		try:
			while self.dump_period:
				logging.info("sleeping")
				time.sleep(self.dump_period)
				with self.lock:
					dataspan = set()
					for job, url in sorted(self.urls.items()):
						assert job
						# FIXME Hack
						url = url.replace(".local","")
						for d in self.dataurls:
							jobtime, jobdata = get_url(url,d)
							gps1, gps2 = gps_range(jobtime, dataspan)
							for s,e in zip(gps1, gps2):
								for (typ, func) in self.datatypes:
									path = "/".join([self.base_dir, gps_to_leaf_directory(s), "by_job", job, typ])
									update_lowest_level_data(job, path, d, s, e, typ, self, jobtime, jobdata, func)

					# Data reduction across jobs at the lowest level
					gps1, gps2 = job_expanse(dataspan)
					for s,e in zip(gps1, gps2):
						# FIXME don't hardcode this range
						for level in range(7):
							this_level_dir = "/".join([self.base_dir, gps_to_leaf_directory(s, level = level)])
							for d in self.dataurls:
								for (typ,func) in self.datatypes:
									# Produce the data at this level by descending a level.
									if level > 0:
										for job in sorted(self.urls):
											reduce_data_from_lower_level(level, self, this_level_dir, job, typ, d, func, s, e)
									# reduce data across jobs at this level
									reduce_across_jobs(self, this_level_dir, typ, d, func, level, s, e)
		except:
			mainloop.quit()
			raise

	def quit(self):
		logging.info("waiting for wget loop to finish ...")
		self.dump_period = 0	# signal exit
		self.wget_thread.join()


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


options = parse_command_line()

# FIXME don't hardcode some of these?
datatypes = [("min", min), ("max", max), ("median", median)]
jobs = ["%04d" % b for b in numpy.arange(0, options.num_jobs)]
dataurls = ["latency_history", "snr_history"]


logging.basicConfig(level = logging.INFO, format = "%(asctime)s %(levelname)s:%(processName)s(%(process)d):%(funcName)s: %(message)s")


mainloop = GLib.MainLoop()

collector = Collector(mainloop, datatypes, jobs, dataurls, options.base_dir, job_tag = options.job_tag, dump_period = options.dump_period)
browser = servicediscovery.ServiceBrowser(collector)

try:
	mainloop.run()
except:
	collector.quit()
	raise


#
# always end on an error so that condor won't think we're done and will
# restart us
#


sys.exit(1)
