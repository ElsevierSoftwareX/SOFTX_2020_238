#!/usr/bin/env python
#
# Copyright (C) 2016  Kipp Cannon, Chad Hanna
# Copyright (C) 2019  Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


import argparse
import json
import logging
from multiprocessing import Pool
import sys, os
import time
import timeit

import numpy

from datamon import aggregator
from datamon import io


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


# Read command line options
def parse_command_line():

	parser = argparse.ArgumentParser(description="Online data aggregator")

	# directory to put everything in
	parser.add_argument("--base-dir", action="store", default="aggregator", help="Specify output path")
	parser.add_argument("--job-start", type=int, help="job id to start aggregating from")
	parser.add_argument("--route", action="append", help="Specify routes to download. Can be given multiple times.")
	parser.add_argument("--data-type", action="append", help="Specify datatypes to aggregate from 'min', 'max', 'median'. Can be given multiple times. Default all")
	parser.add_argument("--dump-period", type = float, default = 1., help = "Wait this many seconds between dumps of the  URLs (default = 1., set to 0 to disable)")
	parser.add_argument("--num-jobs", action="store", type=int, default=10, help="number of running jobs")
	parser.add_argument("--job-tag", help = "Collect URLs for jobs reporting this job tag (default = collect all gstlal_inspiral URLs).")
	parser.add_argument("--num-threads", type = int, default = 16, help = "Number of threads to use concurrently, default 16.")
	parser.add_argument("--kafka-server", action="store", help="Specify kakfa server to read data from, example: 10.14.0.112:9092")
	parser.add_argument("--data-backend", default="hdf5", help = "Choose the backend for data to be stored into, options: [hdf5|influx]. default = hdf5.")
	parser.add_argument("--influx-hostname", help = "Specify the hostname for the influxDB database. Required if --data-backend = influx.")
	parser.add_argument("--influx-port", help = "Specify the port for the influxDB database. Required if --data-backend = influx.")
	parser.add_argument("--influx-database-name", help = "Specify the database name for the influxDB database. Required if --data-backend = influx.")

	args = parser.parse_args()

	#FIXME do error checking
	if args.data_type is None:
		args.data_type = ["min", "max", "median"]

	assert args.data_backend in ('hdf5', 'influx'), '--data-backend must be one of [hdf5|influx]'

	return args


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#

if __name__ == '__main__':
	options = parse_command_line()

	# FIXME don't hardcode some of these?
	jobs = ["%04d" % b for b in numpy.arange(options.job_start, options.job_start + options.num_jobs)]
	routes = options.route

	logging.basicConfig(level = logging.INFO, format = "%(asctime)s %(levelname)s:%(processName)s(%(process)d):%(funcName)s: %(message)s")

	pool = Pool(options.num_threads)

	# We instantiate multiple consumers (based on --num-threads) to subscribe to all of our topics, i.e., jobs
	if options.kafka_server:
		from kafka import KafkaConsumer
		consumer = KafkaConsumer(*jobs, bootstrap_servers=[options.kafka_server], value_deserializer=lambda m: json.loads(m.decode('utf-8')), auto_offset_reset='latest', group_id='%s_%s_aggregator' % (routes[0], options.data_type[0]), max_poll_interval_ms = 60000)
	else:
		consumer = None

        # set up aggregator sink
	if options.data_backend == 'influx':
		agg_sink = io.influx.InfluxDBAggregator(hostname=options.influx_hostname, port=options.influx_port, db=options.influx_database_name)
	else: ### hdf5 data backend
		agg_sink = io.hdf5.HDF5Aggregator(rootdir=options.base_dir, num_processes=options.num_threads)

	# start an infinite loop to keep updating and aggregating data
	while True:
		logging.info("sleeping for %.1f s" % options.dump_period)
		time.sleep(options.dump_period)

		if consumer:
			# this is not threadsafe!
			logging.info("retrieving data from kafka")
			start = timeit.default_timer()
			datadata = io.kafka.retrieve_timeseries(consumer, jobs, routes, max_records = 2 * len(jobs) * len(routes))
			elapsed = timeit.default_timer() - start
			logging.info("time to retrieve data: %.1f s" % elapsed)
		else:
			logging.info("retrieving data from bottle routes")
			datadata = io.http.retrieve_timeseries(options.base_dir, jobs, routes, options.job_tag, num_threads=options.num_threads)

		# store and reduce data for each job
		start = timeit.default_timer()
		for route in routes:
			logging.info("storing and reducing timeseries for measurement: %s" % route)
			agg_sink.store_and_reduce(route, datadata[route], 'data', tags='job', aggregates=options.data_type)
		elapsed = timeit.default_timer() - start
		logging.info("time to store/reduce timeseries: %.1f s" % elapsed)

	#
	# always end on an error so that condor won't think we're done and will
	# restart us
	#

	sys.exit(1)
