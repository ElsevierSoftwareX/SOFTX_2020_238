#!/usr/bin/env python
#
# Copyright (C) 2011  Chad Hanna
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
This program makes a dag for ER1
"""

__author__ = 'Chad Hanna <channa@caltech.edu>'

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import subprocess, socket, tempfile, shutil

##############################################################################
# import the modules we need to build the pipeline
from glue import iterutils
from glue import pipeline
from glue import lal
from glue.ligolw import lsctables
from glue import segments
from glue.ligolw import array
import glue.ligolw.utils as utils
import glue.ligolw.utils.segments as ligolw_segments
from optparse import OptionParser
from gstlal import inspiral
from gstlal import inspiral_pipe
import numpy


#
# gstlal_inspiral
#


class gstlal_inspiral_job(pipeline.CondorDAGJob):
	"""
	A gstlal_inspiral job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_ll_inspiral'), tag_base='gstlal_inspiral'):
		"""
		"""
		self.__prog__ = 'gstlal_inspiral'
		self.__executable = executable
		self.__universe = 'vanilla'
		pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
		self.add_condor_cmd('getenv','True')
		# these jobs gracefully shutdown with SIGINT
		self.add_condor_cmd("want_graceful_removal", "True")
		self.add_condor_cmd("kill_sig", "15")
		self.add_condor_cmd('+Online_CBC_GSTLAL', 'True')
		self.tag_base = tag_base
		self.set_sub_file(tag_base+'.sub')
		self.set_stdout_file('logs/'+tag_base+'-$(macrojobtag)-$(macronodename)-$(cluster)-$(process).out')
		self.set_stderr_file('logs/'+tag_base+'-$(macrojobtag)-$(macronodename)-$(cluster)-$(process).err')
		self.number = 1

class gstlal_inspiral_marginalize_likelihoods_online_job(pipeline.CondorDAGJob):
	"""
	A gstlal_inspiral_marginalize_likelihoods_online job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_inspiral_marginalize_likelihoods_online'), tag_base='gstlal_inspiral_marginalize_likelihoods_online'):
		"""
		"""
		self.__prog__ = 'gstlal_inspiral_marginalize_likelihoods_online'
		self.__executable = executable
		self.__universe = 'local'
		pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
		self.add_condor_cmd('getenv','True')
		# these jobs gracefully shutdown with SIGINT
		self.add_condor_cmd("want_graceful_removal", "True")
		self.add_condor_cmd("kill_sig", "15")
		self.tag_base = tag_base
		self.set_sub_file(tag_base+'.sub')
		self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(macronodename)-$(cluster)-$(process).out')
		self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(macronodename)-$(cluster)-$(process).err')

class gstlal_inspiral_marginalize_likelihoods_online_node(pipeline.CondorDAGNode):
	"""
	gstlal_inspiral_marginalize_likelihoods_onlinenode
	"""
	def __init__(self, job, dag, output):
		pipeline.CondorDAGNode.__init__(self,job)
		self.add_var_arg("%s %s" % (os.getcwd(), output))
		dag.add_node(self)


class gstlal_inspiral_node(pipeline.CondorDAGNode):
	"""
	A gstlal_inspiral node
	"""
	#FIXME add frame segments, name and veto segments name
	def __init__(self, job, dag, channel_dict, reference_psd, svd_bank, tmp_space=inspiral_pipe.log_path(), ht_gate_thresh=10.0, control_peak_time = 5, fir_stride = 5, thinca_interval = 10, likelihood_file = None, marginalized_likelihood_file = None, gracedb_far_threshold = None, gracedb_group = None, gracedb_type = None, injections = None, fake_data = None, copy_likelihoods = False, veto_segments_file = None, veto_segments_name = None, p_node=[]):

		pipeline.CondorDAGNode.__init__(self,job)
		self.add_var_opt("channel-name", inspiral.pipeline_channel_list_from_channel_dict(channel_dict))
		if reference_psd is not None:
			self.add_var_opt("reference-psd", reference_psd)
		self.add_var_opt("svd-bank", svd_bank)
		self.add_var_opt("tmp-space", tmp_space)
		self.add_var_opt("track-psd", "")
		self.add_var_opt("control-peak-time", control_peak_time)
		self.add_var_opt("fir-stride", fir_stride)
		self.add_var_opt("thinca-interval", thinca_interval)
		self.add_var_opt("job-tag", "%04d" % (job.number,))
		self.add_macro("macrojobtag", "%04d" % (job.number,))
		if ht_gate_thresh is not None:
			self.add_var_opt("ht-gate-threshold", ht_gate_thresh)
		if gracedb_far_threshold is not None:
			self.add_var_opt("gracedb-far-threshold", gracedb_far_threshold)
		if gracedb_group is not None:
			self.add_var_opt("gracedb-group", gracedb_group)
		if gracedb_type is not None:
			self.add_var_opt("gracedb-type", gracedb_type)
		if fake_data is not None:
			self.add_var_opt("fake-data", fake_data)
		if injections is not None:
			self.add_var_opt("injections", injections)
		if veto_segments_file is not None:
			self.add_var_opt("veto-segments-file", veto_segments_file)
		if veto_segments_name is not None:
			self.add_var_opt("veto-segments-name", veto_segments_name)
		# self.add_var_opt("verbose", "") #Put this in for debugging
		# FIXME, the way the likelihood file is handled here is a mess.
		likefile = os.path.split(likelihood_file)[1]
		path = os.getcwd()
		# make a new likelihood file
		likefile = "%s/%04d_%s" % (path, job.number, likefile)
		if copy_likelihoods:
			shutil.copyfile(likelihood_file, likefile)
		self.add_var_opt("likelihood-file", likefile)
		self.add_var_opt("marginalized-likelihood-file", marginalized_likelihood_file)
		svd_bank = os.path.split(svd_bank)[1].replace('.xml','')
		job.number += 1
		for p in p_node:
			self.add_parent(p)
		dag.add_node(self)


#
# Parse the command line
#


def parse_command_line():
	parser = OptionParser(description = __doc__)
	parser.add_option("--reference-psd", metavar = "filename", help = "Set the reference psd file.")
	parser.add_option("--bank-cache", metavar = "filenames", help = "Set the bank cache files in format H1=H1.cache,H2=H2.cache, etc..")
	parser.add_option("--channel", metavar = "name", default=[], action = "append", help = "Set the name of the channel to process (optional).  The default is \"LSC-STRAIN\" for all detectors. Override with IFO=CHANNEL-NAME can be given multiple times")
	parser.add_option("--ht-gate-threshold", metavar = "float", help = "Set the h(t) gate threshold to reject glitches", type="float")
	parser.add_option("--do-iir-pipeline", action = "store_true", help = "run the iir pipeline instead of lloid")
	parser.add_option("--num-banks", metavar = "str", help = "the number of banks per job. can be given as a list like 1,2,3,4 then it will split up the bank cache into N groups with M banks each.")
	parser.add_option("--max-jobs", metavar = "num", type = "int", help = "stop parsing the cache after reaching a certain number of jobs")
	parser.add_option("--likelihood-file", help = "set the likelihood file, required if --copy-likelihoods is used")	
	parser.add_option("--marginalized-likelihood-file", help = "set the marginalized likelihood file, required")	
	parser.add_option("--control-peak-time", default = 5, metavar = "secs", help = "set the control peak time, default 5")
	parser.add_option("--fir-stride", default = 5, metavar = "secs", help = "set the fir bank stride, default 5")
	parser.add_option("--thinca-interval", default = 10, metavar = "secs", help = "set the thinca interval, default 10")
	parser.add_option("--gracedb-far-threshold", type = "float", help = "false alarm rate threshold for gracedb (Hz), if not given gracedb events are not sent")
	parser.add_option("--gracedb-type", default = "LowMass", help = "gracedb type, default LowMass")
	parser.add_option("--gracedb-group", default = "Test", help = "gracedb group, default Test")
	parser.add_option("--fake-data", metavar = "[LIGO|AdvLIGO]", help = "Instead of reading data from .gwf files, generate and process coloured Gaussian noise.")
	parser.add_option("--copy-likelihoods", action = "store_true", help = "Copy the likelihood files from a seed, must give --likelihood-file")
	parser.add_option("--injections", metavar = "filename", help = "Set the name of the LIGO light-weight XML file from which to load injections (optional).")
	parser.add_option("--veto-segments-file", metavar = "filename", help = "Set the name of the LIGO light-weight XML file from which to load vetoes (optional).")
	parser.add_option("--veto-segments-name", metavar = "name", help = "Set the name of the segments to extract from the segment tables and use as the veto list.", default = "vetoes")
	options, filenames = parser.parse_args()
	options.num_banks = [int(v) for v in options.num_banks.split(",")]

	fail = ""
	for option in ("bank_cache", "gracedb_far_threshold"):
		if getattr(options, option) is None:
			fail += "must provide option %s\n" % (option)
	if fail: raise ValueError, fail

	if options.copy_likelihoods and options.likelihood_file is None:
		raise ValueError("Must include --likelihood-file when giving --copy-likelihoods")

	#FIXME add consistency check?
	bankcache = inspiral_pipe.parse_cache_str(options.bank_cache)
	channel_dict = inspiral.channel_dict_from_channel_list(options.channel)
	
	return options, filenames, bankcache, channel_dict


###############################################################################
# MAIN
###############################################################################

options, filenames, bank_cache, channel_dict = parse_command_line()

try: os.mkdir("logs")
except: pass
dag = inspiral_pipe.DAG("trigger_pipe")

#
# setup the job classes
#

if options.do_iir_pipeline is not None:
	gstlalInspiralJob = gstlal_inspiral_job(executable = inspiral_pipe.which('gstlal_iir_ll_inspiral'))
else:
	gstlalInspiralJob = gstlal_inspiral_job()

# A local universe job that will run in a loop marginalizing all of the likelihoods
margJob = gstlal_inspiral_marginalize_likelihoods_online_job()
margNode = gstlal_inspiral_marginalize_likelihoods_online_node(margJob, dag, options.marginalized_likelihood_file) 

###############################################################################
# loop over banks to run gstlal inspiral pre clustering and far computation
###############################################################################

for s, trials_factor in inspiral_pipe.build_bank_string(bank_cache, options.num_banks, options.max_jobs):
	gstlal_inspiral_node(gstlalInspiralJob, dag, channel_dict, reference_psd=options.reference_psd, svd_bank=s, ht_gate_thresh = options.ht_gate_threshold, fir_stride = options.fir_stride, thinca_interval = options.thinca_interval, likelihood_file = options.likelihood_file, marginalized_likelihood_file = options.marginalized_likelihood_file, control_peak_time = options.control_peak_time, gracedb_far_threshold = options.gracedb_far_threshold, gracedb_group = options.gracedb_group, gracedb_type = options.gracedb_type, injections = options.injections, fake_data = options.fake_data, copy_likelihoods = options.copy_likelihoods, veto_segments_file = options.veto_segments_file, veto_segments_name = options.veto_segments_name)


dag.write_sub_files()
# we probably want these jobs to retry indefinitely on dedicated nodes. A user
# can intervene and fix a problem without having to bring the dag down and up.
# There are few enough total jobs that this really shouldn't bog down the
# scheduler. For now 1000 will be considered indefinite
[node.set_retry(1000) for node in dag.get_nodes()]
dag.write_dag()
dag.write_script()
dag.write_cache()
