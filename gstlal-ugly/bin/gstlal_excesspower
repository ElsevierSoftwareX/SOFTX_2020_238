#!/usr/bin/env python
#
# Copyright (C) 2011 Chris Pankow
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
"""Stream-based burst analysis tool"""

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


import os
import numpy
import signal

from optparse import OptionParser
import ConfigParser
from ConfigParser import SafeConfigParser

from gstlal.pipeutil import gst, mkelem
from gstlal.pipeparts import *
from gstlal.pipeio import parse_spectrum_message
from gstlal.lloidparts import LLOIDHandler, DetectorData, mkLLOIDbasicsrc
from gstlal.reference_psd import write_psd, read_psd

import gstlal.excesspower as ep
from gstlal.excesspower import *
from gstlal.inspiral import add_cbc_metadata

from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import utils
from glue.ligolw.utils import process as ligolw_process

from glue.segments import segment
from glue import gpstime
from glue.lal import LIGOTimeGPS

from pylal.xlal.datatypes.snglburst import from_buffer as sngl_bursts_from_buffer
from pylal.xlal.datatypes.real8frequencyseries import REAL8FrequencySeries

#
# =============================================================================
#
#                                Unit Handling
#
# =============================================================================
#

EXCESSPOWER_UNIT_SCALE = {
	'Hz':  1024**0,
	'kHz': 1024**1,
	'MHz': 1024**2,
	'GHz': 1024**3,
	'mHz': 1024**-1,
	'uHz': 1024**-2,
	'nHz': 1024**-3,
}

#
# =============================================================================
#
#                                Handler Class
#
# =============================================================================
#

class EPHandler( LLOIDHandler ):
	"""
	Handler class for the excess power pipeline. Keeps various bits of information that the pipeline emits and consumes. This is also in charge of signalling the rebuild of various matrices and vectors needed by the pipeline.
	"""
	def __init__( self, mainloop, pipeline ):

		# Instrument and channel
		self.inst = None
		self.channel = None

		# Book keeping
		self.start = 0
		self.stop = -1
		# How long of a time to ignore output from the whitener stabilizing
		# FIXME: This should be ~number of PSDs stored in history * fft_length
		# Right now that's probably something like 80 s.
		self.whitener_offset = 20

		# Keep track of units -- enable us to go below rates of 1 Hz
		self.units = EXCESSPOWER_UNIT_SCALE['Hz']

		# Defaults -- Time-frequency map settings
		self.base_band = 16
		self.flow = 64 
		self.fhigh = 1000
		self.fft_length = 8 # s

		# Defaults -- Resolution settings
		self.rate = 2048
		#self.rate = 4096
		self.max_level = 1

		# Defaults -- filtering
		self.filter_len = 2*int(2*self.rate/self.base_band)
		self.filter_bank = None
		# TODO: Maybe not necessary
		self.firbank = None

		# Defaults -- PSD settings
		# This is used to store the previous value of the PSD power
		self.psd_power = 0
		self.cache_psd = False
		self.psd_change_thresh = 0.5 # fifty percent

		# Defaults -- Two-point spectral correlation settings
		self.cache_spec_corr = False

		# Defaults -- mixer matrices
		self.chan_matrix = None
		self.mmixers = {}

		# Defaults -- data products
		self.output = True
		self.triggers = None
		self.outfile = "test.xml"
		self.make_output_table()
		self.snr_thresh = 5.5
		self.fap = None
		self.dump_frequency = 600 # s
		self.max_events = 1e4
		self.time_since_dump = self.start

		self.trigger_segment = None

		self.spec_corr = self.build_default_correlation( self.rate )
		"""
		self.psd = self.build_default_psd( self.rate, self.filter_len )
		self.rebuild_filter()
		self.rebuild_chan_mix_matrix()
		"""

		# required for file locking
		self.lock = threading.Lock()

		self.bus = pipeline.get_bus()
		self.bus.add_signal_watch()
		self.bus.connect( "message", self.process_message )

		self.pipeline = pipeline

		super(type(self), self).__init__(mainloop, pipeline)

	def set_trigger_time_and_action( self, trig_seg, action=["psd"] ):
		"""
		Inform the handler of a specific time of interest.
		"""
		# TODO: Bounds checking
		self.trigger_segment = trig_seg

		# TODO: Handle only specific action requests

	def process_message( self, bus, message ):
		"""
		Process a message from the bus. Depending on what it is, we may drop various data products on to disk, or perform various other actions.
		"""
		if( message.type == gst.MESSAGE_EOS ):
			self.shutdown(None, None)
			return
		elif( message.type == gst.MESSAGE_LATENCY ):
			print >>sys.stderr, "Got latency message, ignoring for now."
			return
		elif( message.structure is None ): 
			print >>sys.stderr, "Got message with type: %s ...but no handling logic, so ignored."
			return

		# TODO: Move this to PSD difference checker
		if( message.structure.get_name() == "spectrum" ):
			# FIXME: Units
			ts = message.structure[ "timestamp" ]*1e-9
			if( self.trigger_segment is not None 
				and ts in self.trigger_segment ):
				self.dump_psd( timestamp = ts )
			elif( self.cache_psd ): 
				self.dump_psd( timestamp = ts )

	def dump_psd( self, filename=None, timestamp=None ):
		"""
		Dump the currently cached PSD to a LIGOLW XML file. If the filename isn't specified, it will be constructed from metadata.
		"""
		if( filename is None ):
			if( os.path.isdir( handler.cache_psd ) ):
				filename = "%s/%s-%s_PSD_%d.xml" % ( handler.cache_psd, handler.inst, handler.channel.replace("-","_"), round(timestamp) )
			else:
				filename = handler.cache_psd
		write_psd( filename, { self.inst: self.psd } )

	def add_firbank( self, firbank ):
		"""
		Set the main base band FIR bank, and build the FIR matrix.
		"""
		self.firbank = firbank
		firbank.set_property( "fir-matrix", self.rebuild_filter() )

	def add_matmixer( self, mm, res_level ):
		self.mmixers[ res_level ] = mm
		self.rebuild_matrix_mixers( res_level )

	def build_default_psd( self, rate, filter_len ):
		"""
		Builds a dummy PSD to use until we get the right one.
		"""
		psd = REAL8FrequencySeries()
		psd.deltaF = float(rate)/filter_len
		psd.data = numpy.ones( filter_len/2 + 1 ) #/ 2 / psd.deltaF
		psd.f0 = 0
		self.psd = psd
		return psd

	def build_default_correlation( self, rate ):
		"""
		Builds a Kronecker delta correlation series for k, k'.
		"""
		corr = numpy.zeros(rate + 1)
		corr[0] = 1
		return corr

	def rebuild_matrix_mixers( self, res_level = None ):
		"""
		Rebuilds the matrix mixer matrices from the coefficients calculated in rebuild_chan_mix_matrix and assigns them to their proper element.
		"""
		for i, mm in self.mmixers.iteritems():
			if( res_level != None and res_level != i ): continue

			nchannels = self.filter_bank.shape[0]
			up_factor = int(numpy.log2(nchannels/(nchannels >> i)))
			cmatrix = ep.build_chan_matrix( 
				nchannels = nchannels,
				up_factor = up_factor,
				norm = self.chan_matrix[i] 
			)
			mm.set_property( "matrix", cmatrix )

	def rebuild_filter( self ):
		"""
		Calling this function rebuilds the filter FIR banks and assigns them to their proper element. This is normally called when the PSD or spectrum correlation changes.
		"""
		self.filter_bank = ep.build_filter( fhigh = self.fhigh, flow=self.flow, rate=self.rate, psd = self.psd, corr = self.spec_corr, b_wind = self.base_band )
		return self.filter_bank

	def build_filter_xml( self, res_level, loc="" ):
		"""
		Calls the EP library to create a XML of sngl_burst tables representing the filter banks. At the moment, this dumps them to the current directory, but this can be changed by supplying the 'loc' argument. The written filename is returned for easy use by the trigger generator.
		"""
		self.filter_xml = ep.create_bank_xml(
			self.flow,
			self.fhigh,
			self.base_band*(res_level+1),
			# FIXME: Is there a factor of two here? -- No, remove the factor of two...
			1.0 / (2*self.base_band*(res_level+1)), # resolution level starts from 0
			self.inst
		)
		output = "%sgstlal_excesspower_bank_%s_%s_level_%d.xml" % (loc, self.inst, self.channel, res_level)
		utils.write_filename( self.filter_xml, output, verbose = True,
		       gz = (output or "stdout").endswith(".gz") )
		return output

	def destroy_filter_xml( self, loc="" ):
		import glob
		for f in glob.glob( "%s/gstlal_excesspower_bank_%s_level_*.xml" % (loc, self.inst) ):
			os.rm( f )

	def rebuild_chan_mix_matrix( self ):
		"""
		Calling this function rebuilds the matrix mixer coefficients for higher resolution components. This is normally called when the PSD or spectrum correlation changes.
		"""
		self.chan_matrix = ep.build_inner_product_norm( 
			corr = self.spec_corr, 
			band = self.base_band, 
			del_f = self.psd.deltaF,
			nfilts = len(self.filter_bank),
			flow = self.flow,
			# TODO: PSD option to lalburst IP doesn't work
			#psd = self.psd
			max_level = self.max_level
		)
		return self.chan_matrix

	def rebuild_everything( self ):
		"""
		Top-level function to handle the asynchronous updating of FIR banks and matrix mixer elements.
		"""
		# Rebuild filter bank and hand it off to the FIR element
		#print >> sys.stderr, "Rebuilding FIR bank"
		self.firbank.set_property( "fir_matrix", self.rebuild_filter() )

		#print >> sys.stderr, "Rebuilding matrix mixer"
		self.rebuild_chan_mix_matrix()
		# Rebuild the matrix mixer with new normalization coefficients
		self.rebuild_matrix_mixers()

	def make_output_table( self ):
		self.triggers = lsctables.New(lsctables.SnglBurstTable,
			["ifo", "peak_time", "peak_time_ns", "start_time", "start_time_ns",
			"duration",  "search", "event_id", "process_id",
			"central_freq", "channel", "amplitude", "snr", "confidence",
			"chisq", "chisq_dof", "bandwidth"])
			#"peak_frequency",
			#"stop_time", "peak_time_ns", "start_time_ns", "stop_time_ns",
 			#"time_lag", "flow", "fhigh", tfvolume, hrss, process_id
		return self.triggers

	def write_triggers( self, flush=True, overwrite=False, filename=None ):
		if( not handler.output ): return
		if( len(self.triggers) == 0 ): return

		if( filename == None ):
			filename = self.outfile

		output = ligolw.Document()
		output.appendChild(ligolw.LIGO_LW())

		# TODO: Before or after Paused?
		requested_segment = segment(
			LIGOTimeGPS( self.time_since_dump ), 
			LIGOTimeGPS( self.stop )
		)

		analysis_segment = requested_segment
		# If we include start up time, indicate it in the search summary
		whiten_seg = segment( 
			LIGOTimeGPS(self.start), 
			LIGOTimeGPS(self.start + self.whitener_offset)
		)
		if( whiten_seg.intersects( analysis_segment) ):
			if( analysis_segment in whiten_seg ):
				# All the analyzed time is within the settling time
				# We make this explicit because the segment constructor will just reverse the arguments if arg2 < arg1 and create an incorrect segment
				analysis_segment = segment( 
					analysis_segment[1], analysis_segment[1]
				)
			else:
				analysis_segment -= whiten_seg

		process_params = vars( options )
		process = ligolw_process.register_to_xmldoc( output, "gstlal_excesspower", vars(options) )
		process.set_ifos( [self.inst] )

		# Assign process ids to events
		for trig in self.triggers:
			trig.process_id = process.process_id
			# If we're using a different units system, indicate it
			trig.duration *= handler.units
			#trig.peak_time /= handler.units
			# Readjust start time for units
			trig.start_time -= handler.start
			trig.start_time /= handler.units
			trig.start_time += handler.start
			trig.central_freq /= handler.units

		output.childNodes[0].appendChild( self.triggers )

		add_cbc_metadata( output, process, requested_segment )
		search_sum = lsctables.table.get_table( output, lsctables.SearchSummaryTable.tableName )
		for row in search_sum:
			# TODO: This shouldn't set every one of them in case we reuse XML documents later
			row.set_out( analysis_segment )

		# TODO: replace cbc filter table with our own
		#cbc_filter_table = lsctables.getTablesByType( output, lsctables.FilterTable )[0]
		#ep_filter_table = lsctables.getTablesByType( self.filter_xml, lsctables.FilterTable )[0]
		#output.replaceChild( ep_filter_table, cbc_filter_table )
		print >>sys.stderr, "Outputting triggers for %s\n" % str(requested_segment)

		# write the new distribution stats to disk
		self.lock.acquire()
		# Enable to debug LIGOLW stream
		#utils.write_fileobj(output, sys.stdout)
		utils.write_filename(output, filename, verbose = options.verbose, gz = (filename or "stdout").endswith(".gz"), trap_signals = None)
		self.lock.release()

		if( flush ): self.make_output_table()

	def shutdown( self, signum, frame ):
		"""
		Method called to flush buffers and shutdown the pipeline.
		"""
		print >>sys.stderr, "Caught signal, signal received, if any: " + str(signum)
		self.pipeline.set_state( gst.STATE_PAUSED )
		bus = self.pipeline.get_bus()
		bus.post(gst.message_new_eos(pipeline))
		self.pipeline.set_state( gst.STATE_NULL )
		self.mainloop.quit()

		print >>sys.stderr, "Please wait (don't ctrl+c) while I dump triggers to disk."
		# FIXME: Check units
		dur = numpy.ceil(float(handler.stop)) - numpy.floor(float(handler.time_since_dump))
		# FIXME: Handle this better
		if( os.path.isdir( self.outfile ) ):
			outfile = "%s/%s-%s_excesspower-%d-%d.xml" % (self.outfile, self.inst, self.channel.replace("-","_"), self.time_since_dump, dur)
		elif( self.outfile is None or len(self.outfile) == 0 ):
			outfile = "%s-%s_excesspower-%d-%d.xml" % (self.inst, self.channel.replace("-","_"), self.time_since_dump, dur)
		else:
			outfile = self.outfile
		self.write_triggers( False, filename = outfile )
		self.destroy_filter_xml()


#
# =============================================================================
#
#                        Message Handler Methods
#
# =============================================================================
#

# These are linked later in the pipeline to do the appropriate actions when signals are sent up.

def on_psd_change( elem, pspec, hand ):
	"""
	Get the PSD object and signal the handler to rebuild everything.
	"""
	if( options.verbose ):
		print >> sys.stderr, "Intercepted spectrum signal."

	hand.psd = REAL8FrequencySeries(
		name = "PSD",
		#epoch = laltypes.LIGOTimeGPS(0, message.structure["timestamp"]),
		f0 = 0.0,
		deltaF = elem.get_property( "delta-f" ),
		#sampleUnits = laltypes.LALUnit(message.structure["sample-units"].strip()),
		data = numpy.array( elem.get_property( "mean-psd" ) )
	)


	# Determine if the PSD has changed enough to warrant rebuilding the filter
	# bank.
	psd_power = sum(hand.psd.data)
	change = abs((hand.psd_power - psd_power) / psd_power )
	hand.psd_power = psd_power
	if( change > handler.psd_change_thresh ):
		if( options.verbose ):
			print >> sys.stderr, "Processed signal. PSD change %d per, regenerating filters" % int(change*100)
		hand.rebuild_everything()

def on_spec_corr_change( elem, pspec, hand ):
	"""
	Get the 2-point spectral correlation object and signal the handler to rebuild everything.
	"""
	if( options.verbose ):
		print >> sys.stderr, "Intercepted correlation signal."
	hand.spec_corr = elem.get_property( "spectral-correlation" )

	if( hand.cache_spec_corr ):
		f = open( "spec_corr.dat", "w" )
		k_end = len( hand.spec_corr ) / 2
		for k, sp in enumerate( hand.spec_corr ):
			f.write( "%d %g\n" % (k, sp) )
	
	# If the spectrum correlation changes, rebuild everything
	if( hand.psd != None ):
		hand.rebuild_everything()

def get_triggers(elem, handler, ndof):

	buffer = elem.emit("pull-buffer")

	if( not handler.output ):
		return # We don't want event information

	# TODO: Can I set units here on the buffer fields, avoid changing the triggers themslves and *not* screw up the underlying framework?

	for row in sngl_bursts_from_buffer(buffer):
		row.duration *= ndof
		if( options.compat ):
			row.snr = row.snr / ndof - 1
		handler.triggers.append( row )
	
	#buf_ts = (buffer.timestamp*1e-9 - handler.start) / handler.units + handler.start
	# TODO: Why does the buf_dur need unit conversion, but not the timestamp
	buf_ts = buffer.timestamp*1e-9 
	buf_dur = buffer.duration*1e-9 / handler.units
	handler.stop = (buf_ts + buf_dur)
	if( handler.stop - handler.time_since_dump > handler.dump_frequency or
			len(handler.triggers) >= handler.max_events ):
		# FIXME: Handle units better
		dur = numpy.ceil(float(handler.stop)) - numpy.floor(float(handler.time_since_dump))
		if( os.path.isdir( handler.outfile ) ):
			fname = "%s/%s-%s_excesspower-%d-%d.xml" % (handler.outfile, handler.inst, handler.channel.replace("-","_"), handler.time_since_dump, dur)
		elif( handler.outfile is None or len(handler.outfile) == 0 ):
			fname = "%s-%s_excesspower-%d-%d.xml" % (handler.inst, handler.channel.replace("-","_"), handler.time_since_dump, dur)
		else:
			fname = handler.outfile

		uniq = 1
		# FIXME: Needs to address the directory structure above
		while( os.path.exists( fname ) ):
			fname = "%s-%s_excesspower-%d-%d.%d.xml" % (handler.inst, handler.channel.replace("-","_"), handler.time_since_dump, dur, uniq)
			uniq += 1
		handler.write_triggers( filename = fname, flush = True )
		handler.time_since_dump = handler.stop 

# TODO: Update a single file every couple of seconds.
"""
	dur = 0
	if( len(handler.triggers) > 0 ):
		dur = handler.triggers[-1].peak_time - handler.triggers[0].peak_time

	if( len(handler.triggers) > 1000 or dur > 16 ):
		handler.write_triggers( flush=True )
"""

#
# =============================================================================
#
#                             Options Handling
#
# =============================================================================
#

parser = OptionParser()
parser.add_option("-f", "--initialization-file", dest="infile", help="Options to be pased to the pipeline handler. Strongly recommended.", default=None)
parser.add_option("-d", "--diagnostics", dest="diagnostics", action="store_true", help="Turn on multiple diagnostic dumps. Use with caution, as it will dump gigabytes of data (potentially) in a matter of minutes. Useful in nongraphical environemnts to monitor data throughput.", default=False)
parser.add_option("-v", "--verbose", dest="verbose", action="store_true", help="Be verbose.", default=False)
parser.add_option("-n", "--channel-name", dest="channame", action="store", help="Specify the channel name. Note that this will override the specification in the ini file (if any). This is useful if you want to run the same analysis on channels with the same characterisitics.")
parser.add_option("-w", "--channel-width", dest="nbyte", action="store", type=int, help="Specify the channel width in bits (default = 64)", default=64)
parser.add_option("-r", "--sample-rate", dest="sample_rate", action="store", type="int", help="Sample rate of the incoming data.")
parser.add_option("-D", "--data-source", dest="data_source", action="store", help="Data source to read from. Valid options are gwffile,lldata,whitedata,fakeLIGO,fakeadvLIGO. One is required. If gwffile is selected, then a cache file should also be provided. If whitedata, fakeLIGO, or fakeadvLIGO is selected, then a sample rate must also be provided.")
parser.add_option("-S", "--stream-tfmap", dest="stream_tfmap", action="store", help="Encode the time frequency map to video as it is analyzed. If the argument to this option is \"video\" then the pipeline will attempt to stream to a video source. If the option is instead a filename, the video will be sent to that name. Prepending \"keyframe=\" to the filename will start a new video every time a keyframe is hit.")
parser.add_option("-s", "--gps-start", dest="gps_start", action="store", type="float", help="Seek to gps time before beginning analysis.", default=None)
parser.add_option("-e", "--gps-end", dest="gps_end", action="store", type="float", help="End the analysis at this gps time.", default=None)
parser.add_option("-t", "--disable-triggers", dest="disable_triggers", action="store_false", help="Don't record triggers.", default=False)
parser.add_option("-c", "--lalapps-power-compatibility", dest="compat", action="store_true", default=False, help="Output trigger information which conforms to the lalapps_power conventions.")

(options, args) = parser.parse_args()

# The data rate at which we wish to do analysis
# Assumed lower than the input data
data_source = options.data_source

# TODO: Stop reinventing the wheel and replace this with mkLLOIDsrc
valid_data_sources = [ "gwffile", 
	  "nds",
	  "lldata", 
	  "whitedata", 
	  "fakeLIGO", 
	  "fakeadvLIGO" ]
if( not data_source in valid_data_sources ):
	print >>sys.stderr, "Either no data soruce was selected, or an invalid one was requested."
	sys.exit(-1)

if( not options.sample_rate 
	and ( data_source in ["whitedata", "fakeLIGO", "fakeadvLIGO"] ) ):
	print >>sys.stderr, "Sample rate not specified and fake data requested."
	sys.exit(-1)
else:
	sample_rate = options.sample_rate

# Verbosity and diagnostics
verbose = options.verbose
diagnostics = options.diagnostics

#
# =============================================================================
#
#                           Handler / Pipeline options
#
# =============================================================================
#

# We need a pipeline and pipeline handler instance to configure
pipeline = gst.Pipeline( "gstlal_excesspower" )
mainloop = gobject.MainLoop()
handler = EPHandler(mainloop, pipeline)

handler.output = not options.disable_triggers
handler.rate = options.sample_rate

# Used to keep track if we need to lock the PSD into the whitener
psdfile = None
if( not options.infile ):
	print >>sys.stderr, "No initialization file specified. Default values will be used."
elif( not os.path.exists( options.infile ) ):
	print >>sys.stderr, "Initialization file path is invalid."
	sys.exit(-1)
else:
	cfg = SafeConfigParser()
	cfg.read( options.infile )

	# Instruments and channels
	handler.inst = cfg.get( "instrument", "detector" )
	try:
		handler.channel = cfg.get( "instrument", "channel" )
	except ConfigParser.NoOptionError:
		handler.channel = None

	if( options.channame is not None ):
		handler.channel = options.channame
	if( handler.channel is None ):
		exit("No channel specified in the configuration file or on the command line.")
	print "Channel name: " + handler.channel

	try:
		site = cfg.get( "instrument", "site" )
	except ConfigParser.NoOptionError:
		print >>sys.stderr, "No site requested, using detector as default cache sieve."
		site = None

	# Handler options
	try:
		handler.fft_length = cfg.getfloat( "tf_parameters", "fft-length" )
	except ConfigParser.NoOptionError:
		pass

	handler.flow = cfg.getfloat( "tf_parameters", "min-frequency" )
	handler.fhigh = cfg.getfloat( "tf_parameters", "max-frequency" )
	handler.base_band = cfg.getfloat( "tf_parameters", "base-resolution" )
	handler.max_level = cfg.getint( "tf_parameters", "max-resolution-level" )
	handler.max_duration = cfg.getfloat( "tf_parameters", "max-time-resolution" )
	if( cfg.has_option( "cache", "reference-psd" ) ):
		psdfile = cfg.get( "cache", "reference-psd" )
		try:
			handler.psd = read_psd( psdfile )[ handler.inst ]
			print "Reference PSD for instrument %s from file %s loaded" % ( handler.inst, psdfile )
			# Reference PSD disables caching (since we already have it
			handler.cache_psd = False
			# TODO: Make a --track-psd option
		except KeyError: # Make sure we have a PSD for this instrument
			sys.exit( "PSD for instrument %s requested, but not found in file %s. Available instruments are %s" % (handler.inst, psdfile, str(handler.psd.keys())) )
		
	else:
		handler.cache_psd = cfg.get( "cache", "cache-psd" )
		if( handler.cache_psd == "" ): handler.cache_psd = False
		else: print "PSD caching enabled."

	handler.cache_spec_corr = cfg.getboolean( "cache", "cache-spectral-correlation" )

	handler.outfile = cfg.get( "triggering", "output-file" )
	handler.snr_thresh = cfg.getfloat( "triggering", "snr-thresh" )

	if( cfg.has_option( "triggering", "events_per_file" ) ):
		handler.max_events = cfg.get_int( "triggering", "events_per_file" )

	# If a specific (trigger) time is of interest, specify its GPS here
	# TODO: Read from sngl_inspirals and sngl_bursts
	trigger_begin, trigger_end = None, None
	if( cfg.has_option( "triggering", "trig_time_start" ) ):
		trigger_begin = cfg.getfloat( "triggering", "trig_time_start" )
	if( cfg.has_option( "triggering", "trig_time_end" ) ):
		trigger_end = cfg.getfloat( "triggering", "trig_time_end" )
	if( trigger_begin and trigger_end ):
		handler.set_trigger_time_and_action( segment( trigger_begin, trigger_end ) )

	gwflocation = cfg.get( "instrument", "location" )

	inj_loc = cfg.get( "injections", "xml-location" )
	if( not os.path.isfile( inj_loc ) ):
		print >>sys.stderr, "Injection file not found, disabling option."
		inj_loc = None

base_band = handler.base_band

# This is invoked here, or else the default rate is used, which will cause funny behavior for the defaults with some cases
# TODO: Less hardcodish -- update this when the rate or base_band is updated
handler.filter_len = 2*int(2*handler.rate/handler.base_band)
handler.build_default_psd( handler.rate, handler.filter_len )
handler.rebuild_filter()
handler.rebuild_chan_mix_matrix()

# Max trigger duration (s)
handler.max_duration

#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#

if options.verbose:
	print >>sys.stderr, "Assembling pipeline...\n",

duration = -1

# Data source
if( data_source == "nds" ):
	head = mkndssrc( pipeline,
		host = cfg.get( "instrument", "ndshost" ),
		instrument = handler.inst,
		channel_name = handler.channel
	)
	try:
		head.set_property( "port", cfg.getint( "instrument", "ndsport" ) )
	except ConfigParser.NoOptionError:
		print >>sys.stderr, "Warning, no NDS host port specified. Using default."

	# FIXME: Don't assume real-time
	head.set_property( "channel-type", "online" )
elif( data_source == "fakeLIGO" ):
	head = mkfakeLIGOsrc( pipeline, 
		instrument = handler.inst,
		channel_name = handler.channel
	)
elif( data_source == "fakeadvLIGO" ):
	head = mkfakeadvLIGOsrc( pipeline, 
		instrument = handler.inst,
		channel_name = handler.channel
	)
elif( data_source == "whitedata" ):
	head = gst.element_factory_make( "audiotestsrc" )
	pipeline.add( head )
	head.set_property( "wave", 9 ) # unity variance zero mean gaussian noise
elif( data_source == "lldata" ):
	shmmap = { "L1": "LLO_Data",
	           "H1": "LHO_Data",
	           "V1": "VIRGO_Data"}
	head = mklvshmsrc( pipeline, shm_name = shmmap[handler.inst] )
	src = head = mkframecppchanneldemux( pipeline, head )
	sink = mkaudiorate( pipeline, None, skip_to_first = True, silent = False )
	src_deferred_link( src, "%s:%s" % (handler.inst, handler.channel), sink.get_pad("sink") )
	head = sink

	# FIXME: This is a guess. Not a terrible one, but still inaccurate
	handler.time_since_dump = gpstime.GpsSecondsFromPyUTC( time.time() )
elif( data_source == "gwffile" ):
	framesrc = head = mkframesrc( pipeline, 
		gwflocation, 
		handler.inst, 
		handler.channel
	)
	start, duration = ep.duration_from_cache( gwflocation )
	if( options.gps_start ):
		handler.start = start = options.gps_start 
	else:
		options.gps_start = handler.start = start
		options.gps_start = float(options.gps_start)
	if( options.gps_end ):
		duration = options.gps_end - start
	else:
		options.gps_end = start + duration
		options.gps_end = float(options.gps_end)
	handler.time_since_dump = start
	# TODO: Must unhardcode this -- but requires knowledge of native rate
	if( options.nbyte == 64 ):
		bsize=16384*8 # 64 bit stream
	elif( options.nbyte == 32 ):
		bsize=16384*4 # 32 bit stream
	if( verbose ):
		print "Blocksize %d" % bsize
	#duration = int(duration*handler.rate/16384.0)
	head.set_property( "blocksize", bsize )
	print >>sys.stderr, "Warning, inferring analysis duration from cache."

	# FIXME: Make this an option
	if( handler.inst == "V1" ): 
		sieve = "V*"
		head.set_property( "cache-dsc-regex", sieve )
	if( site ):
		sieve = site[0] + "*"
	else: sieve = handler.inst + "*"
	head.set_property( "cache-dsc-regex", sieve )
	
else:
	print >>sys.stderr, "Data source %s not recognized. Check the valid options in the help message."
	sys.exit(-1)

# Seeking
seekevent = None
if( options.gps_start is not None and options.gps_end is not None ):

	# TODO: Use LIGOTimeGPS
	print "Duration: " + str(duration)
	seek, dur = long(start*1e9), long(duration*1e9)
	print >>sys.stderr, "Seeking to GPS %d (ns), segment duration %d (ns)" % (seek, dur)
	print >>sys.stderr, "Will stop at GPS %d (ns)" % (seek + dur)
	seekevent = gst.event_new_seek( 1.0, 
		gst.Format(gst.FORMAT_TIME),
		gst.SEEK_FLAG_KEY_UNIT | gst.SEEK_FLAG_FLUSH,
		gst.SEEK_TYPE_SET, seek,
		gst.SEEK_TYPE_SET, seek + dur
	)

elif( options.gps_start is not None ):

	print >>sys.stderr, "Seeking to GPS %d (ns)" % seek
	seek = long(options.gps_start*1e9)
	seekevent = gst.event_new_seek( 1.0, 
		gst.Format(gst.FORMAT_TIME),
		gst.SEEK_FLAG_KEY_UNIT | gst.SEEK_FLAG_FLUSH,
		gst.SEEK_TYPE_SET, seek,
		gst.SEEK_TYPE_NONE, 0
	)

if( seekevent is not None ):
	if( head.set_state(gst.STATE_READY) != gst.STATE_CHANGE_SUCCESS ):
		exit("Unable to ready pipeline to accept seek.")
	if( not head.send_event( seekevent ) ):
		exit("Unable to send seek event to " + str(head))

# Diagnostic plot
if( diagnostics ):
	head = postdatatee = mktee( pipeline, head )
	mknxydumpsink( pipeline, 
		mkqueue( pipeline, postdatatee ), 
		cfg.get( "diagnostics", "strain-data-output" )
	)

# Convert to 64 bit
head = mkcapsfilter( pipeline, mkaudioconvert( pipeline, head ), "audio/x-raw-float,width=64" )
# Data conditioning
head = mkcapsfilter( pipeline, mkresample( pipeline, head ), "audio/x-raw-float,rate=%d" % handler.rate )

if( inj_loc ):
	head = mkinjections( pipeline, head, inj_loc )

if( inj_loc and verbose ):
	head = mkprogressreport( pipeline, head, "injection stream" )

head = whitener = mkwhiten( pipeline, head )
whitener.set_property( "fft-length", handler.fft_length ) # GSTLAL_PSDMODE_FIXED
if( psdfile is not None ): # In other words, we have a reference PSD
	whitener.set_property( "mean-psd", handler.psd.data )
	whitener.set_property( "psd-mode", 1 ) # GSTLAL_PSDMODE_FIXED

head = mkqueue( pipeline, head )

# Diagnostic plot
if( diagnostics ):
	head = postresamptee = mktee( pipeline, head )
	mknxydumpsink( pipeline, 
		mkqueue( pipeline, head ), 
		cfg.get( "diagnostics", "whitened-data-output" )
	)

if( verbose ):
	head = mkprogressreport( pipeline, head, "whitened stream" )

# excess power channel firbank
# NOTE: This is where the inspiral pipeline will feed in?
head = mkfirbank( pipeline, head, time_domain=False, block_stride=handler.rate )

# TODO: Make this less hardcodish
# This needs to be done since what is returned by mkfirbank is actually a link to the nofakedisconts element, so the fir bank is hidden
handler.add_firbank( head )
nchannels = handler.filter_bank.shape[0]
print "FIR bank constructed with %d %f Hz channels" % (nchannels, base_band)

if( verbose ):
	head = mkprogressreport( pipeline, head, "FIR bank stream" )

# TODO: Uncomment here
#head = postfirtee = mkqueue( pipeline, mktee( pipeline, head ) )
#####

if( diagnostics ):
	mknxydumpsink( pipeline, 
		postfirtee, 
		cfg.get( "diagnostics", "fir-output" )
	)

# First branch -- send fully sampled data to wider channels for processing
nlevels = int(numpy.ceil( numpy.log2( nchannels ) )) 
for res_level in range(0, min(handler.max_level, nlevels)):
	# TODO: Uncomment here
	#head = postfirtee
	#######

	band = base_band * 2**res_level
	# TODO: Check this
	chan = numpy.ceil( nchannels / 2.0**res_level )

	# The undersample_rate for band = R/2 is => sample_rate (passthrough)
	undersamp_rate = 2 * band

	#head = mkgeneric( pipeline, head, "lal_audioundersample" )

	# If the rate which would be set by the undersampler falls below one, we have
	# to take steps to prevent this, as gstreamer can't handle this. The 
	# solution is to change the "units" of the rate. Ideally, this should be done
	# much earlier in the pipeline (e.g. as the data comes out of the source),
	# however, to avoid things like figuring out what that means for the FIR bank
	# we change units here, and readjust appropriately in the trigger output.
	if( undersamp_rate < 1 ):
		print "Automatically adjusting units to compensate for undersample rate falling below unity."
		# No, it's not factors of ten, but rates which aren't factors
		# of two are often tricky, thus if the rate is a factor of two, the units
		# conversion won't change that.
		if( undersamp_rate > EXCESSPOWER_UNIT_SCALE['mHz'] ):
			unit = 'mHz'
		elif( undersamp_rate > EXCESSPOWER_UNIT_SCALE['uHz'] ):
			unit = 'uHz'
		elif( undersamp_rate > EXCESSPOWER_UNIT_SCALE['nHz'] ):
			unit = 'nHz'
		else:
			sys.exit( "Requested undersampling rate would fall below 1 nHz." )
		# FIXME: No love for positive power of 10 units?

		handler.units = EXCESSPOWER_UNIT_SCALE[unit]
		undersamp_rate /= handler.units
		print "Undersampling rate for level %d: %f %s" % (res_level, undersamp_rate, unit)
		head = mkcapssetter( pipeline, head, "audio/x-raw-float,rate=%d" % (handler.rate/handler.units), replace=False )
		head = mkgeneric( pipeline, head, "lal_audioundersample" )
		head = mkcapssetter( pipeline, head, "audio/x-raw-float,rate=%d" % undersamp_rate, replace=False )
	else:
		print "Undersampling rate for level %d: %f Hz" % (res_level, undersamp_rate)
		head = mkgeneric( pipeline, head, "lal_audioundersample" )
		head = mkcapsfilter( pipeline, head, "audio/x-raw-float,rate=%d" % undersamp_rate )

	if( diagnostics ):
		head = postustee = mktee( pipeline, mkqueue( pipeline, head ) )
		mknxydumpsink( pipeline, postustee, "postundersamp_res_%d.txt" % res_level )

	if( verbose ):
		head = mkprogressreport( pipeline, head, 
			"Undersampled stream level %d" % res_level
		)

	head = matmixer = mkmatrixmixer( pipeline, head )
	handler.add_matmixer( matmixer, res_level )

	if( verbose ):
		head = mkprogressreport( pipeline, head,
			"post matrix mixer %d" % res_level 
		)

	if( diagnostics ):
		head = postmmtee = mktee( pipeline, mkqueue( pipeline, head ) )
		mknxydumpsink( pipeline, postmmtee, "postmatmix_res_%d.txt" % res_level )

	head = mkgeneric( pipeline, head, "pow" )
	head.set_property( "exponent", 2 )

	if( verbose ):
		head = mkprogressreport( pipeline, head, 
			"Energy stream level %d" % res_level
		)

	# TODO: Uncomment here
	#head = mkqueue( pipeline, head )
	#####

	ndof = 2 # samples -- min number
	# Second branch -- duration
	# max_samp = int(handler.max_duration*rate)
	#while duration <= max_samp:
		#duration = duration << 1

	# Multi channel FIR filter -- used to add together frequency bands into tiles
	# FIXME: This is a workaround around until the audiofirfilter is fixed.
	# Basically, if the firfilter gets a small enough kernel, it invokes 
	# time-domain convolution. Something about the way it fills the buffers is
	# wrong in regards to the way it keeps history and thus the output buffers
	# partially desynched from the input -- even with an identity transform.

	# Workaround: Zero pad the kernel past the length (32 samples) in which the 
	# element switches to FFT convolution which doesn't exhibit bad behavior.

	#head = mkchecktimestamps( pipeline, head, "before audiofir" )
	head = mkgeneric( pipeline, head, "audiofirfilter" )
	head.set_property( "kernel", 
		ep.build_fir_sq_adder( ndof, padding=max(0, 33-ndof) )
	)
	head = mknofakedisconts( pipeline, head )
	#head = mkchecktimestamps( pipeline, head, "after audiofir" )

	if( diagnostics ):
		head = postdurtee = mktee( pipeline, mkqueue( pipeline, head ) )
		mknxydumpsink( pipeline, postdurtee, "postdur_res_%d_%d.txt" % (res_level, ndof) )

	if( verbose ):
		head = mkprogressreport( pipeline, head, 
			"After energy summation resolution level %d, %d DOF" % 
				(res_level, ndof) 
		)

	# Reenable for amplitude SNR stream
	#head = mkgeneric( pipeline, head, "pow" )
	#head.set_property( "exponent", 0.5 )

	# TODO: Audio
	if( options.stream_tfmap ):
		if len(options.stream_tfmap.split("=")) == 2:
			split_opt, filename = options.stream_tfmap.split("=")
		else:
			filename = options.stream_tfmap
			# TODO: Make this more elegant
			if( filename == "video" ): filename = None
			split_opt = None

		head = stream_tfmap_video( pipeline, head, 
			handler, 
			filename,
			split_opt
		)

	# Trigger generator
	head = mkbursttriggergen( pipeline, head, ndof, 
		bank = handler.build_filter_xml( res_level )
	)

	if( handler.fap is not None ):
		# Still needs magic number... or use the EP version
		# ndof_eff = ndof * 0.62
		snr_thresh = ep_utils.determine_thresh_from_fap(fap, ndof)
	else:
		# TODO: Make clear in the ini file that the thresh is power SNR, not amplitude and then remove the square here
		snr_thresh = handler.snr_thresh**2
	head.set_property( "snr-thresh", snr_thresh )

	if( verbose ):
		head = mkprogressreport( pipeline, head, 
			"Trigger generator resolution level %d, %d DOF" % (res_level, ndof) 
		)

	# TODO: combine trigger streams from various levels

	# TODO: This will have to be linked to multiple outgoing streams
	appsink = mkappsink(pipeline, mkqueue(pipeline, head))
	appsink.connect_after("new-buffer", get_triggers, handler, ndof)

### END OF PIPELINE

# Spectrum notification processing
whitener.connect_after( "notify::mean-psd", on_psd_change, handler )
# Handle spectral correlation changes
# TODO: Make sure this doesn't have to be in the mm loop
whitener.connect_after( "notify::spectral-correlation", on_spec_corr_change, handler )

# Handle shutdowns
signal.signal( signal.SIGINT, handler.shutdown )
signal.signal( signal.SIGTERM, handler.shutdown )

print >>sys.stderr, "Startin' up."
pipeline.set_state( gst.STATE_PLAYING )
if( diagnostics ):
	write_dump_dot(pipeline, "test", verbose = True)
	#gst.DEBUG_BIN_TO_DOT_FILE( pipeline,
		#gst.DEBUG_GRAPH_SHOW_ALL,
		#cfg.get( "diagnostics", "dot-file-location" )
	#)
mainloop.run()

