#!/usr/bin/env python
#
# Copyright (C) 2011 Chris Pankow
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
"""Stream-based burst analysis tool"""

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


import os
import numpy
import signal

from optparse import OptionParser
import ConfigParser
from ConfigParser import SafeConfigParser

from gstlal.pipeutil import gst, mkelem
from gstlal.pipeparts import *
from gstlal.pipeio import parse_spectrum_message
from gstlal.lloidparts import LLOIDHandler, DetectorData, mkLLOIDbasicsrc, get_gate_state
from gstlal.reference_psd import write_psd, read_psd_xmldoc

import gstlal.excesspower as ep
from gstlal.excesspower import *
from gstlal.inspiral import add_cbc_metadata

from glue.ligolw import ligolw
from glue.ligolw import array
from glue.ligolw import param
from glue.ligolw import lsctables
from glue.ligolw import table
array.use_in(ligolw.LIGOLWContentHandler)
param.use_in(ligolw.LIGOLWContentHandler)
lsctables.use_in(ligolw.LIGOLWContentHandler)
from glue.ligolw import utils
from glue.ligolw.utils import process as ligolw_process
from glue.ligolw.utils import segments as ligolw_segments

from glue.segments import segment, segmentlist, PosInfinity
from glue import segmentsUtils
from glue import gpstime
from glue.lal import LIGOTimeGPS
from glue.lal import Cache
from glue.lal import CacheEntry

from pylal import ligolw_bucluster
from pylal.xlal.datatypes.snglburst import from_buffer as sngl_bursts_from_buffer
from pylal.xlal.datatypes.real8frequencyseries import REAL8FrequencySeries
from pylal.xlal.lalburst import XLALlnOneMinusChisqCdf

#
# =============================================================================
#
#                                Unit Handling
#
# =============================================================================
#

EXCESSPOWER_UNIT_SCALE = {
	'Hz':  1024**0,
	'kHz': 1024**1,
	'MHz': 1024**2,
	'GHz': 1024**3,
	'mHz': 1024**-1,
	'uHz': 1024**-2,
	'nHz': 1024**-3,
}

#
# =============================================================================
#
#                                Handler Class
#
# =============================================================================
#

class EPHandler( LLOIDHandler ):
	"""
	Handler class for the excess power pipeline. Keeps various bits of information that the pipeline emits and consumes. This is also in charge of signalling the rebuild of various matrices and vectors needed by the pipeline.
	"""
	def __init__( self, mainloop, pipeline ):

		# Instrument and channel
		self.inst = None
		self.channel = None

		# Book keeping
		self.start = 0
		self.stop = -1
		self.seglist = segments.segmentlistdict()
		self.seglist["state"] = segments.segmentlist([])
		self.current_segment = None
		# How long of a time to ignore output from the whitener stabilizing
		# FIXME: This should be ~number of PSDs stored in history * fft_length
		# Right now that's probably something like 80 s.
		self.whitener_offset = 40

		# Keep track of units -- enable us to go below rates of 1 Hz
		self.units = EXCESSPOWER_UNIT_SCALE['Hz']

		# Defaults -- Time-frequency map settings
		self.base_band = 16
		self.flow = 64 
		self.fhigh = 1000
		self.fft_length = 8 # s

		# Defaults -- Resolution settings
		self.rate = 2048
		#self.rate = 4096
		self.max_level = 1

		# Defaults -- filtering
		self.filter_len = 2*int(2*self.rate/self.base_band)
		self.filter_bank = None
		# TODO: Maybe not necessary
		self.firbank = None

		# Defaults -- PSD settings
		# This is used to store the previous value of the PSD power
		self.psd_power = 0
		self.cache_psd = False
		self.psd_change_thresh = 0.5 # fifty percent

		# Defaults -- Two-point spectral correlation settings
		self.cache_spec_corr = False

		# Defaults -- mixer matrices
		self.chan_matrix = None
		self.mmixers = {}

		# Defaults -- data products
		self.output = True
		self.triggers = None
		self.outfile = "test.xml"
		self.make_output_table()
		self.output_cache = Cache()
		self.snr_thresh = 5.5
		self.fap = None
		self.dump_frequency = 600 # s
		self.max_events = 1e4
		self.time_since_dump = self.start
		self.db_thresh = None  # off
		self.db_client = None  # off

		self.trigger_segment = None

		self.spec_corr = self.build_default_correlation( self.rate )
		"""
		self.psd = self.build_default_psd( self.rate, self.filter_len )
		self.rebuild_filter()
		self.rebuild_chan_mix_matrix()
		"""

		# required for file locking
		self.lock = threading.Lock()

		self.bus = pipeline.get_bus()
		self.bus.add_signal_watch()
		self.bus.connect( "message", self.process_message )

		self.pipeline = pipeline

		super(type(self), self).__init__(mainloop, pipeline)

	def set_trigger_time_and_action( self, trig_seg, action=["psd"] ):
		"""
		Inform the handler of a specific time of interest.
		"""
		# TODO: Bounds checking
		self.trigger_segment = trig_seg

		# TODO: Handle only specific action requests

	def handle_segment( self, elem, timestamp, segment_type ):
		"""
		Process state changes from the state vector mechanism.
		"""
		if( segment_type == "on" ):
			self.current_segment = segment( LIGOTimeGPS(timestamp) / 1e9, PosInfinity )
			#segmentsUtils.tosegwizard( sys.stdout, self.seglist["state"] )
			if( options.verbose ):
				print >>sys.stderr, "Starting segment #%d: %.9f" % (len(self.seglist["state"]), self.current_segment[0])
		elif( segment_type == "off" ):
			if( self.current_segment is None ): 
				print >>sys.stderr, "Got a message to end a segment, but no current segment exists. Ignoring."
				return
			self.seglist["state"].append(
				segment( self.current_segment[0], LIGOTimeGPS(timestamp / 1e9) )
			)
			if( options.verbose ):
				print >>sys.stderr, "Ending segment #%d: %s" % (len(self.seglist["state"]), str(self.seglist["state"][-1]))
		else:
			print >>sys.stderr, "Unrecognized state change, ignoring."

	def process_message( self, bus, message ):
		"""
		Process a message from the bus. Depending on what it is, we may drop various data products on to disk, or perform various other actions.
		"""
		if( message.type == gst.MESSAGE_EOS ):
			self.shutdown(None, None)
			return
		elif( message.type == gst.MESSAGE_LATENCY ):
			print >>sys.stderr, "Got latency message, ignoring for now."
			return
		elif( message.structure is None ): 
			print >>sys.stderr, "Got message with type: %s ...but no handling logic, so ignored."
			return

		# TODO: Move this to PSD difference checker
		if( message.structure.get_name() == "spectrum" ):
			# FIXME: Units
			ts = message.structure[ "timestamp" ]*1e-9
			if( self.trigger_segment is not None 
				and ts in self.trigger_segment ):
				self.dump_psd( timestamp = ts )
			elif( self.cache_psd ): 
				self.dump_psd( timestamp = ts )

	def dump_psd( self, filename=None, timestamp=None ):
		"""
		Dump the currently cached PSD to a LIGOLW XML file. If the filename isn't specified, it will be constructed from metadata.
		"""
		if( filename is None ):
			if( os.path.isdir( handler.cache_psd ) ):
				filename = "%s/%s-%s_PSD_%d.xml" % ( handler.cache_psd, handler.inst, handler.channel.replace("-","_"), round(timestamp) )
			else:
				filename = handler.cache_psd
		write_psd( filename, { self.inst: self.psd } )

	def add_firbank( self, firbank ):
		"""
		Set the main base band FIR bank, and build the FIR matrix.
		"""
		self.firbank = firbank
		firbank.set_property( "fir-matrix", self.rebuild_filter() )

	def add_matmixer( self, mm, res_level ):
		self.mmixers[ res_level ] = mm
		self.rebuild_matrix_mixers( res_level )

	def build_default_psd( self, rate, filter_len ):
		"""
		Builds a dummy PSD to use until we get the right one.
		"""
		psd = REAL8FrequencySeries()
		psd.deltaF = float(rate)/filter_len
		psd.data = numpy.ones( filter_len/2 + 1 ) #/ 2 / psd.deltaF
		psd.f0 = 0
		self.psd = psd
		return psd

	def build_default_correlation( self, rate ):
		"""
		Builds a Kronecker delta correlation series for k, k'.
		"""
		corr = numpy.zeros(rate + 1)
		corr[0] = 1
		return corr

	def rebuild_matrix_mixers( self, res_level = None ):
		"""
		Rebuilds the matrix mixer matrices from the coefficients calculated in rebuild_chan_mix_matrix and assigns them to their proper element.
		"""
		for i, mm in self.mmixers.iteritems():
			if( res_level != None and res_level != i ): continue

			nchannels = self.filter_bank.shape[0]
			up_factor = int(numpy.log2(nchannels/(nchannels >> i)))
			cmatrix = ep.build_chan_matrix( 
				nchannels = nchannels,
				up_factor = up_factor,
				norm = self.chan_matrix[i] 
			)
			mm.set_property( "matrix", cmatrix )

	def rebuild_filter( self ):
		"""
		Calling this function rebuilds the filter FIR banks and assigns them to their proper element. This is normally called when the PSD or spectrum correlation changes.
		"""
		self.filter_bank = ep.build_filter( fhigh = self.fhigh, flow=self.flow, rate=self.rate, psd = self.psd, corr = self.spec_corr, b_wind = self.base_band )
		return self.filter_bank

	def build_filter_xml( self, res_level, loc="" ):
		"""
		Calls the EP library to create a XML of sngl_burst tables representing the filter banks. At the moment, this dumps them to the current directory, but this can be changed by supplying the 'loc' argument. The written filename is returned for easy use by the trigger generator.
		"""
		self.filter_xml = ep.create_bank_xml(
			self.flow,
			self.fhigh,
			self.base_band*(res_level+1),
			# FIXME: Is there a factor of two here? -- No, remove the factor of two...
			1.0 / (2*self.base_band*(res_level+1)), # resolution level starts from 0
			self.inst
		)
		output = "%sgstlal_excesspower_bank_%s_%s_level_%d.xml" % (loc, self.inst, self.channel, res_level)
		utils.write_filename( self.filter_xml, output, verbose = True,
		       gz = (output or "stdout").endswith(".gz") )
		return output

	def destroy_filter_xml( self, loc="" ):
		import glob
		for f in glob.glob( "%s/gstlal_excesspower_bank_%s_level_*.xml" % (loc, self.inst) ):
			os.rm( f )

	def rebuild_chan_mix_matrix( self ):
		"""
		Calling this function rebuilds the matrix mixer coefficients for higher resolution components. This is normally called when the PSD or spectrum correlation changes.
		"""
		self.chan_matrix = ep.build_inner_product_norm( 
			corr = self.spec_corr, 
			band = self.base_band, 
			del_f = self.psd.deltaF,
			nfilts = len(self.filter_bank),
			flow = self.flow,
			# TODO: PSD option to lalburst IP doesn't work
			#psd = self.psd
			max_level = self.max_level
		)
		return self.chan_matrix

	def rebuild_everything( self ):
		"""
		Top-level function to handle the asynchronous updating of FIR banks and matrix mixer elements.
		"""
		# Rebuild filter bank and hand it off to the FIR element
		if( options.verbose ):
			print >>sys.stderr, "Rebuilding FIR bank"
		self.firbank.set_property( "fir_matrix", self.rebuild_filter() )

		if( options.verbose ):
			print >>sys.stderr, "Rebuilding matrix mixer"
		self.rebuild_chan_mix_matrix()
		# Rebuild the matrix mixer with new normalization coefficients
		self.rebuild_matrix_mixers()

	def make_output_table( self ):
		self.triggers = lsctables.New(lsctables.SnglBurstTable,
			["ifo", "peak_time", "peak_time_ns", "start_time", "start_time_ns",
			"duration",  "search", "event_id", "process_id",
			"central_freq", "channel", "amplitude", "snr", "confidence",
			"chisq", "chisq_dof", "bandwidth"])
			#"peak_frequency",
			#"stop_time", "peak_time_ns", "start_time_ns", "stop_time_ns",
 			#"time_lag", "flow", "fhigh", tfvolume, hrss, process_id
		return self.triggers

	def write_triggers( self, flush=True, overwrite=False, filename=None ):
		if( not handler.output ): return
		if( len(self.triggers) == 0 ): return

		if( filename == None ):
			filename = self.outfile

		output = ligolw.Document()
		output.appendChild(ligolw.LIGO_LW())

		# TODO: Before or after Paused?
		requested_segment = segment(
			LIGOTimeGPS( self.time_since_dump ), 
			LIGOTimeGPS( self.stop )
		)

		analysis_segment = requested_segment
		# If we include start up time, indicate it in the search summary
		self.whiten_seg = segment( 
			LIGOTimeGPS(self.start), 
			LIGOTimeGPS(self.start + self.whitener_offset)
		)
		if( self.whiten_seg.intersects( analysis_segment ) ):
			if( analysis_segment in self.whiten_seg ):
				# All the analyzed time is within the settling time
				# We make this explicit because the segment constructor will just reverse the arguments if arg2 < arg1 and create an incorrect segment
				analysis_segment = segment( 
					analysis_segment[1], analysis_segment[1]
				)
			else:
				analysis_segment -= self.whiten_seg

		process_params = vars( options )
		process = ligolw_process.register_to_xmldoc( output, "gstlal_excesspower", vars(options) )
		process.set_ifos( [self.inst] )

		# Assign process ids to events
		for trig in self.triggers:
			trig.process_id = process.process_id
			# If we're using a different units system, indicate it
			trig.duration *= handler.units
			#trig.peak_time /= handler.units
			# Readjust start time for units
			trig.start_time -= handler.start
			trig.start_time /= handler.units
			trig.start_time += handler.start

		output.childNodes[0].appendChild( self.triggers )

		add_cbc_metadata( output, process, requested_segment )
		search_sum = lsctables.table.get_table( output, lsctables.SearchSummaryTable.tableName )
		# TODO: This shouldn't set every one of them in case we reuse XML documents later
		for row in search_sum:
			row.set_out( analysis_segment )

		if( self.current_segment is not None ):
			# add the current segment
			cur_seg = segment( self.current_segment[0], LIGOTimeGPS(analysis_segment[1]) )
			self.seglist["state"].append( cur_seg )

		# Write segments
		llwseg = ligolw_segments.LigolwSegments( output )
		# FIXME: Better names and comments?
		llwseg.insert_from_segmentlistdict( self.seglist, "gstlal_excesspower segments", "gstlal_excesspower segments" )

		llwseg.finalize(process)

		# FIXME: We should be careful to not fragment segments across output too 
		# much
		self.seglist.clear()

		# FIXME: We actually will end up writing the same segment more than once 
		# across adjacent files. This is probably okay but, fair warning.
		# What maybe should be done is to intersect the final segment with the
		# analysis_segment's end and then make the current segment have the same 
		# start time as the last written segment's end.
		self.seglist["state"] = segments.segmentlist([])

		# Do a temporary write to make the SnglBurst objects into XML rows -- this 
		# should probably be done only if clustering is requested, since it's the 
		# only thing that thinks the triggers should be XML rows

		# TODO: replace cbc filter table with our own
		#cbc_filter_table = lsctables.getTablesByType( output, lsctables.FilterTable )[0]
		#ep_filter_table = lsctables.getTablesByType( self.filter_xml, lsctables.FilterTable )[0]
		#output.replaceChild( ep_filter_table, cbc_filter_table )
		print >>sys.stderr, "Outputting triggers for %s\n" % str(requested_segment)

		# write the new distribution stats to disk
		self.lock.acquire()
		# Enable to debug LIGOLW stream
		#utils.write_fileobj(output, sys.stdout)
		utils.write_filename(output, filename, verbose = options.verbose, gz = (filename or "stdout").endswith(".gz"), trap_signals = None)
		self.lock.release()

		# Reload the document to convert the SnglBurst type to rows
		output = utils.load_filename(filename, verbose = options.verbose)
		process = lsctables.table.get_table( output, lsctables.ProcessTable.tableName )[0]

		# FIXME: Should this be moved to the trigger import function?
		changed = True

		# We need this because we might have just set it to be an infinitesimally 
		# small segment. Clustering will choke on this, and we want to know what 
		# we can send to dbs anyway.
		while changed and options.clustering and abs(analysis_segment) != 0:
			ligolw_bucluster.add_ms_columns( output )
			output, changed = ligolw_bucluster.ligolw_bucluster( 
				xmldoc = output,
				program = "gstlal_excesspower",
				process = process,
				prefunc = ligolw_bucluster.ExcessPowerPreFunc,
				postfunc = ligolw_bucluster.ExcessPowerPostFunc,
				testfunc = ligolw_bucluster.ExcessPowerTestFunc,
				clusterfunc = ligolw_bucluster.ExcessPowerClusterFunc,
				sortfunc = ligolw_bucluster.ExcessPowerSortFunc,
				bailoutfunc = ligolw_bucluster.ExcessPowerBailoutFunc,
				verbose = options.verbose
			)

		# TODO: Respect max events by removing output XML file and returning
		# if clustering reduces trigger number below required.
		if( options.clustering ):
			# write the new distribution stats to disk
			self.lock.acquire()
			# Enable to debug LIGOLW stream
			#utils.write_fileobj(output, sys.stdout)
			utils.write_filename(output, filename, verbose = options.verbose, gz = (filename or "stdout").endswith(".gz"), trap_signals = None)
			self.lock.release()

		# TODO: Determine the right number to put here
		self.discard_segment = segment( 
			LIGOTimeGPS(self.start), 
			LIGOTimeGPS(self.start + 300)
		)

		# Keep track of the output files we make for later convience
		if( self.output_cache is not None ):
			self.output_cache.append(
				CacheEntry(
					self.inst, self.channel + "_excesspower",
					analysis_segment,
					("file://localhost" + os.path.abspath(filename))
				)
			)

		if( flush ): self.make_output_table()
		if( self.db_thresh is None ): return

		# FIXME: get_table doesn't return the type of table you want it just returns
		# a "Table" this is probbably why the upload_tbl needs the full definition
		clus_triggers = table.get_table( output, lsctables.SnglBurstTable.tableName )

		for sb in filter( lambda sb : sb.snr > self.db_thresh**2, clus_triggers ):
			# TODO: Merge these two
			if( sb.peak_time in self.discard_segment ): continue
			if( sb.peak_time not in analysis_segment ): continue
			upload_tbl = lsctables.New(lsctables.SnglBurstTable,
			["ifo", "peak_time", "peak_time_ns", "start_time", "start_time_ns",
			"duration",  "search", "event_id", "process_id",
			"central_freq", "channel", "amplitude", "snr", "confidence",
			"chisq", "chisq_dof", "bandwidth"])

			upload_tbl.append( sb )
			upload_to_db( upload_tbl, search = "EP", db = self.db_client )
		
	def shutdown( self, signum, frame ):
		"""
		Method called to flush buffers and shutdown the pipeline.
		"""
		print >>sys.stderr, "Caught signal, signal received, if any: " + str(signum)
		self.pipeline.set_state( gst.STATE_PAUSED )
		bus = self.pipeline.get_bus()
		bus.post(gst.message_new_eos(pipeline))
		self.pipeline.set_state( gst.STATE_NULL )
		self.mainloop.quit()

		print >>sys.stderr, "Please wait (don't ctrl+c) while I dump triggers to disk."
		# FIXME: Check units
		dur = numpy.ceil(float(self.stop)) - numpy.floor(float(self.time_since_dump))
		# FIXME: Handle this better
		if( os.path.isdir( self.outfile ) ):
			outfile = "%s/%s-%s_excesspower-%d-%d.xml" % (self.outfile, self.inst, self.channel.replace("-","_"), self.time_since_dump, dur)
		elif( self.outfile is None or len(self.outfile) == 0 ):
			outfile = "%s-%s_excesspower-%d-%d.xml" % (self.inst, self.channel.replace("-","_"), self.time_since_dump, dur)
		else:
			outfile = self.outfile
		self.write_triggers( False, filename = outfile )
		if( self.output_cache is not None and len(self.output_cache) != 0 ):
			dur = numpy.ceil(float(self.stop)) - numpy.floor(float(self.start))
			outfile = "%s-%s_excesspower-%d-%d.xml" % (self.inst, self.channel.replace("-","_"), self.start, dur)
			self.output_cache.tofile( file(outfile.replace( "xml", "cache" ), "w") )
		self.destroy_filter_xml()


#
# =============================================================================
#
#                        Message Handler Methods
#
# =============================================================================
#

# These are linked later in the pipeline to do the appropriate actions when signals are sent up.

def on_psd_change( elem, pspec, hand ):
	"""
	Get the PSD object and signal the handler to rebuild everything.
	"""
	if( options.verbose ):
		print >> sys.stderr, "Intercepted spectrum signal."

	hand.psd = REAL8FrequencySeries(
		name = "PSD",
		#epoch = laltypes.LIGOTimeGPS(0, message.structure["timestamp"]),
		f0 = 0.0,
		deltaF = elem.get_property( "delta-f" ),
		#sampleUnits = laltypes.LALUnit(message.structure["sample-units"].strip()),
		data = numpy.array( elem.get_property( "mean-psd" ) )
	)


	# Determine if the PSD has changed enough to warrant rebuilding the filter
	# bank.
	psd_power = sum(hand.psd.data)
	change = abs((hand.psd_power - psd_power) / (hand.psd_power + psd_power) )
	if( change > handler.psd_change_thresh ):
		if( options.verbose ):
			print >> sys.stderr, "Processed signal. PSD change %d, regenerating filters" % int(change*100)
		hand.psd_power = psd_power
		hand.rebuild_everything()

def on_spec_corr_change( elem, pspec, hand ):
	"""
	Get the 2-point spectral correlation object and signal the handler to rebuild everything.
	"""
	if( options.verbose ):
		print >> sys.stderr, "Intercepted correlation signal."
	hand.spec_corr = elem.get_property( "spectral-correlation" )

	if( hand.cache_spec_corr ):
		f = open( "spec_corr.dat", "w" )
		k_end = len( hand.spec_corr ) / 2
		for k, sp in enumerate( hand.spec_corr ):
			f.write( "%d %g\n" % (k, sp) )
	
	# If the spectrum correlation changes, rebuild everything
	if( hand.psd != None ):
		hand.rebuild_everything()

def get_triggers(elem, handler, ndof):

	buffer = elem.emit("pull-buffer")

	if( not handler.output ):
		return # We don't want event information

	# TODO: Can I set units here on the buffer fields, avoid changing the triggers themslves and *not* screw up the underlying framework?

	for row in sngl_bursts_from_buffer(buffer):
		row.duration *= ndof
		# TODO Move this into the trigger generator
		# FIXME: Determine "magic number" or remove it
		row.confidence = -XLALlnOneMinusChisqCdf(row.snr * 0.62, ndof * 0.62)
		if( options.compat ):
			row.snr = row.snr / ndof - 1
		handler.triggers.append( row )
	
	#buf_ts = (buffer.timestamp*1e-9 - handler.start) / handler.units + handler.start
	# TODO: Why does the buf_dur need unit conversion, but not the timestamp
	buf_ts = buffer.timestamp*1e-9 
	buf_dur = buffer.duration*1e-9 / handler.units
	handler.stop = (buf_ts + buf_dur)
	if( handler.stop - handler.time_since_dump > handler.dump_frequency or
			len(handler.triggers) >= handler.max_events ):
		# FIXME: Handle units better
		dur = numpy.ceil(float(handler.stop)) - numpy.floor(float(handler.time_since_dump))
		if( os.path.isdir( handler.outfile ) ):
			fname = "%s/%s-%s_excesspower-%d-%d.xml" % (handler.outfile, handler.inst, handler.channel.replace("-","_"), handler.time_since_dump, dur)
		elif( handler.outfile is None or len(handler.outfile) == 0 ):
			fname = "%s-%s_excesspower-%d-%d.xml" % (handler.inst, handler.channel.replace("-","_"), handler.time_since_dump, dur)
		else:
			fname = handler.outfile

		uniq = 1
		# FIXME: Needs to address the directory structure above
		while( os.path.exists( fname ) ):
			fname = "%s-%s_excesspower-%d-%d.%d.xml" % (handler.inst, handler.channel.replace("-","_"), handler.time_since_dump, dur, uniq)
			uniq += 1
		handler.write_triggers( filename = fname, flush = True )
		handler.time_since_dump = handler.stop 

# TODO: Update a single file every couple of seconds.
"""
	dur = 0
	if( len(handler.triggers) > 0 ):
		dur = handler.triggers[-1].peak_time - handler.triggers[0].peak_time

	if( len(handler.triggers) > 1000 or dur > 16 ):
		handler.write_triggers( flush=True )
"""

#
# =============================================================================
#
#                             Options Handling
#
# =============================================================================
#

parser = OptionParser()
parser.add_option("-f", "--initialization-file", dest="infile", help="Options to be pased to the pipeline handler. Strongly recommended.", default=None)
parser.add_option("-d", "--diagnostics", dest="diagnostics", action="store_true", help="Turn on multiple diagnostic dumps. Use with caution, as it will dump gigabytes of data (potentially) in a matter of minutes. Useful in nongraphical environemnts to monitor data throughput.", default=False)
parser.add_option("-v", "--verbose", dest="verbose", action="store_true", help="Be verbose.", default=False)
parser.add_option("-n", "--channel-name", dest="channame", action="store", help="Specify the channel name. Note that this will override the specification in the ini file (if any). This is useful if you want to run the same analysis on channels with the same characterisitics.")
parser.add_option("-w", "--channel-width", dest="nbyte", action="store", type=int, help="Specify the channel width in bits (default = 64)", default=64)
parser.add_option("-r", "--sample-rate", dest="sample_rate", action="store", type="int", help="Sample rate of the incoming data.")
parser.add_option("-D", "--data-source", dest="data_source", action="store", help="Data source to read from. Valid options are gwffile,lldata,lldata_sv,whitedata,fakeLIGO,fakeadvLIGO. One is required. If gwffile is selected, then a cache file should also be provided. If whitedata, fakeLIGO, or fakeadvLIGO is selected, then a sample rate must also be provided.")
parser.add_option("-S", "--stream-tfmap", dest="stream_tfmap", action="store", help="Encode the time frequency map to video as it is analyzed. If the argument to this option is \"video\" then the pipeline will attempt to stream to a video source. If the option is instead a filename, the video will be sent to that name. Prepending \"keyframe=\" to the filename will start a new video every time a keyframe is hit.")
parser.add_option("-s", "--gps-start", dest="gps_start", action="store", type="float", help="Seek to gps time before beginning analysis.", default=None)
parser.add_option("-e", "--gps-end", dest="gps_end", action="store", type="float", help="End the analysis at this gps time.", default=None)
parser.add_option("-t", "--disable-triggers", dest="disable_triggers", action="store_true", help="Don't record triggers.", default=False)
parser.add_option("-T", "--disable-file-cache", dest="disable_file_cache", action="store_true", help="Don't create file caches of output files. Default is to create file caches.", default=False)
parser.add_option("-c", "--lalapps-power-compatibility", dest="compat", action="store_true", default=False, help="Output trigger information which conforms to the lalapps_power conventions.")
parser.add_option("-C", "--clustering", dest="clustering", action="store_true", default=False, help="Employ trigger tile clustering before output stage. Default or if not specificed is off." )
parser.add_option("-u", "--enable-db-uploads", dest="db_uploads", action="store_true", default=False, help="Upload uploads to trigger archiving services (e.g. gracedb). The threshold for upload should be in the initialization file. Default or if not specificed is off." )

(options, args) = parser.parse_args()

# The data rate at which we wish to do analysis
# Assumed lower than the input data
data_source = options.data_source

# TODO: Stop reinventing the wheel and replace this with mkLLOIDsrc
valid_data_sources = [ "gwffile", 
	  "nds",
	  "lldata", 
	  "lldata_sv", 
	  "whitedata", 
	  "fakeLIGO", 
	  "fakeadvLIGO" ]
if( not data_source in valid_data_sources ):
	print >>sys.stderr, "Either no data soruce was selected, or an invalid one was requested."
	sys.exit(-1)

if( not options.sample_rate 
	and ( data_source in ["whitedata", "fakeLIGO", "fakeadvLIGO"] ) ):
	print >>sys.stderr, "Sample rate not specified and fake data requested."
	sys.exit(-1)
else:
	sample_rate = options.sample_rate

# Verbosity and diagnostics
verbose = options.verbose
diagnostics = options.diagnostics

#
# =============================================================================
#
#                           Handler / Pipeline options
#
# =============================================================================
#

# We need a pipeline and pipeline handler instance to configure
pipeline = gst.Pipeline( "gstlal_excesspower" )
mainloop = gobject.MainLoop()
handler = EPHandler(mainloop, pipeline)

#if( options.disable_triggers ): handler.output = None
if( options.disable_file_cache ): handler.output_cache = None

handler.rate = options.sample_rate

# Used to keep track if we need to lock the PSD into the whitener
psdfile = None
if( not options.infile ):
	print >>sys.stderr, "Initialization file required."
elif( not os.path.exists( options.infile ) ):
	print >>sys.stderr, "Initialization file path is invalid."
	sys.exit(-1)

cfg = SafeConfigParser()
cfg.read( options.infile )

# Instruments and channels
handler.inst = cfg.get( "instrument", "detector" )
try:
	handler.channel = cfg.get( "instrument", "channel" )
except ConfigParser.NoOptionError:
	handler.channel = None

if( options.channame is not None ):
	handler.channel = options.channame
if( handler.channel is None ):
	exit("No channel specified in the configuration file or on the command line.")
print "Channel name: " + handler.channel

try:
	site = cfg.get( "instrument", "site" )
except ConfigParser.NoOptionError:
	print >>sys.stderr, "No site requested, using detector as default cache sieve."
	site = None

# Handler options
try:
	handler.fft_length = cfg.getfloat( "tf_parameters", "fft-length" )
except ConfigParser.NoOptionError:
	pass

handler.flow = cfg.getfloat( "tf_parameters", "min-frequency" )
handler.fhigh = cfg.getfloat( "tf_parameters", "max-frequency" )
handler.base_band = cfg.getfloat( "tf_parameters", "base-resolution" )
handler.max_level = cfg.getint( "tf_parameters", "max-resolution-level" )
handler.max_duration = cfg.getfloat( "tf_parameters", "max-time-resolution" )
if( cfg.has_option( "cache", "reference-psd" ) ):
	psdfile = cfg.get( "cache", "reference-psd" )
	try:
		handler.psd = read_psd_xmldoc( utils.load_filename( psdfile, contenthandler = ligolw.LIGOLWContentHandler ) )[ handler.inst ]
		print "Reference PSD for instrument %s from file %s loaded" % ( handler.inst, psdfile )
		# Reference PSD disables caching (since we already have it
		handler.cache_psd = False
		# TODO: Make a --track-psd option
	except KeyError: # Make sure we have a PSD for this instrument
		sys.exit( "PSD for instrument %s requested, but not found in file %s. Available instruments are %s" % (handler.inst, psdfile, str(handler.psd.keys())) )

else:
	handler.cache_psd = cfg.get( "cache", "cache-psd" )
if( handler.cache_psd == "" ): handler.cache_psd = False
else: print "PSD caching enabled."

handler.cache_spec_corr = cfg.getboolean( "cache", "cache-spectral-correlation" )

handler.outfile = cfg.get( "triggering", "output-file" )
handler.snr_thresh = cfg.getfloat( "triggering", "snr-thresh" )
if( options.db_uploads ):
	handler.db_thresh = cfg.getfloat( "triggering", "db-thresh" )
	handler.db_client = cfg.get( "triggering", "db-client" )
	if( handler.db_thresh is None ):
		print >>sys.stderr, "Warning, DB upload requested, but no threshold provided. Disabling."
	if( handler.db_thresh is None ):
		print >>sys.stderr, "Warning, DB upload requested, but no DB path provided. Disablinhg"
		handler.db_thresh = None

if( cfg.has_option( "triggering", "events_per_file" ) ):
	handler.max_events = cfg.get_int( "triggering", "events_per_file" )

# If a specific (trigger) time is of interest, specify its GPS here
# TODO: Read from sngl_inspirals and sngl_bursts
trigger_begin, trigger_end = None, None
if( cfg.has_option( "triggering", "trig_time_start" ) ):
	trigger_begin = cfg.getfloat( "triggering", "trig_time_start" )
if( cfg.has_option( "triggering", "trig_time_end" ) ):
	trigger_end = cfg.getfloat( "triggering", "trig_time_end" )
if( trigger_begin and trigger_end ):
	handler.set_trigger_time_and_action( segment( trigger_begin, trigger_end ) )

gwflocation = cfg.get( "instrument", "location" )

inj_loc = cfg.get( "injections", "xml-location" )
if( not os.path.isfile( inj_loc ) ):
	print >>sys.stderr, "Injection file not found, disabling option."
	inj_loc = None

base_band = handler.base_band

# This is invoked here, or else the default rate is used, which will cause funny behavior for the defaults with some cases
# TODO: Less hardcodish -- update this when the rate or base_band is updated
handler.filter_len = 2*int(2*handler.rate/handler.base_band)
handler.build_default_psd( handler.rate, handler.filter_len )
handler.rebuild_filter()
handler.rebuild_chan_mix_matrix()

# Max trigger duration (s)
handler.max_duration

#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#

if options.verbose:
	print >>sys.stderr, "Assembling pipeline...\n",

duration = -1

# Data source
if( data_source == "nds" ):
	head = mkndssrc( pipeline,
		host = cfg.get( "instrument", "ndshost" ),
		instrument = handler.inst,
		channel_name = handler.channel
	)
	try:
		head.set_property( "port", cfg.getint( "instrument", "ndsport" ) )
	except ConfigParser.NoOptionError:
		print >>sys.stderr, "Warning, no NDS host port specified. Using default."

	# FIXME: Don't assume real-time
	head.set_property( "channel-type", "online" )
elif( data_source == "fakeLIGO" ):
	head = mkfakeLIGOsrc( pipeline, 
		instrument = handler.inst,
		channel_name = handler.channel
	)
elif( data_source == "fakeadvLIGO" ):
	head = mkfakeadvLIGOsrc( pipeline, 
		instrument = handler.inst,
		channel_name = handler.channel
	)
elif( data_source == "whitedata" ):
	head = gst.element_factory_make( "audiotestsrc" )
	pipeline.add( head )
	head.set_property( "wave", 9 ) # unity variance zero mean gaussian noise
elif( data_source == "lldata" ):
	# FIXME: Unhardcode this.
	shmmap = { "L1": "LLO_Data",
	           "H1": "LHO_Data",
	           "V1": "VIRGO_Data"}
	head = mklvshmsrc( pipeline, shm_name = shmmap[handler.inst] )
	src = head = mkframecppchanneldemux( pipeline, head )
	sink = mkaudiorate( pipeline, None, skip_to_first = True, silent = False )
	src_deferred_link( src, "%s:%s" % (handler.inst, handler.channel), sink.get_pad("sink") )
	head = sink

	# FIXME: This is a guess. Not a terrible one, but still inaccurate
	handler.start = gpstime.GpsSecondsFromPyUTC( time.time() )
	handler.time_since_dump = gpstime.GpsSecondsFromPyUTC( time.time() )
elif( data_source == "lldata_sv" ):
	# This is really only for the strain channels where one might anticipate needing a state vector.
	dqv = {
		# 1  Science mode on 
		"SCIENCE_MODE" : 0b1,
		# 2  ITF fully locked 
		"ITF_LOCK" : 0b10,
		# 3  h(t) reconstruction ok 
		"HREC_OK": 0b100,
		# 4  Reserved for future use
		"RESERVED" : 0b1000,
		# 5  CBC Injection 
		"CBC_INJECTION" : 0b10000,
		# 6  CBC_CAT1 
		"CBC_CAT1" : 0b100000,
		# 7  CBC_CAT2 [not used]
		"CBC_CAT2" : 0b1000000,
		# 8  CBC_CAT3 [not used]
		"CBC_CAT3" : 0b10000000,
		# 9  Burst injection 
		"BURST_INJECTION" : 0b100000000,
		# 10 Burst_CAT1 
		"BURST_CAT1" : 0b1000000000,
		# 11 Burst_CAT2 
		"BURST_CAT2" : 0b10000000000,
		# 12 Burst_CAT3 
		"BURST_CAT3" : 0b100000000000,
		# 13 CW injection 
		"CW_INJECTION" : 0b1000000000000,
		# 14 Reserved for future use, possibly by CW 
		# 15 Reserved for future use, possibly by CW 
		# 16 Reserved for future use, possibly by CW 
		# 17 Stochastic injection 
		"STOCHASTIC_INJECTION" : 0b10000000000000000
		# 18 Reserved for future use, possibly by stochastic 
		# 19 Reserved for future use, possibly by stochastic [not used]
		# 20 Reserved for future use, possibly by stochastic [not used]
	}
	DEFAULT_DQ_VECTOR_ON = dqv["SCIENCE_MODE"] | dqv["ITF_LOCK"] | dqv["HREC_OK"] # | dqv["BURST_CAT1"]
	DEFAULT_DQ_VECTOR_OFF = dqv["RESERVED"]
	head = mkLLOIDbasicsrc( pipeline,
		seekevent = None,
		instrument = handler.inst,
		detector = DetectorData(None, handler.channel),
		data_source = "online",
		state_vector_on_off_dict = { handler.inst: (DEFAULT_DQ_VECTOR_ON, DEFAULT_DQ_VECTOR_OFF) }
	)
	# FIXME: Need to get the gate by something other than name
	gate = pipeline.get_by_name( "lal_gate0" )
	gate.connect("start", handler.handle_segment, "on" )
	gate.connect("stop", handler.handle_segment, "off" )

	# FIXME: This is a guess. Not a terrible one, but still inaccurate
	handler.time_since_dump = gpstime.GpsSecondsFromPyUTC( time.time() )
elif( data_source == "gwffile" ):
	framesrc = head = mkframesrc( pipeline, 
		gwflocation, 
		handler.inst, 
		handler.channel
	)
	start, duration = ep.duration_from_cache( gwflocation )
	if( options.gps_start ):
		handler.start = start = options.gps_start 
	else:
		options.gps_start = handler.start = start
		options.gps_start = float(options.gps_start)
	if( options.gps_end ):
		duration = options.gps_end - start
	else:
		options.gps_end = start + duration
		options.gps_end = float(options.gps_end)
	handler.time_since_dump = start
	# TODO: Must unhardcode this -- but requires knowledge of native rate
	if( options.nbyte == 64 ):
		bsize=16384*8 # 64 bit stream
	elif( options.nbyte == 32 ):
		bsize=16384*4 # 32 bit stream
	if( verbose ):
		print "Blocksize %d" % bsize
	#duration = int(duration*handler.rate/16384.0)
	head.set_property( "blocksize", bsize )
	print >>sys.stderr, "Warning, inferring analysis duration from cache."

	# FIXME: Make this an option
	if( handler.inst == "V1" ): 
		sieve = "V*"
		head.set_property( "cache-dsc-regex", sieve )
	if( site ):
		sieve = site[0] + "*"
	else: sieve = handler.inst + "*"
	head.set_property( "cache-dsc-regex", sieve )
	
else:
	print >>sys.stderr, "Data source %s not recognized. Check the valid options in the help message."
	sys.exit(-1)

# Seeking
seekevent = None
if( options.gps_start is not None and options.gps_end is not None ):

	# TODO: Use LIGOTimeGPS
	print "Duration: " + str(duration)
	seek, dur = long(start*1e9), long(duration*1e9)
	print >>sys.stderr, "Seeking to GPS %d (ns), segment duration %d (ns)" % (seek, dur)
	print >>sys.stderr, "Will stop at GPS %d (ns)" % (seek + dur)
	seekevent = gst.event_new_seek( 1.0, 
		gst.Format(gst.FORMAT_TIME),
		gst.SEEK_FLAG_KEY_UNIT | gst.SEEK_FLAG_FLUSH,
		gst.SEEK_TYPE_SET, seek,
		gst.SEEK_TYPE_SET, seek + dur
	)

elif( options.gps_start is not None ):

	print >>sys.stderr, "Seeking to GPS %d (ns)" % seek
	seek = long(options.gps_start*1e9)
	seekevent = gst.event_new_seek( 1.0, 
		gst.Format(gst.FORMAT_TIME),
		gst.SEEK_FLAG_KEY_UNIT | gst.SEEK_FLAG_FLUSH,
		gst.SEEK_TYPE_SET, seek,
		gst.SEEK_TYPE_NONE, 0
	)

if( seekevent is not None ):
	if( head.set_state(gst.STATE_READY) != gst.STATE_CHANGE_SUCCESS ):
		exit("Unable to ready pipeline to accept seek.")
	if( not head.send_event( seekevent ) ):
		exit("Unable to send seek event to " + str(head))

# Diagnostic plot
if( diagnostics ):
	head = postdatatee = mktee( pipeline, head )
	mknxydumpsink( pipeline, 
		mkqueue( pipeline, postdatatee ), 
		cfg.get( "diagnostics", "strain-data-output" )
	)

# Convert to 64 bit
head = mkcapsfilter( pipeline, mkaudioconvert( pipeline, head ), "audio/x-raw-float,width=64" )
# Data conditioning
head = mkcapsfilter( pipeline, mkresample( pipeline, head ), "audio/x-raw-float,rate=%d" % handler.rate )

if( inj_loc ):
	head = mkinjections( pipeline, head, inj_loc )

if( inj_loc and verbose ):
	head = mkprogressreport( pipeline, head, "injection stream" )

head = whitener = mkwhiten( pipeline, head )
whitener.set_property( "fft-length", handler.fft_length ) # GSTLAL_PSDMODE_FIXED
if( psdfile is not None ): # In other words, we have a reference PSD
	whitener.set_property( "mean-psd", handler.psd.data )
	whitener.set_property( "psd-mode", 1 ) # GSTLAL_PSDMODE_FIXED

head = mkqueue( pipeline, head )

# Diagnostic plot
if( diagnostics ):
	head = postresamptee = mktee( pipeline, head )
	mknxydumpsink( pipeline, 
		mkqueue( pipeline, head ), 
		cfg.get( "diagnostics", "whitened-data-output" )
	)

if( verbose ):
	head = mkprogressreport( pipeline, head, "whitened stream" )

# excess power channel firbank
# NOTE: This is where the inspiral pipeline will feed in?
head = mkfirbank( pipeline, head, time_domain=False, block_stride=handler.rate )

# TODO: Make this less hardcodish
# This needs to be done since what is returned by mkfirbank is actually a link to the nofakedisconts element, so the fir bank is hidden
handler.add_firbank( head )
nchannels = handler.filter_bank.shape[0]
print "FIR bank constructed with %d %f Hz channels" % (nchannels, base_band)

if( verbose ):
	head = mkprogressreport( pipeline, head, "FIR bank stream" )

# TODO: Uncomment here
#head = postfirtee = mkqueue( pipeline, mktee( pipeline, head ) )
#####

if( diagnostics ):
	mknxydumpsink( pipeline, 
		postfirtee, 
		cfg.get( "diagnostics", "fir-output" )
	)

# First branch -- send fully sampled data to wider channels for processing
nlevels = int(numpy.ceil( numpy.log2( nchannels ) )) 
for res_level in range(0, min(handler.max_level, nlevels)):
	# TODO: Uncomment here
	#head = postfirtee
	#######

	band = base_band * 2**res_level
	# TODO: Check this
	chan = numpy.ceil( nchannels / 2.0**res_level )

	# The undersample_rate for band = R/2 is => sample_rate (passthrough)
	undersamp_rate = 2 * band

	#head = mkgeneric( pipeline, head, "lal_audioundersample" )

	# If the rate which would be set by the undersampler falls below one, we have
	# to take steps to prevent this, as gstreamer can't handle this. The 
	# solution is to change the "units" of the rate. Ideally, this should be done
	# much earlier in the pipeline (e.g. as the data comes out of the source),
	# however, to avoid things like figuring out what that means for the FIR bank
	# we change units here, and readjust appropriately in the trigger output.
	if( undersamp_rate < 1 ):
		print "Automatically adjusting units to compensate for undersample rate falling below unity."
		# No, it's not factors of ten, but rates which aren't factors
		# of two are often tricky, thus if the rate is a factor of two, the units
		# conversion won't change that.
		if( undersamp_rate > EXCESSPOWER_UNIT_SCALE['mHz'] ):
			unit = 'mHz'
		elif( undersamp_rate > EXCESSPOWER_UNIT_SCALE['uHz'] ):
			unit = 'uHz'
		elif( undersamp_rate > EXCESSPOWER_UNIT_SCALE['nHz'] ):
			unit = 'nHz'
		else:
			sys.exit( "Requested undersampling rate would fall below 1 nHz." )
		# FIXME: No love for positive power of 10 units?

		handler.units = EXCESSPOWER_UNIT_SCALE[unit]
		undersamp_rate /= handler.units
		print "Undersampling rate for level %d: %f %s" % (res_level, undersamp_rate, unit)
		head = mkcapssetter( pipeline, head, "audio/x-raw-float,rate=%d" % (handler.rate/handler.units), replace=False )
		head = mkgeneric( pipeline, head, "lal_audioundersample" )
		head = mkcapssetter( pipeline, head, "audio/x-raw-float,rate=%d" % undersamp_rate, replace=False )
	else:
		print "Undersampling rate for level %d: %f Hz" % (res_level, undersamp_rate)
		head = mkgeneric( pipeline, head, "lal_audioundersample" )
		head = mkcapsfilter( pipeline, head, "audio/x-raw-float,rate=%d" % undersamp_rate )

	if( diagnostics ):
		head = postustee = mktee( pipeline, mkqueue( pipeline, head ) )
		mknxydumpsink( pipeline, postustee, "postundersamp_res_%d.txt" % res_level )

	if( verbose ):
		head = mkprogressreport( pipeline, head, 
			"Undersampled stream level %d" % res_level
		)

	head = matmixer = mkmatrixmixer( pipeline, head )
	handler.add_matmixer( matmixer, res_level )

	if( verbose ):
		head = mkprogressreport( pipeline, head,
			"post matrix mixer %d" % res_level 
		)

	if( diagnostics ):
		head = postmmtee = mktee( pipeline, mkqueue( pipeline, head ) )
		mknxydumpsink( pipeline, postmmtee, "postmatmix_res_%d.txt" % res_level )

	head = mkgeneric( pipeline, head, "pow" )
	head.set_property( "exponent", 2 )

	if( verbose ):
		head = mkprogressreport( pipeline, head, 
			"Energy stream level %d" % res_level
		)

	# TODO: Uncomment here
	#head = mkqueue( pipeline, head )
	#####

	ndof = 2 # samples -- min number
	# Second branch -- duration
	# max_samp = int(handler.max_duration*rate)
	#while duration <= max_samp:
		#duration = duration << 1

	# Multi channel FIR filter -- used to add together frequency bands into tiles
	# FIXME: This is a workaround around until the audiofirfilter is fixed.
	# Basically, if the firfilter gets a small enough kernel, it invokes 
	# time-domain convolution. Something about the way it fills the buffers is
	# wrong in regards to the way it keeps history and thus the output buffers
	# partially desynched from the input -- even with an identity transform.

	# Workaround: Zero pad the kernel past the length (32 samples) in which the 
	# element switches to FFT convolution which doesn't exhibit bad behavior.

	#head = mkchecktimestamps( pipeline, head, "before audiofir" )
	head = mkgeneric( pipeline, head, "audiofirfilter" )
	head.set_property( "kernel", 
		ep.build_fir_sq_adder( ndof, padding=max(0, 33-ndof) )
	)
	head = mknofakedisconts( pipeline, head )
	#head = mkchecktimestamps( pipeline, head, "after audiofir" )

	if( diagnostics ):
		head = postdurtee = mktee( pipeline, mkqueue( pipeline, head ) )
		mknxydumpsink( pipeline, postdurtee, "postdur_res_%d_%d.txt" % (res_level, ndof) )

	if( verbose ):
		head = mkprogressreport( pipeline, head, 
			"After energy summation resolution level %d, %d DOF" % 
				(res_level, ndof) 
		)

	# Reenable for amplitude SNR stream
	#head = mkgeneric( pipeline, head, "pow" )
	#head.set_property( "exponent", 0.5 )

	# TODO: Audio
	if( options.stream_tfmap ):
		if len(options.stream_tfmap.split("=")) == 2:
			split_opt, filename = options.stream_tfmap.split("=")
		else:
			filename = options.stream_tfmap
			# TODO: Make this more elegant
			if( filename == "video" ): filename = None
			split_opt = None

		head = stream_tfmap_video( pipeline, head, 
			handler, 
			filename,
			split_opt
		)

	if( options.disable_triggers ):
		mkfakesink( pipeline, head )
		continue

	# Trigger generator
	head = mkbursttriggergen( pipeline, head, ndof, 
		bank = handler.build_filter_xml( res_level )
	)

	if( handler.fap is not None ):
		# Still needs magic number... or use the EP version
		# ndof_eff = ndof * 0.62
		snr_thresh = ep_utils.determine_thresh_from_fap(fap, ndof)
	else:
		# TODO: Make clear in the ini file that the thresh is power SNR, not amplitude and then remove the square here
		snr_thresh = handler.snr_thresh**2
	head.set_property( "snr-thresh", snr_thresh )

	if( verbose ):
		head = mkprogressreport( pipeline, head, 
			"Trigger generator resolution level %d, %d DOF" % (res_level, ndof) 
		)

	# TODO: combine trigger streams from various levels

	# TODO: This will have to be linked to multiple outgoing streams
	appsink = mkappsink(pipeline, mkqueue(pipeline, head))
	appsink.connect_after("new-buffer", get_triggers, handler, ndof)

### END OF PIPELINE

# Spectrum notification processing
whitener.connect_after( "notify::mean-psd", on_psd_change, handler )
# Handle spectral correlation changes
# TODO: Make sure this doesn't have to be in the mm loop
whitener.connect_after( "notify::spectral-correlation", on_spec_corr_change, handler )

# Handle shutdowns
signal.signal( signal.SIGINT, handler.shutdown )
signal.signal( signal.SIGTERM, handler.shutdown )

print >>sys.stderr, "Startin' up."
pipeline.set_state( gst.STATE_PLAYING )
if( diagnostics ):
	write_dump_dot(pipeline, "test", verbose = True)
	#gst.DEBUG_BIN_TO_DOT_FILE( pipeline,
		#gst.DEBUG_GRAPH_SHOW_ALL,
		#cfg.get( "diagnostics", "dot-file-location" )
	#)
mainloop.run()

