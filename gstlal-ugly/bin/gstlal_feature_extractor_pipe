#!/usr/bin/env python
#
# Copyright (C) 2011-2018 Chad Hanna, Duncan Meacher, Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
This program makes a dag to run gstlal_feature_extractor offline
"""

__author__ = 'Duncan Meacher <duncan.meacher@ligo.org>, Patrick Godwin <patrick.godwin@ligo.org>'

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, stat
import itertools
import numpy
import math
from optparse import OptionParser

##############################################################################
# import the modules we need to build the pipeline
import lal
import lal.series
from lal.utils import CacheEntry
from glue import pipeline
from glue.lal import Cache
from glue import segments
from glue.ligolw import ligolw
from glue.ligolw import lsctables
import glue.ligolw.utils as ligolw_utils
import glue.ligolw.utils.segments as ligolw_segments
from gstlal import inspiral, inspiral_pipe
from gstlal import dagparts as gstlaldagparts
from gstlal import datasource
from gstlal import multichannel_datasource
from gstlal import idq_multirate_datasource
from gstlal import idq_utils

class LIGOLWContentHandler(ligolw.LIGOLWContentHandler):
	pass
lsctables.use_in(LIGOLWContentHandler)

#
# get a dictionary of all the segments
#

def breakupseg(seg, maxextent, overlap):
	if maxextent <= 0:
		raise ValueError, "maxextent must be positive, not %s" % repr(maxextent)

	# Simple case of only one segment
	if abs(seg) < maxextent:
		return segments.segmentlist([seg])

	# adjust maxextent so that segments are divided roughly equally
	maxextent = max(int(abs(seg) / (int(abs(seg)) // int(maxextent) + 1)), overlap)
	maxextent = int(math.ceil(abs(seg) / math.ceil(abs(seg) / maxextent)))
	end = seg[1]

	seglist = segments.segmentlist()

	while abs(seg):
		if (seg[0] + maxextent + overlap) < end:
			# Round down segment gps end time to integer multiple of cadence.
			seglist.append(segments.segment(seg[0], idq_utils.floor_div(int(seg[0]) + maxextent + overlap, options.cadence)))
			seg = segments.segment(seglist[-1][1] - overlap, seg[1])
		else:
			seglist.append(segments.segment(seg[0], end))
			break

	return seglist

def breakupsegs(seglist, maxextent, overlap):
	newseglist = segments.segmentlist()
	for bigseg in seglist:
		newseglist.extend(breakupseg(bigseg, maxextent, overlap))
	return newseglist

def analysis_segments(ifo, allsegs, boundary_seg, segment_length, max_template_length = 30):
	segsdict = segments.segmentlistdict()
	# 512 seconds for the whitener to settle + the maximum template_length
	start_pad = idq_multirate_datasource.PSD_DROP_TIME + max_template_length

	segsdict[ifo] = segments.segmentlist([boundary_seg])
	segsdict[ifo] = segsdict[ifo].protract(start_pad)
	# FIXME revert to gstlaldagparts.breakupsegs and remove above two functions when we no longer write to ascii.
	segsdict[ifo] = breakupsegs(segsdict[ifo], segment_length, start_pad)
	#segsdict[ifo] = gstlaldagparts.breakupsegs(segsdict[ifo], segment_length, start_pad)
	if not segsdict[ifo]:
		del segsdict[ifo]

	return segsdict

#
# get a dictionary of all the channels per gstlal_feature_extractor job
#

def feature_extractor_node_gen(gstlalFeatureExtractorJob, dag, parent_nodes, segsdict, ifo, options, data_source_info, max_template_length = 30):
	feature_extractor_nodes = {}

	# parallelize jobs by channel subsets
	for ii, channel_subset in enumerate(data_source_info.channel_subsets):

		# parallelize jobs by segments
		for seg in segsdict[ifo]:

			# only produce jobs where the analysis runtime after applying segments is nonzero
			if not data_source_info.frame_segments[ifo].intersects_segment(seg):
				if options.verbose:
					print "Skipping %s since there is no analyzable data here" % repr(seg)
				continue

			# set maximum number of jobs reading concurrently from the same frame file to prevent I/O locks
			if ii / options.concurrency == 0:
				dep_nodes = parent_nodes
			else:
				dep_nodes = [feature_extractor_nodes[(ii - options.concurrency, seg)]]

			# creates a list of channel names with entries of the form --channel-name=IFO:CHANNEL_NAME:RATE
			channels = [''.join(["--channel-name=",':'.join([channel, str(int(data_source_info.channel_dict[channel]['fsamp']))])]) for channel in channel_subset]

			# FIXME: hacky way of getting options to get passed correctly for channels
			channels[0] = channels[0].split('=')[1]

			outpath = os.path.join(options.out_path, "gstlal_feature_extractor")
			trig_start = int(seg[0]) + idq_multirate_datasource.PSD_DROP_TIME + max_template_length

			feature_extractor_nodes[(ii, seg)] = \
				inspiral_pipe.generic_node(gstlalFeatureExtractorJob, dag, parent_nodes = dep_nodes,
					opts = {"gps-start-time":int(seg[0]),
						"gps-end-time":int(seg[1]),
						"feature-start-time":int(trig_start),
						"feature-end-time":int(seg[1]),
						"data-source":"frames",
						"mismatch":options.mismatch,
						"waveform":options.waveform,
						"qhigh":options.qhigh,
						"channel-name":' '.join(channels),
						"job-id":str(ii + 1).zfill(4),
						"cadence":options.cadence,
						"max-streams":options.max_serial_streams,
						"disable-web-service":options.disable_web_service,
						"local-frame-caching":options.local_frame_caching,
						"frame-segments-name": options.frame_segments_name,
						"save-format": options.save_format,
						"verbose":options.verbose
					},
					input_files = {"frame-cache":options.frame_cache,
						"frame-segments-file":options.frame_segments_file},
					output_files = {"out-path":outpath}
				)
			if options.verbose:
				print "Creating node for index, segment %s" % repr((ii, seg))

	return feature_extractor_nodes

#
# Main
#

def parse_command_line():
	parser = OptionParser(description = __doc__)

	# generic data source options
	multichannel_datasource.append_options(parser)

	# trigger generation options
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	parser.add_option("--disable-web-service", action = "store_true", help = "If set, disables web service that allows monitoring of PSDS of aux channels.")
	parser.add_option("--local-frame-caching", action = "store_true", help = "Pre-reads frame data and stores to local filespace.")
	parser.add_option("--description", metavar = "string", default = "GSTLAL_IDQ_TRIGGERS", help = "Set the filename description in which to save the output.")
	parser.add_option("--save-format", action = "store_true", default = "hdf5", help = "Specifies the save format (ascii or hdf5) of features written to disk. Default = hdf5")
	parser.add_option("--cadence", type = "int", default = 32, help = "Rate at which to write trigger files to disk. Default = 32 seconds.")
	parser.add_option("-m", "--mismatch", type = "float", default = 0.05, help = "Mismatch between templates, mismatch = 1 - minimal match. Default = 0.05.")
	parser.add_option("-q", "--qhigh", type = "float", default = 100, help = "Q high value for half sine-gaussian waveforms. Default = 100.")
	parser.add_option("--max-parallel-streams", type = "int", default = 50, help = "Number of streams (sum(channel_i * num_rates_i)) to process in parallel. This gives the maximum number of channels to process for a given job. Default = 50.")
	parser.add_option("--max-serial-streams", type = "int", default = 100, help = "Number of streams (sum(channel_i * num_rates_i)) to process serially within a given job. Default = 100.")
	parser.add_option("--concurrency", type = "int", default = 4, help = "Maximum allowed number of parallel jobs reading from the same file, done to prevent I/O locks")
	parser.add_option("--segment-length", type = "int", default = 6000, help = "Maximum segment length to process per job. Default = 6000 seconds.")
	parser.add_option("-l", "--latency", action = "store_true", help = "Print latency to output ascii file. Temporary.")
	parser.add_option("--waveform", metavar = "string", default = "half_sine_gaussian", help = "Specifies the waveform used for matched filtering. Possible options: (half_sine_gaussian, sine_gaussian). Default = half_sine_gaussian")
	parser.add_option("--save-hdf", action = "store_true", default = False, help = "If set, will save hdf5 files to disk straight from dataframe once every cadence")
	parser.add_option("--out-path", metavar = "path", default = ".", help = "Write to this path. Default = .")

	# Condor commands
	parser.add_option("--request-cpu", default = "2", metavar = "integer", help = "set the requested node CPU count, default = 2")
	parser.add_option("--request-memory", default = "8GB", metavar = "integer", help = "set the requested node memory, default = 8GB")
	parser.add_option("--request-disk", default = "50GB", metavar = "integer", help = "set the requested node local scratch space size needed, default = 50GB")
	parser.add_option("--condor-command", action = "append", default = [], metavar = "command=value", help = "set condor commands of the form command=value; can be given multiple times")

	options, filenames = parser.parse_args()

	# set max parallel streams to options.max_streams for use in data_source_info for splitting up channel lists to process in parallel
	options.max_streams = options.max_parallel_streams

	# FIXME: once we figure out what the maximum concurrency is for parallel reads, should set that as a sanity check

	# sanity check to enforce a minimum segment length
	# Minimum segment length chosen so that the overlap is a ~33% hit in run time
	min_segment_length = int(4 * idq_multirate_datasource.PSD_DROP_TIME)
	assert options.segment_length >= min_segment_length


	return options, filenames

#
# Useful variables
#

options, filenames = parse_command_line()

output_dir = "plots"

listdir = os.path.join(options.out_path, "gstlal_feature_extractor/channel_lists")
if not os.path.exists(listdir):
    os.makedirs(listdir)

#
#
#

data_source_info = multichannel_datasource.DataSourceInfo(options)
ifo = data_source_info.instrument
channels = data_source_info.channel_dict.keys()
boundary_seg = data_source_info.seg

# FIXME Work out better way to determine max template length
max_template_length = 30

#
# Setup the dag
#

try:
	os.mkdir("logs")
except:
	pass
dag = inspiral_pipe.DAG("feature_extractor_pipe")

#
# setup the job classes
#

gstlalFeatureExtractorJob = inspiral_pipe.generic_job("gstlal_feature_extractor", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":options.request_memory, "request_cpus":options.request_cpu, "request_disk":options.request_disk, "want_graceful_removal":"True", "kill_sig":"15"}))

segsdict = analysis_segments(ifo, data_source_info.frame_segments, boundary_seg, options.segment_length, max_template_length=max_template_length)

#
# feature extractor jobs
#

feature_extractor_nodes = feature_extractor_node_gen(gstlalFeatureExtractorJob, dag, [], segsdict, ifo, options, data_source_info)

#
# all done
#

dag.write_sub_files()
dag.write_dag()
dag.write_script()
