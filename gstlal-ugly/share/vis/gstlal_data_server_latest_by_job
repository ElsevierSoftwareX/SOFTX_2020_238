#!/usr/bin/python

import gviz_api
import h5py
import os,sys
import cgi
import cgitb
cgitb.enable()
form = cgi.parse()
import numpy
import re
import time
from scipy.interpolate import interp1d
#import pyparsing

def now():
	#FIXME use pylal when available
	return time.time() - 315964785

def parse_form(form):
	# we have to get a query according to the google standard
	assert "tq" in form
	assert "tqx" in form
	reqId = form["tqx"][0].split(":")[1]
	query = form["tq"][0]

	# FIXME these are not google API complient.
	# Figure out if a GPS time is being specified, else assume "now"
	if "gpstime" in form:
		query_time = form["gpstime"][0]
		if query_time != "-1":
			query_time = int(query_time)
		else:
			query_time = int(now())
	else:
		query_time = int(now())

	if "duration" in form:
		duration = form["duration"][0]
		if duration != "-1":
			duration = int(duration)
		else:
			duration = 1000
	else:
		duration = int(now())


	if "type" in form:
		datatype = form["type"]
	else:
		datatype = "median"

	# FIXME don't hard code, get from URL
	base_path = "/home/gstlalcbctest/engineering/10/S6/bns"
	jobdirs = ["by_job/%04d" % i for i in range(10)]

	return reqId, query, query_time, duration, datatype, base_path, jobdirs


def gps_by_level(t, level):
	return "/".join([x for x in str(t)[0:level]])


def get_partial_paths_to_aggregated_data(query_time, duration):
	if query_time is None:
		return [""]
	level = 9 - int(numpy.ceil(numpy.log10(10000)))
	prev_time = query_time - 10**(10-level)
	return [gps_by_level(prev_time, level), gps_by_level(query_time, level)]


# Always print a header first
print "Content-type: application/json"
print "Cache-Control: max-age=10"
print


reqId, query, query_time, duration, datatype, base_path, jobdirs = parse_form(form)

#
# "SQL" parser.  FIXME. First, google query langauge isn't really SQL and
# second, if this keeps going we need to switch to a parsing library e.g.,
# pyparsing.  For now this is likely to be a collection of once-offs.
#

def stats_on_data(data):
	return float(numpy.min(data)), float(numpy.percentile(data, 15.9)), float(numpy.percentile(data, 84.1)), float(numpy.max(data))

def read_aggregated_data_by_job(route, query_time, duration, base_path, jobdirs = [""], rootdir = "aggregator", datatype = "median"):
	max_time = []
	for jobdir in jobdirs:
		this_data = numpy.empty((0))
		this_time = numpy.empty((0))
		for partial_path in get_partial_paths_to_aggregated_data(query_time, duration):
			try:
				fname = "%s/%s/%s/%s/%s/%s.hdf5" % (base_path, rootdir, partial_path, jobdir, datatype, route)
				f = h5py.File(fname, "r")
				this_data = numpy.hstack((this_data, numpy.array(f["data"])))
				this_time = numpy.hstack((this_time, numpy.array(f["time"])))
			except IOError:
				pass
		if query_time is not None:
			ix = numpy.logical_and(this_time > (query_time-duration), this_time <= query_time)
			this_time = this_time[ix]
			this_data = this_data[ix]
			if len(this_time) == 0: # possible there is no data, so register the query time
				this_time = numpy.array([query_time - duration, query_time])
				# FIXME allow a different default value
				this_data = numpy.array([0,0])
		yield jobdir, this_time, this_data

#
# latency, SNR, FAR, likelihood
#

for route, label in (("latency_history", "Latency (s)"), ("snr_history", "SNR"), ("far_history", "FAR (Hz)"), ("likelihood_history", "L")):
	if route in query:
		if "status by node" in query:
			# get the max time and the stats on the data
			data = [(max(x[1]), x[0].split("/")[-1], stats_on_data(x[2])) for x in read_aggregated_data_by_job(route, query_time, duration, base_path, jobdirs, datatype = "median")]
			print data
			description = [
				("job", "string"),
				("%d" % float(max(data)[0]), "number"),
				("j", "number"),
				("", "number"),
				("", "number")
				]
			
			data_table = gviz_api.DataTable(description)
			data_table.LoadData([[x[1]]+list(x[2]) for x in data])
			print data_table.ToJSonResponse(order_by = "job", req_id = reqId)
			sys.exit()

		if "where node is all" in query:
			_, this_time, this_data = read_aggregated_data_by_job(route, query_time, duration, base_path, jobdirs = [""], datatype = "median").next()
			out_data = [[float(t),float(d)] for t,d in zip(this_time, this_data)]
			description = [("time", "number"), ("%d" % float(this_time[-1]), "number")]
			data_table = gviz_api.DataTable(description)
			data_table.LoadData(out_data)
			print data_table.ToJSonResponse(order_by = "time", req_id = reqId)
			sys.exit()

		if "now" in query:
			_, this_time, this_data = read_aggregated_data_by_job(route, query_time, duration, base_path, jobdirs = [""], datatype = "median").next()
			description = [(label, "number")]
			data_table = gviz_api.DataTable(description)
			data_table.LoadData([[float(this_data[-1])]])
			print data_table.ToJSonResponse(req_id = reqId)
			sys.exit()


#
# horizon history and noise
#

for route, x, y, units in (("horizon_history", "time", "horizon", "(Mpc)"), ("noise", "time", "noise", "")):
	if route in query:
		latest = {}
		if "now" in query:
			for ifo, this_data, this_time in read_aggregated_data_by_job(route, query_time, duration, base_path, jobdirs = ["H1", "L1"], rootdir = "dq", datatype = "max"):
				latest[ifo] = float(this_time[-1]), float(this_data[-1])
			out_data = [[float(latest["H1"][1]), float(latest["L1"][1])]]
			description = [
				("H1 %s" % (units,), "number"),
				("L1 %s" % (units,), "number")
				]
		else:
			out_data = []; H1L1time = {}; H1L1data = {}
			# FIXME don't harcode
			for ifo, this_data, this_time in read_aggregated_data_by_job(route, query_time, duration, base_path, jobdirs = ["H1", "L1"], rootdir = "dq", datatype = "max"):
				H1L1data[ifo] = interp1d(this_time, this_data, fill_value = 0., bounds_error = False)
				H1L1time[ifo] = this_time

			for t in numpy.hstack((H1L1time["H1"], H1L1time["L1"])):
				out_data.append([float(t), float(H1L1data["H1"](t)), float(H1L1data["L1"](t))])
			description = [
				(x, "number"),
				("H1 %s @ %d" % (units, float(H1L1time["H1"][-1])), "number"),
				("L1 %s @ %d" % (units, float(H1L1time["L1"][-1])), "number")
				]
		
		data_table = gviz_api.DataTable(description)
		data_table.LoadData(out_data)
		print data_table.ToJSonResponse(order_by = "time", req_id = reqId)


#
# uptime, dropped data and individual ifo SNR
#

for route, label in (("_state_vector_on_off_gap", "(s)"), ("_strain_add_drop", "(s)")):
	if route in query:
		out_data = []
		if "status by node" in query:
			jobs = []; this_time = {}; this_data = {};
			for ifo in ("H1", "L1"):
				for job, t, d in read_aggregated_data_by_job("%s%s" % (ifo, route), None, None, base_path, jobdirs = jobdirs, rootdir = "aggregator", datatype = ""):
					this_time.setdefault(ifo, []).append(t[-1])
					this_data.setdefault(ifo, []).append(d[-1])
					jobs.append(job.split("/")[-1])
			# FIXME the 16 comes from the 16 Hz sample rate
			# used for state vectors in ALIGO. if that
			# changes this needs to change too
			for job, H1d, L1d in zip(jobs, this_data["H1"], this_data["L1"]):
				out_data.append([job, H1d / 16., L1d/ 16.])
			description = [
				("job", "string"),
				("H1 %d" % float(max(this_time["H1"])), "number"),
				("L1 %d" % float(max(this_time["L1"])), "number"),
				]
		
		data_table = gviz_api.DataTable(description)
		data_table.LoadData(out_data)
		print data_table.ToJSonResponse(order_by = "job", req_id = reqId)
		sys.exit()


#
# RAM history
#

for route, label in (("ram_history", "(GB)"),):
	if route in query:
		out_data = []
		if "status by node" in query:
			# Set the duration for the highest level
			for job, this_time, this_data in read_aggregated_data_by_job(route, None, None, base_path, jobdirs = jobdirs, rootdir = "aggregator", datatype = ""):
				out_data.append([job.split("/")[-1], float(this_data[-1])])
			description = [
				("job", "string"),
				("RAM %d" % int(this_time[-1]), "number"),
				]
			data_table = gviz_api.DataTable(description)
			data_table.LoadData(out_data)
			print data_table.ToJSonResponse(order_by = "job", req_id = reqId)
			sys.exit()

#
# Time since last
#

if "time_since_last" in query:
	if "status by node" in query:
		nowtime = now()
		out_data = []
		for job, this_time, this_data in read_aggregated_data_by_job("ram_history", None, None, base_path, jobdirs = jobdirs, rootdir = "aggregator", datatype = ""):
			out_data.append([job.split("/")[-1], nowtime - this_time[-1]])
		description = [
			("job", "string"),
			("Time since %d" % nowtime, "number"),
			]
		data_table = gviz_api.DataTable(description)
		data_table.LoadData(out_data)
		print data_table.ToJSonResponse(order_by = "job", req_id = reqId)
		sys.exit()


#
# PSDs
#

freq = numpy.array([])
datadict = {"H1": numpy.array([]), "L1": numpy.array([])}
out_data = []
lowestdir = gps_by_level(query_time, 6)
if "psd" in query:
	for ifo in ("H1", "L1"):
		if "now" in query:
			fname = "%s/dq/%s/%s/psd.hdf5" % (base_path, ifo, lowestdir)
		else:
			# PSD files are like: H1-PSD-1159632700-100.hdf5
			fname = "%s/dq/%s/%s/%s-PSD-%d-100.hdf5" % (base_path, ifo, lowestdir, ifo, int(round(query_time,-2)))
		try:
			f = h5py.File(fname, "r")
			datadict[ifo] = numpy.array(f["asd"])
			freq = numpy.array(f["freq"])
			f.close()
		except IOError:
			pass
	# FIXME don't harcode
	for f, H1asd, L1asd in zip(freq, datadict["H1"], datadict["L1"]):
		out_data.append([float(f), float(H1asd), float(L1asd)])
	description = [
		("freq", "number"),
		("H1 ASD @ %d" % query_time, "number"),
		("L1 ASD @ %d" % query_time, "number")
		]
	
	data_table = gviz_api.DataTable(description)
	data_table.LoadData(out_data)
	print data_table.ToJSonResponse(req_id = reqId)


