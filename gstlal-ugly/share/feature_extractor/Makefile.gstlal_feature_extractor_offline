SHELL := /bin/bash
# condor commands
# Set the accounting tag from https://ldas-gridmon.ligo.caltech.edu/ldg_accounting/user
ACCOUNTING_TAG=ligo.dev.o3.detchar.onlinedq.idq
GROUP_USER=albert.einstein
CONDOR_COMMANDS:=--condor-command=accounting_group=$(ACCOUNTING_TAG) --condor-command=accounting_group_user=$(GROUP_USER)

#########################
# Triggering parameters #
#########################

SEG_PAD = 1000

# The GPS start time for analysis
START = 1187000000
FSTART=$(shell echo $$((${START}-${SEG_PAD})))

# The GPS end time for analysis
STOP  = 1187100000

OUTPATH = $(PWD)

# channel list for analysis
CHANNEL_LIST = H1_O2_standard_channel_list.txt

# Target number of streams (N_channels x N_rates_per_channel) that each cpu will process
# NOTE: * if max_serial_streams > max_parallel_streams, all jobs will be parallelized by channel
#       * if max_parallel_streams > num_channels in channel list, all jobs will be processed serially, with processing driven by max_serial_streams
#       * any other combination will produce a mix of parallelization by channels and processing channels serially per job
MAX_PARALLEL_STREAMS = 300
MAX_SERIAL_STREAMS = 110

# Maximum number of concurrent reads from the same frame file, done to prevent I/O locks
CONCURRENCY = 1

# Length of time to process for a given job
SEGMENT_LENGTH = 3500

# Waveform type and parameter space settings
WAVEFORM = half_sine_gaussian
MISMATCH = 0.05
QHIGH = 100

# Detector
CLUSTER:=$(shell hostname -d)

IFO = H1
#IFO = L1

###############################
# Segment and frame type info #
###############################

# Info from https://wiki.ligo.org/viewauth/LSC/JRPComm/ObsRun2
# Select correct calibration type
# GSTLAL_SEGMENTS Options
SEG_SERVER=https://segments.ligo.org
# C00
LIGO_SEGMENTS="$(IFO):DMT-ANALYSIS_READY:1"
# C01
#LIGO_SEGMENTS="$*:DCS-ANALYSIS_READY_C01:1"
# C02
#LIGO_SEGMENTS="$*:DCS-ANALYSIS_READY_C02:1"

SEGMENT_TRIM = 0
SEGMENT_MIN_LENGTH = 512

FRAME_TYPE=R

#################
# Web directory #
#################

# A user tag for the run
#TAG = O2_C00
# Run number
#RUN = run_1
# A web directory for output (note difference between cit+uwm and Atlas)
# cit & uwm
#WEBDIR = ~/public_html/observing/$(TAG)/$(START)-$(STOP)-$(RUN)
# Atlas
#WEBDIR = ~/WWW/LSC/testing/$(TAG)/$(START)-$(STOP)-test_dag-$(RUN)

############
# Workflow #
############

all : dag
	sed -i '/gstlal_feature_extractor / s/$$/ |& grep -v '\''XLAL\|GSL\|Generic'\''/' feature_extractor_pipe.sh
	@echo "Submit with: condor_submit_dag feature_extractor_pipe.dag"

# Run etg pipe to produce dag
dag : frame.cache plots $(CHANNEL_LIST) segments.xml.gz
	gstlal_feature_extractor_pipe \
		--data-source frames \
		--gps-start-time $(START) \
		--gps-end-time $(STOP) \
		--frame-cache frame.cache \
		--frame-segments-file segments.xml.gz \
		--frame-segments-name datasegments \
		--local-frame-caching \
		--waveform $(WAVEFORM) \
		--channel-list $(CHANNEL_LIST) \
		--out-path $(OUTPATH) \
		--max-serial-streams $(MAX_SERIAL_STREAMS) \
		--max-parallel-streams $(MAX_PARALLEL_STREAMS) \
		--concurrency $(CONCURRENCY) \
		--segment-length $(SEGMENT_LENGTH) \
		--mismatch $(MISMATCH) \
		--qhigh $(QHIGH) \
		$(CONDOR_COMMANDS) \
		--request-cpu 2 \
		--request-memory 14GB \
		--request-disk 12GB \
		--verbose \
		--disable-web-service

#		--web-dir $(WEBDIR) \

# FIXME Determine channel list automatically.
#full_channel_list.txt : frame.cache
#	FrChannels $$(head -n 1 $^ | awk '{ print $$5}' | sed -e "s@file://localhost@@g") > $@

# Produce segments file
segments.xml.gz : frame.cache
	ligolw_segment_query_dqsegdb --segment-url=${SEG_SERVER} -q --gps-start-time ${FSTART} --gps-end-time ${STOP} --include-segments=$(LIGO_SEGMENTS) --result-name=datasegments > $@
	ligolw_cut --delete-column segment:segment_def_cdb --delete-column segment:creator_db --delete-column segment_definer:insertion_time $@
	gstlal_segments_trim --trim $(SEGMENT_TRIM) --gps-start-time $(FSTART) --gps-end-time $(STOP) --min-length $(SEGMENT_MIN_LENGTH) --output $@ $@

frame.cache :
	if [[ $(IFO) == H1 ]] ; then \
		gw_data_find -o H -t H1_$(FRAME_TYPE) -l  -s $(FSTART) -e $(STOP) --url-type file -O $@ ; \
	elif [[ $(IFO) == L1 ]] ; then \
		gw_data_find -o L -t L1_$(FRAME_TYPE) -l  -s $(FSTART) -e $(STOP) --url-type file -O $@ ; \
	fi

# FIXME Add webpages once we have output
# Make webpage directory and copy files across
#$(WEBDIR) : $(MAKEFILE_LIST)
#	mkdir -p $(WEBDIR)/OPEN-BOX
#	cp $(MAKEFILE_LIST) $@

# Makes local plots directory
plots :
	mkdir plots

clean :
	-rm -rvf *.sub *.dag* *.cache *.sh logs *.sqlite plots *.html Images *.css *.js

