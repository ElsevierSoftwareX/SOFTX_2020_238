SHELL := /bin/bash # Use bash syntax

########################
#        Guide         #
########################

# Author: Patrick Godwin (patrick.godwin@ligo.org)
#
# This Makefile is designed to launch online feature extractor jobs as well
# as auxiliary jobs as needed (synchronizer/hdf5 file sinks).
#
# There are four separate modes that can be used to launch online jobs:
#
#   1. Auxiliary channel ingestion:
#
#     a. Reading from framexmit protocol (DATA_SOURCE=framexmit).
#        This mode is recommended when reading in live data from LHO/LLO.
#
#     b. Reading from shared memory (DATA_SOURCE=lvshm).
#        This mode is recommended for reading in data for O2 replay (e.g. UWM).
#
#  2. Data transfer of features:
#
#     a. Saving features directly to disk, e.g. no data transfer.
#        This will save features to disk directly from the feature extractor,
#        and saves features periodically via hdf5.
#
#     b. Transfer of features via Kafka topics.
#        This requires a Kafka/Zookeeper service to be running (can be existing LDG
#        or your own). Features get transferred via Kafka from the feature extractor,
#        parallel instances of the extractor get synchronized, and then sent downstream
#        where it can be read by other processes (e.g. iDQ). In addition, an streaming
#        hdf5 file sink is launched where it'll dump features periodically to disk.
#
# Configuration options:
#
#   General:
#     * TAG: sets the name used for logging purposes, Kafka topic naming, etc.
#
#   Data ingestion (using detchar channel list INI):
#     * IFO: select the IFO for auxiliary channels to be ingested (H1/L1).
#     * EPOCH: set epoch (O1/O2/etc).
#     * LEVEL: set types of channels to look over (standard/reduced).
#     * SECTION_INCLUDE: specify sections to include (no sections imply all sections).
#     * SAFETY_INCLUDE: specify safety types to include (default: safe).
#     * FIDELITY_EXCLUDE: specify fidelity types to exclude (default: none).
#     * UNSAFE_CHANNEL_INCLUDE: specify unsafe channels to include, ignoring safety information.
#     * MAX_STREAMS: Maximum # of streams that a single gstlal_feature_extractor process will
#         process. This is determined by sum_i(channel_i * # rates_i). Number of rates for a
#         given channels is determined by log2(max_rate/min_rate) + 1.
#
#   Waveform parameters:
#     * WAVEFORM: type of waveform used to perform matched filtering (sine_gaussian/half_sine_gaussian).
#     * MISMATCH: maximum mismatch between templates (corresponding to Omicron's mismatch definition).
#     * QHIGH: maximum value of Q
#
#   Data transfer/saving:
#     * OUTPATH: directory in which to save features.
#     * SAMPLE_RATE: rate at which to aggregate features for a given channel. Can be sampled at 1 Hz or higher (powers of 2).
#     * SAVE_FORMAT: determines whether to transfer features downstream or save directly (kafka/hdf5).
#     * SAVE_CADENCE: span of a typical dataset within an hdf5 file.
#     * PERSIST_CADENCE: span of a typical hdf5 file.
#
#   Kafka options:
#     * KAFKA_TOPIC: basename of topic for features generated from feature_extractor
#     * KAFKA_SERVER: Kafka server address where Kafka is hosted. If features are run in same location,
#         as in condor's local universe, setting localhost:port is fine. Otherwise you'll need to determine
#         the IP address where your Kafka server is running (using 'ip addr show' or equivalent).
#     * KAFKA_GROUP: group for which Kafka producers for feature_extractor jobs report to.
#
#   Synchronizer/File sink options:
#     * PROCESSING_CADENCE: cadence at which incoming features are processed, so as to limit polling
#         of topics repeatedly, etc. Default value of 0.1s is fine.
#     * REQUEST_TIMEOUT: timeout for waiting for a single poll from a Kafka consumer.
#     * LATENCY_TIMEOUT: timeout for the feature synchronizer before older features are dropped. This
#         is to prevent a single feature extractor job from holding up the online pipeline. This will
#         also depend on the latency induced by the feature extractor, especially when using templates
#         that have latencies associated with them such as Sine-Gaussians.
#
# In order to start up online runs, you'll need an installation of gstlal. An installation Makefile that
# includes Kafka dependencies are located at: gstlal/gstlal-burst/share/feature_extractor/Makefile.gstlal_idq_icc
#
# To run, making sure that the correct environment is sourced:
#
#   $ make -f Makefile.gstlal_feature_extractor_online
#

########################
# User/Accounting Tags #
########################

# Set the accounting tag from https://ldas-gridmon.ligo.caltech.edu/ldg_accounting/user
ACCOUNTING_TAG=ligo.prod.o3.detchar.onlinedq.idq
GROUP_USER=patrick.godwin

CONDOR_UNIVERSE=vanilla

#########################
# Online DAG Parameters #
#########################

IFO = H1
#IFO = L1

TAG = production_online

#DATA_SOURCE = framexmit
DATA_SOURCE = lvshm

MAX_STREAMS = 100
SAMPLE_RATE = 16

# Parameter space config of waveform
WAVEFORM = tapered_sine_gaussian
MISMATCH = 0.03
QHIGH = 40

# data transfer options
OUTPATH = $(PWD)
SAVE_FORMAT = kafka

# save options
SAVE_CADENCE = 20
PERSIST_CADENCE = 20

# aggregator settings
DATA_BACKEND = influx
INFLUX_HOSTNAME:=${INFLUXDB_HOSTNAME}
INFLUX_PORT = 8086
DATABASE_NAME = $(IFO)_gstlal_features

# kafka options
KAFKA_TOPIC = gstlal_features
KAFKA_GROUP = feature_production_online
KAFKA_PORT = 9182
ZOOKEEPER_PORT = 2271
ifeq ($(IFO),H1)
	KAFKA_SERVER:=10.21.6.226
	TARGET_MACHINE:=TARGET.Machine
	NODE:=node502.dcs.ligo-wa.caltech.edu
	SHM_PARTITION:=LHO_Data
else
	KAFKA_SERVER:=10.9.11.227
	TARGET_MACHINE:=Machine
	NODE:=node227.ldas.ligo-la.caltech.edu
	SHM_PARTITION:=LLO_Data
endif

# synchronizer/file sink options (kafka only)
PROCESSING_CADENCE = 0.001
REQUEST_TIMEOUT = 0.025
LATENCY_TIMEOUT = 10

# cluster where analysis is run
CLUSTER:=$(shell hostname -d)

##########################
# Auxiliary channel info #
##########################

EPOCH = O3
LEVEL = lldetchar

# if not using standard .ini file, comment and supply custom channel list instead
CHANNEL_LIST = $(IFO)-$(EPOCH)-$(LEVEL).ini
#CHANNEL_LIST = custom_channel_list.txt

# target channel
TARGET_CHANNEL = $(IFO):CAL-DELTAL_EXTERNAL_DQ

### used for channel list .ini file
# if not specified, use all sections (replace spaces with underscores'_')
SECTION_INCLUDE = 

# if not specified, use defaults
SAFETY_INCLUDE = safe unsafe unsafeabove2kHz unknown
FIDELITY_EXCLUDE =

# if specified, override safety checks for these channels
UNSAFE_CHANNEL_INCLUDE = $(TARGET_CHANNEL)

# parse include/excludes into command line options
SECTION_INCLUDE_COMMANDS := $(addprefix --section-include ,$(SECTION_INCLUDE))
SAFETY_INCLUDE_COMMANDS := $(addprefix --safety-include ,$(SAFETY_INCLUDE))
FIDELITY_EXCLUDE_COMMANDS := $(addprefix --fidelity-exclude ,$(FIDELITY_EXCLUDE))
UNSAFE_CHANNEL_INCLUDE_COMMANDS := $(addprefix --unsafe-channel-include ,$(UNSAFE_CHANNEL_INCLUDE))

#######################
# Other key variables #
#######################

GSTLALSHAREDIR=$(LAL_PATH)/../git/gstlal/gstlal-burst/share

############
# Workflow #
############

all : online-dag
	@echo "launch kafka dag first: condor_submit_dag kafka_broker_$(TAG).dag"
	@echo "then launch online jobs: condor_submit_dag $(IFO)_feature_extraction_pipe.dag"

online-dag : kafka_broker_$(TAG).dag $(CHANNEL_LIST) online-web-deploy
	gstlal_ll_feature_extractor_pipe \
		--data-source $(DATA_SOURCE) \
		--shared-memory-partition $(IFO)=$(SHM_PARTITION) \
		--shared-memory-assumed-duration 1 \
		--psd-fft-length 8 \
		--save-format $(SAVE_FORMAT) \
		--sample-rate $(SAMPLE_RATE) \
		--cadence $(SAVE_CADENCE) \
		--persist-cadence $(PERSIST_CADENCE) \
		--channel-list $(CHANNEL_LIST) \
		--out-path $(OUTPATH) \
		--max-streams $(MAX_STREAMS) \
		--waveform $(WAVEFORM) \
		--mismatch $(MISMATCH) \
		--qhigh $(QHIGH) \
		$(CONDOR_COMMANDS) \
		$(SECTION_INCLUDE_COMMANDS) \
		$(SAFETY_INCLUDE_COMMANDS) \
		$(FIDELITY_EXCLUDE_COMMANDS) \
		$(UNSAFE_CHANNEL_INCLUDE_COMMANDS) \
		--target-channel $(TARGET_CHANNEL) \
		--condor-universe $(CONDOR_UNIVERSE) \
		--condor-command=accounting_group=$(ACCOUNTING_TAG) \
		--condor-command=accounting_group_user=$(GROUP_USER) \
		--condor-command='Requirements=(TARGET.HasLowLatencyDetcharFrames =?= True) && (Machine != "node274.ldas.ligo-la.caltech.edu")' \
		--tag $(TAG) \
		--num-agg-jobs 15 \
		--processing-cadence $(PROCESSING_CADENCE) \
		--request-timeout $(REQUEST_TIMEOUT) \
		--latency-timeout $(LATENCY_TIMEOUT) \
		--kafka-topic $(KAFKA_TOPIC) \
		--kafka-server $(KAFKA_SERVER):$(KAFKA_PORT) \
		--kafka-partition $(KAFKA_GROUP) \
		--agg-data-backend $(DATA_BACKEND) \
		--influx-hostname $(INFLUX_HOSTNAME) \
		--influx-port $(INFLUX_PORT) \
		--influx-database-name $(DATABASE_NAME) \
		--request-cpu 2 \
		--request-memory 8GB \
		--auxiliary-request-memory 8GB \
		--verbose \
		--disable-web-service ; \

kafka_broker_$(TAG).dag : feature_extraction_monitor_$(IFO).yml
	gstlal_kafka_dag \
		--analysis-tag $(TAG) \
		--kafka-hostname $(KAFKA_SERVER) \
		--kafka-port $(KAFKA_PORT) \
		--zookeeper-port $(ZOOKEEPER_PORT) \
		--analysis-tag $(TAG) \
		--condor-universe $(CONDOR_UNIVERSE) \
		--condor-command=accounting_group=$(ACCOUNTING_TAG) \
		--condor-command=accounting_group_user=$(GROUP_USER) \
		--condor-command='Requirements=(TARGET.HasLowLatencyDetcharFrames =?= True) && ($(TARGET_MACHINE) == "$(NODE)")' \
		$(CONDOR_COMMANDS) \

online-web-deploy : feature_extraction_monitor_$(IFO).yml
	scald deploy -c feature_extraction_monitor_$(IFO).yml -o ~/public_html -e -n $(IFO)_feature_extraction_monitor

feature_extraction_monitor_$(IFO).yml :
	cp $(GSTLALSHAREDIR)/feature_extractor/monitoring/$@ .

# Pull latest channel list
$(CHANNEL_LIST) :
	if [ "$(LEVEL)" = "lldetchar" ]; then \
		cp $(GSTLALSHAREDIR)/feature_extractor/$(EPOCH)/channel_lists/$@ . ; \
	else \
		wget https://git.ligo.org/detchar/ligo-channel-lists/raw/master/$(EPOCH)/$@ ; \
	fi ;

clean-lite :
	-rm -rvf *.sub *.dag* *.cache *.sh logs *.txt

clean :
	-rm -rvf *.sub *.dag* *.cache *.sh logs *.ini *.txt

clean-all :
	-rm -rvf *.sub *.dag* *.cache *.sh logs *.ini *.txt monitor aggregator features snapshots synchronizer gstlal_feature_* kafka* zookeeper*
