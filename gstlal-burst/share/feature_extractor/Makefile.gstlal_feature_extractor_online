SHELL := /bin/bash # Use bash syntax

########################
#        Guide         #
########################

# Author: Patrick Godwin (patrick.godwin@ligo.org)
#
# This Makefile is designed to launch online feature extractor jobs as well
# as auxiliary jobs as needed (synchronizer/hdf5 file sinks).
#
# There are four separate modes that can be used to launch online jobs:
#
#   1. Auxiliary channel ingestion:
#
#     a. Reading from framexmit protocol (DATA_SOURCE=framexmit).
#        This mode is recommended when reading in live data from LHO/LLO.
#
#     b. Reading from shared memory (DATA_SOURCE=lvshm).
#        This mode is recommended for reading in data for O2 replay (e.g. UWM).
#
#  2. Data transfer of features:
#
#     a. Saving features directly to disk, e.g. no data transfer.
#        This will save features to disk directly from the feature extractor,
#        and saves features periodically via hdf5.
#
#     b. Transfer of features via Kafka topics.
#        This requires a Kafka/Zookeeper service to be running (can be existing LDG
#        or your own). Features get transferred via Kafka from the feature extractor,
#        parallel instances of the extractor get synchronized, and then sent downstream
#        where it can be read by other processes (e.g. iDQ). In addition, an streaming
#        hdf5 file sink is launched where it'll dump features periodically to disk.
#
# Configuration options:
#
#   General:
#     * TAG: sets the name used for logging purposes, Kafka topic naming, etc.
#
#   Data ingestion:
#     * IFO: select the IFO for auxiliary channels to be ingested.
#     * CHANNEL_LIST: a list of channels for the feature extractor to process. Provided
#         lists for O1/O2 and H1/L1 lists are in gstlal/gstlal-burst/share/feature_extractor.
#     * DATA_SOURCE: Protocol for reading in auxiliary channels (framexmit/lvshm).
#     * MAX_STREAMS: Maximum # of streams that a single gstlal_feature_extractor process will
#         process. This is determined by sum_i(channel_i * # rates_i). Number of rates for a
#         given channels is determined by log2(max_rate/min_rate) + 1.
#
#   Waveform parameters:
#     * WAVEFORM: type of waveform used to perform matched filtering (sine_gaussian/half_sine_gaussian).
#     * MISMATCH: maximum mismatch between templates (corresponding to Omicron's mismatch definition).
#     * QHIGH: maximum value of Q
#
#   Data transfer/saving:
#     * OUTPATH: directory in which to save features.
#     * SAVE_FORMAT: determines whether to transfer features downstream or save directly (kafka/hdf5).
#     * SAVE_CADENCE: span of a typical dataset within an hdf5 file.
#     * PERSIST_CADENCE: span of a typical hdf5 file.
#
#   Kafka options:
#     * KAFKA_TOPIC: basename of topic for features generated from feature_extractor
#     * KAFKA_SERVER: Kafka server address where Kafka is hosted. If features are run in same location,
#         as in condor's local universe, setting localhost:port is fine. Otherwise you'll need to determine
#         the IP address where your Kafka server is running (using 'ip addr show' or equivalent).
#     * KAFKA_GROUP: group for which Kafka producers for feature_extractor jobs report to.
#
#   Synchronizer/File sink options:
#     * PROCESSING_CADENCE: cadence at which incoming features are processed, so as to limit polling
#         of topics repeatedly, etc. Default value of 0.1s is fine.
#     * REQUEST_TIMEOUT: timeout for waiting for a single poll from a Kafka consumer.
#     * LATENCY_TIMEOUT: timeout for the feature synchronizer before older features are dropped. This
#         is to prevent a single feature extractor job from holding up the online pipeline. This will
#         also depend on the latency induced by the feature extractor, especially when using templates
#         that have latencies associated with them such as Sine-Gaussians.
#
# In order to start up online runs, you'll need an installation of gstlal. An installation Makefile that
# includes Kafka dependencies are located at: gstlal/gstlal-burst/share/feature_extractor/Makefile.gstlal_idq_icc
#
# To run, making sure that the correct environment is sourced:
#
#   $ make -f Makefile.gstlal_feature_extractor_online
#

########################
# User/Accounting Tags #
########################

# Set the accounting tag from https://ldas-gridmon.ligo.caltech.edu/ldg_accounting/user
ACCOUNTING_TAG=ligo.dev.o3.detchar.onlinedq.idq
GROUP_USER=albert.einstein
CONDOR_COMMANDS:=--condor-command=accounting_group=$(ACCOUNTING_TAG) --condor-command=accounting_group_user=$(GROUP_USER)

#########################
# Online DAG Parameters #
#########################

TAG = online_test

IFO = H1
#IFO = L1

# channel list for analysis
CHANNEL_LIST = channel_list.txt 

DATA_SOURCE = lvshm
MAX_STREAMS = 200

# Parameter space config of waveform
WAVEFORM = sine_gaussian
MISMATCH = 0.02
QHIGH = 40

# data transfer options
OUTPATH = $(PWD)
SAVE_FORMAT = kafka
#SAVE_FORMAT = hdf5

# save options
SAVE_CADENCE = 20
PERSIST_CADENCE = 200

# kafka options
KAFKA_TOPIC = gstlal_features
KAFKA_SERVER = localhost:9092
KAFKA_GROUP = group_1

# synchronizer/file sink options (kafka only)
PROCESSING_CADENCE = 0.1
REQUEST_TIMEOUT = 0.2
LATENCY_TIMEOUT = 12

# cluster where analysis is run
CLUSTER:=$(shell hostname -d)

#################
# Web directory #
#################

# A user tag for the run
#TAG = O2_C00
# Run number
#RUN = run_1
# A web directory for output (note difference between cit+uwm and Atlas)
# cit & uwm
#WEBDIR = ~/public_html/observing/$(TAG)/$(START)-$(STOP)-$(RUN)
# Atlas
#WEBDIR = ~/WWW/LSC/testing/$(TAG)/$(START)-$(STOP)-test_dag-$(RUN)

############
# Workflow #
############

all : dag
	@echo "Submit with: condor_submit_dag feature_extractor_pipe.dag"

# Run etg pipe to produce dag
dag : plots $(CHANNEL_LIST)
	if [[ $(DATA_SOURCE) == "lvshm" ]] && [[ $(SAVE_FORMAT) == "hdf5" ]] ; then \
		gstlal_ll_feature_extractor_pipe \
			--data-source $(DATA_SOURCE) \
			--shared-memory-partition H1=LHO_RedDtchr \
			--shared-memory-assumed-duration 1 \
			--save-format $(SAVE_FORMAT) \
			--cadence $(SAVE_CADENCE) \
			--persist-cadence $(PERSIST_CADENCE) \
			--channel-list $(CHANNEL_LIST) \
			--out-path $(OUTPATH) \
			--max-streams $(MAX_STREAMS) \
			--waveform $(WAVEFORM) \
			--mismatch $(MISMATCH) \
			--qhigh $(QHIGH) \
			$(CONDOR_COMMANDS) \
			--request-cpu 2 \
			--request-memory 15GB \
			--verbose \
			--disable-web-service ; \
	elif [[ $(DATA_SOURCE) == "framexmit" ]] && [[ $(SAVE_FORMAT) == "hdf5" ]] ; then \
		gstlal_ll_feature_extractor_pipe \
			--data-source $(DATA_SOURCE) \
			--save-format $(SAVE_FORMAT) \
			--cadence $(SAVE_CADENCE) \
			--persist-cadence $(PERSIST_CADENCE) \
			--channel-list $(CHANNEL_LIST) \
			--out-path $(OUTPATH) \
			--max-streams $(MAX_STREAMS) \
			--waveform $(WAVEFORM) \
			--mismatch $(MISMATCH) \
			--qhigh $(QHIGH) \
			$(CONDOR_COMMANDS) \
			--request-cpu 2 \
			--request-memory 15GB \
			--verbose \
			--disable-web-service ; \
	elif [[ $(DATA_SOURCE) == "lvshm" ]] && [[ $(SAVE_FORMAT) == "kafka" ]] ; then \
		gstlal_ll_feature_extractor_pipe \
			--data-source $(DATA_SOURCE) \
			--shared-memory-partition H1=LHO_RedDtchr \
			--shared-memory-assumed-duration 1 \
			--save-format $(SAVE_FORMAT) \
			--kafka-topic $(KAFKA_TOPIC) \
			--kafka-server $(KAFKA_SERVER) \
			--kafka-partition $(KAFKA_GROUP) \
			--channel-list $(CHANNEL_LIST) \
			--out-path $(OUTPATH) \
			--max-streams $(MAX_STREAMS) \
			--waveform $(WAVEFORM) \
			--mismatch $(MISMATCH) \
			--qhigh $(QHIGH) \
			--tag $(TAG) \
			--processing-cadence $(PROCESSING_CADENCE) \
			--request-timeout $(REQUEST_TIMEOUT) \
			--latency-timeout $(LATENCY_TIMEOUT) \
			$(CONDOR_COMMANDS) \
			--request-cpu 2 \
			--request-memory 15GB \
			--verbose \
			--disable-web-service ; \
	elif [[ $(DATA_SOURCE) == "framexmit" ]] && [[ $(SAVE_FORMAT) == "kafka" ]] ; then \
		gstlal_ll_feature_extractor_pipe \
			--data-source $(DATA_SOURCE) \
			--save-format $(SAVE_FORMAT) \
			--kafka-topic $(KAFKA_TOPIC) \
			--kafka-server $(KAFKA_SERVER) \
			--kafka-partition $(KAFKA_GROUP) \
			--channel-list $(CHANNEL_LIST) \
			--out-path $(OUTPATH) \
			--max-streams $(MAX_STREAMS) \
			--waveform $(WAVEFORM) \
			--mismatch $(MISMATCH) \
			--qhigh $(QHIGH) \
			--tag $(TAG) \
			--processing-cadence $(PROCESSING_CADENCE) \
			--request-timeout $(REQUEST_TIMEOUT) \
			--latency-timeout $(LATENCY_TIMEOUT) \
			$(CONDOR_COMMANDS) \
			--request-cpu 2 \
			--request-memory 15GB \
			--verbose \
			--disable-web-service ; \
	fi ;

# FIXME Determine channel list automatically.
#full_channel_list.txt : frame.cache
#	FrChannels $$(head -n 1 $^ | awk '{ print $$5}' | sed -e "s@file://localhost@@g") > $@

# FIXME Add webpages once we have output
# Make webpage directory and copy files across
#$(WEBDIR) : $(MAKEFILE_LIST)
#	mkdir -p $(WEBDIR)/OPEN-BOX
#	cp $(MAKEFILE_LIST) $@

# Makes local plots directory
plots :
	mkdir plots

clean :
	-rm -rvf *.sub *.dag* *.cache *.sh logs *.sqlite plots *.html Images *.css *.js
