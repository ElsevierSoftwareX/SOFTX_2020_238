SHELL := /bin/bash # Use bash syntax

#################################################################################
# GUIDE                                                                         #
#################################################################################

# Author: Patrick Godwin (patrick.godwin@ligo.org)
#
# This Makefile is designed to launch offline feature extractor jobs.
#
# For general use cases, the only configuration options that need to be changed are:
#
#  * User/Accounting tags: GROUP_USER, ACCOUNTING_TAG
#  * Analysis times: START, STOP
#  * Data ingestion: all options
#  * Waveform parameters: WAVEFORM, MISMATCH, QHIGH
#
# To get the full list of commands, run:
#
#   $ make help -f Makefile.gstlal_feature_extractor_offline
#
# To generate the DAG needed to start an analysis, run:
#
#   $ make dag -f Makefile.gstlal_feature_extractor_offline
#

#################################################################################
# CONFIGURATION                                                                 #
#################################################################################

#-------------------------------------
### User/Accounting Tags

ACCOUNTING_TAG=ligo.dev.o2.detchar.onlinedq.idq
GROUP_USER=albert.einstein
CONDOR_COMMANDS:=--condor-command=accounting_group=$(ACCOUNTING_TAG) --condor-command=accounting_group_user=$(GROUP_USER)

# Set accounting tag at:
#     https://ldas-gridmon.ligo.caltech.edu/ldg_accounting/user

#-------------------------------------
### Analysis configuration

#  General:
#    * TAG: sets the name used for logging purposes, Kafka topic naming, etc.
#    * SAMPLE_RATE: rate at which to aggregate features for a given channel.
#        Can be sampled at 1 Hz or higher (powers of 2).
#
#  Analysis times:
#    * START: set the analysis gps start time
#    * STOP: set the analysis gps stop time
#
#  Waveform parameters:
#    * WAVEFORM: type of waveform used to perform matched filtering.
#                options: sine_gaussian/half_sine_gaussian/tapered_sine_gaussian
#    * MISMATCH: maximum mismatch between templates (corresponding to Omicron's mismatch definition).
#    * QHIGH: maximum value of Q
#
#  Data transfer/saving:
#    * SAVE_CADENCE: span of a typical dataset within an hdf5 file.
#    * PERSIST_CADENCE: span of a typical hdf5 file.
#    * OUTPATH: directory in which to save features.

SAMPLE_RATE = 16
TAG = production_offline

# analysis times
START = 1187000000
STOP  = 1187100000

# data transfer/save options
SAVE_CADENCE = 20
PERSIST_CADENCE = 200
OUTPATH = $(PWD)

# parameter space for waveforms
WAVEFORM = tapered_sine_gaussian
MISMATCH = 0.03
QHIGH = 40

#-------------------------------------
### Channel list configuration

#  * IFO: select the IFO for auxiliary channels to be ingested (H1/L1).
#  * EPOCH: set epoch (O1/O2/etc).
#  * LEVEL: set types of channels to look over (standard/reduced).
#  * SECTION_INCLUDE: specify sections to include (no sections imply all sections).
#  * SAFETY_INCLUDE: specify safety types to include (default: safe).
#  * FIDELITY_EXCLUDE: specify fidelity types to exclude (default: none).
#  * UNSAFE_CHANNEL_INCLUDE: specify unsafe channels to include, ignoring safety information.

IFO = H1
#IFO = L1

EPOCH = O2
LEVEL = standard

CHANNEL_LIST = $(IFO)-$(EPOCH)-$(LEVEL).ini

# if not specified, use all sections (replace spaces with underscores'_')
SECTION_INCLUDE =

# if not specified, use defaults
SAFETY_INCLUDE = safe unsafe unsafeabove2kHz unknown
FIDELITY_EXCLUDE =

# if specified, override safety checks for these channels
UNSAFE_CHANNEL_INCLUDE = $(IFO):CAL-DELTAL_EXTERNAL_DQ

# parse include/excludes into command line options
SECTION_INCLUDE_COMMANDS := $(addprefix --section-include ,$(SECTION_INCLUDE))
SAFETY_INCLUDE_COMMANDS := $(addprefix --safety-include ,$(SAFETY_INCLUDE))
FIDELITY_EXCLUDE_COMMANDS := $(addprefix --fidelity-exclude ,$(FIDELITY_EXCLUDE))
UNSAFE_CHANNEL_INCLUDE_COMMANDS := $(addprefix --unsafe-channel-include ,$(UNSAFE_CHANNEL_INCLUDE))

#-------------------------------------
### Segment configuration

# Info from https://wiki.ligo.org/viewauth/LSC/JRPComm/ObsRun2

SEG_SERVER=https://segments.ligo.org
# C00
LIGO_SEGMENTS="$(IFO):DMT-ANALYSIS_READY:1"
# C01
#LIGO_SEGMENTS="$*:DCS-ANALYSIS_READY_C01:1"
# C02
#LIGO_SEGMENTS="$*:DCS-ANALYSIS_READY_C02:1"

#################################################################################
# DAG CONFIGURATION (OPTIONAL)                                                  #
#################################################################################

# length of time to process for a given job
SEGMENT_LENGTH = 4000

# don't generally have to mess with this, provides padding
# to account for PSD estimation
SEG_PAD = 1000
SEGMENT_TRIM = 0
SEGMENT_MIN_LENGTH = 512
FSTART=$(shell echo $$((${START}-${SEG_PAD})))

# Setting the number of streams (ADVANCED USAGE):
#
#     * MAX_SERIAL_STREAMS: Maximum # of streams that a single gstlal_feature_extractor job will
#         process at once. This is determined by sum_i(channel_i * # rates_i). Number of rates for a
#         given channels is determined by log2(max_rate/min_rate) + 1.
#     * MAX_PARALLEL_STREAMS: Maximum # of streams that a single job will run in the lifespan of a job.
#         This is distinct from serial streams since when a job is first launched, it will cache
#         auxiliary channel frames containing all channels that meet the criterion here, and then process
#         each channel subset sequentially determined by the serial streams. This is to save on input I/O.
#     * CONCURRENCY: determines the maximum # of concurrent reads from the same frame file. For most
#         purposes, it will be set to 1. Use this at your own risk.
#
#   NOTE: This won't have to be changed for almost all use cases, and the current configuration has been
#     optimized to aim for short run times.
#
#   Definition: Target number of streams (N_channels x N_rates_per_channel) that each cpu will process.
#
#     * if max_serial_streams > max_parallel_streams, all jobs will be parallelized by channel
#     * if max_parallel_streams > num_channels in channel list, all jobs will be processed serially,
#         with processing driven by max_serial_streams.
#     * any other combination will produce a mix of parallelization by channels and processing channels serially per job.
#
#   Playing around with combinations of MAX_SERIAL_STREAMS, MAX_PARALLEL_STREAMS, CONCURRENCY, will entirely
#   determine the structure of the offline DAG. Doing so will also change the memory usage for each job, and so you'll
#   need to tread lightly. Changing CONCURRENCY in particular may cause I/O locks due to jobs fighting to read from the same
#   frame file.

MAX_PARALLEL_STREAMS = 600
MAX_SERIAL_STREAMS = 210
CONCURRENCY = 1

#################################################################################
# WORKFLOW                                                                      #
#################################################################################
.PHONY: dag clean clean-all

## Generate offline analysis DAG
dag : frame.cache $(CHANNEL_LIST) segments.xml.gz
	gstlal_feature_extractor_pipe \
		--data-source frames \
		--gps-start-time $(START) \
		--gps-end-time $(STOP) \
		--frame-cache frame.cache \
		--frame-segments-file segments.xml.gz \
		--frame-segments-name datasegments \
		--psd-fft-length 8 \
		--local-frame-caching \
		--sample-rate $(SAMPLE_RATE) \
		--cadence $(SAVE_CADENCE) \
		--persist-cadence $(PERSIST_CADENCE) \
		--channel-list $(CHANNEL_LIST) \
		--out-path $(OUTPATH) \
		--waveform $(WAVEFORM) \
		--max-serial-streams $(MAX_SERIAL_STREAMS) \
		--max-parallel-streams $(MAX_PARALLEL_STREAMS) \
		--concurrency $(CONCURRENCY) \
		--segment-length $(SEGMENT_LENGTH) \
		--mismatch $(MISMATCH) \
		--qhigh $(QHIGH) \
		$(CONDOR_COMMANDS) \
		$(SECTION_INCLUDE_COMMANDS) \
		$(SAFETY_INCLUDE_COMMANDS) \
		$(FIDELITY_EXCLUDE_COMMANDS) \
		$(UNSAFE_CHANNEL_INCLUDE_COMMANDS) \
		--request-cpu 2 \
		--request-memory 15GB \
		--request-disk 12GB \
		--verbose \
		--disable-web-service
	@echo "Submit with: condor_submit_dag feature_extractor_pipe.dag"

# Pull latest channel list
$(CHANNEL_LIST) : frame.cache
	wget https://git.ligo.org/detchar/ligo-channel-lists/raw/master/$(EPOCH)/$(CHANNEL_LIST)

# Produce segments file
segments.xml.gz : frame.cache
	ligolw_segment_query_dqsegdb --segment-url=${SEG_SERVER} -q --gps-start-time ${FSTART} --gps-end-time ${STOP} --include-segments=$(LIGO_SEGMENTS) --result-name=datasegments > $@
	ligolw_no_ilwdchar $@
	ligolw_cut --delete-column segment:segment_def_cdb --delete-column segment:creator_db --delete-column segment_definer:insertion_time $@
	gstlal_segments_trim --trim $(SEGMENT_TRIM) --gps-start-time $(FSTART) --gps-end-time $(STOP) --min-length $(SEGMENT_MIN_LENGTH) --output $@ $@

frame.cache :
	if [[ $(IFO) == H1 ]] ; then \
		gw_data_find -o H -t H1_R -l  -s $(FSTART) -e $(STOP) --url-type file -O $@ ; \
	elif [[ $(IFO) == L1 ]] ; then \
		gw_data_find -o L -t L1_R -l  -s $(FSTART) -e $(STOP) --url-type file -O $@ ; \
	fi

## Clean directory of DAG-related files.
clean :
	-rm -rvf *.sub *.dag* *.cache *.sh logs *.sqlite *.html segments.xml.gz *.ini

## Clean directory of all files, including data products.
clean-all :
	-rm -rvf *.sub *.dag* *.cache *.sh logs *.sqlite *.html segments.xml.gz *.ini gstlal_feature_*

#################################################################################
# SELF DOCUMENTING COMMANDS                                                     #
#################################################################################

.DEFAULT_GOAL := help

# Inspired by <http://marmelab.com/blog/2016/02/29/auto-documented-makefile.html>
# sed script explained:
# /^##/:
# 	* save line in hold space
# 	* purge line
# 	* Loop:
# 		* append newline + line to hold space
# 		* go to next line
# 		* if line starts with doc comment, strip comment character off and loop
# 	* remove target prerequisites
# 	* append hold space (+ newline) to line
# 	* replace newline plus comments by `---`
# 	* print line
# Separate expressions are necessary because labels cannot be delimited by
# semicolon; see <http://stackoverflow.com/a/11799865/1968>
.PHONY: help
help:
	@echo "$$(tput bold)Usage:$$(tput sgr0)"
	@echo
	@echo "    Launch offline feature extraction jobs."
	@echo
	@echo "$$(tput bold)Available commands:$$(tput sgr0)"
	@echo
	@sed -n -e "/^## / { \
		h; \
		s/.*//; \
		:doc" \
		-e "H; \
		n; \
		s/^## //; \
		t doc" \
		-e "s/:.*//; \
		G; \
		s/\\n## /---/; \
		s/\\n/ /g; \
		p; \
	}" ${MAKEFILE_LIST} \
	| LC_ALL='C' sort --ignore-case \
	| awk -F '---' \
		-v ncol=$$(tput cols) \
		-v indent=19 \
		-v col_on="$$(tput setaf 6)" \
		-v col_off="$$(tput sgr0)" \
	'{ \
		printf "%s%*s%s ", col_on, -indent, $$1, col_off; \
		n = split($$2, words, " "); \
		line_length = ncol - indent; \
		for (i = 1; i <= n; i++) { \
			line_length -= length(words[i]) + 1; \
			if (line_length <= 0) { \
				line_length = ncol - indent - length(words[i]) - 1; \
				printf "\n%*s ", -indent, " "; \
			} \
			printf "%s ", words[i]; \
		} \
		printf "\n"; \
	}' \
	| more $(shell test $(shell uname) = Darwin && echo '--no-init --raw-control-chars')
