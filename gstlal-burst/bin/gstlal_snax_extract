#!/usr/bin/env python

# Copyright (C) 2017-2018  Sydney J. Chamberlin, Patrick Godwin, Chad Hanna, Duncan Meacher
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
A program to extract features from auxiliary channel data in real time or in offline mode
"""

# =============================
#
#           preamble
#
# =============================

import math
import optparse
import os
import resource
import socket
import sys
import tempfile

import h5py
import numpy

import gi
gi.require_version('Gst', '1.0')
from gi.repository import GObject, Gst
GObject.threads_init()
Gst.init(None)

from lal import LIGOTimeGPS

from ligo import segments
from ligo.segments import utils as segmentsUtils

from gstlal import aggregator
from gstlal import bottle
from gstlal import datasource
from gstlal import httpinterface
from gstlal import pipeparts
from gstlal import simplehandler

from gstlal.snax import auxcache
from gstlal.snax import feature_extractor
from gstlal.snax import multichannel_datasource
from gstlal.snax import pipeparts as snaxparts
from gstlal.snax import utils
from gstlal.snax import waveforms as fxwaveforms


#
# Make sure we have sufficient resources
# We allocate far more memory than we need, so this is okay
#

def setrlimit(res, lim):
	hard_lim = resource.getrlimit(res)[1]
	resource.setrlimit(res, (lim if lim is not None else hard_lim, hard_lim))


# set the number of processes and total set size up to hard limit and
# shrink the per-thread stack size (default is 10 MiB)
setrlimit(resource.RLIMIT_NPROC, None)
setrlimit(resource.RLIMIT_AS, None)
setrlimit(resource.RLIMIT_RSS, None)

# FIXME:  tests at CIT show that this next tweak has no effect.  it's
# possible that SL7 has lowered the default stack size from SL6 and we
# don't need to do this anymore.  remove?
setrlimit(resource.RLIMIT_STACK, 1024 * 1024)  # 1 MiB per thread


# =============================
#
#     command line parser
#
# =============================

def parse_command_line():

	parser = optparse.OptionParser(usage='%prog [options]', description=__doc__)

	# First append datasource and feature extraction common options
	multichannel_datasource.append_options(parser)
	feature_extractor.append_options(parser)

	# parse the arguments
	options, filenames = parser.parse_args()

	# Sanity check the options

	# set gps ranges for live and offline sources
	if options.data_source in ("framexmit", "lvshm", "white_live"):

		if options.data_source in ("framexmit", "lvshm"):
			options.gps_start_time = int(aggregator.now())
		else:
			# NOTE: set start time for 'fake' live sources to zero,
			#       since seeking doesn't work with 'is_live' option
			options.gps_start_time = 0

		# set the gps end time to be "infinite"
		options.gps_end_time = 2000000000

	if options.feature_start_time is None:
		options.feature_start_time = int(options.gps_start_time)
	if options.feature_end_time is None:
		options.feature_end_time = int(options.gps_end_time)

	# check if input sample rate is sensible
	assert options.sample_rate == 1 or options.sample_rate % 2 == 0
	assert options.min_downsample_rate % 2 == 0

	# check if persist and save cadence times are sensible
	assert options.persist_cadence >= options.cadence
	assert (options.persist_cadence % options.cadence) == 0

	# check if there are any segments to dump to disk
	if options.nxydump_segment:
		options.nxydump_segment, = segmentsUtils.from_range_strings(
			[options.nxydump_segment],
			boundtype=LIGOTimeGPS
		)

	return options, filenames


# =============================
#
#             main
#
# =============================

#
# parsing and setting up some core structures
#

options, filenames = parse_command_line()

data_source_info = multichannel_datasource.DataSourceInfo(options)
instrument = data_source_info.instrument
basename = '%s-%s' % (instrument[:1], options.description)
waveforms = {}

#
# set up logging
#

logger = utils.get_logger('snax_extract', verbose=options.verbose)

#
# set up local frame caching, if specified
#

if options.local_frame_caching:

	# get base temp directory
	if '_CONDOR_SCRATCH_DIR' in os.environ:
		tmp_dir = os.environ['_CONDOR_SCRATCH_DIR']
	else:
		tmp_dir = os.environ['TMPDIR']

	# create local frame directory
	local_path = os.path.join(tmp_dir, 'local_frames/')
	aggregator.makedir(local_path)

	# save local frame cache
	logger.info("caching frame data locally to %s" % local_path)
	f, fname = tempfile.mkstemp(".cache")
	f = open(fname, "w")

	data_source_info.local_cache_list = auxcache.cache_aux(
		data_source_info,
		logger,
		output_path=local_path,
		verbose=options.verbose
	)
	for cacheentry in data_source_info.local_cache_list:
		# guarantee a lal cache compliant file with
		# only integer starts and durations
		cacheentry.segment = segments.segment(
			int(cacheentry.segment[0]),
			int(math.ceil(cacheentry.segment[1]))
		)
		print >>f, str(cacheentry)
	f.close()

	data_source_info.frame_cache = fname

#
# process channel subsets in serial
#

for subset_id, channel_subset in enumerate(data_source_info.channel_subsets, 1):

	#
	# checkpointing for offline analysis for hdf5 output
	#

	if options.data_source not in data_source_info.live_sources and options.save_format == 'hdf5':
		try:
			# get path where triggers are located
			duration = options.feature_end_time - options.feature_start_time
			fname = utils.to_trigger_filename(basename, options.feature_start_time, duration, 'h5')
			fpath = utils.to_trigger_path(
				os.path.abspath(options.out_path),
				basename,
				options.feature_start_time,
				options.job_id,
				str(subset_id).zfill(4)
			)
			trg_file = os.path.join(fpath, fname)

			# visit groups within a given hdf5 file
			with h5py.File(trg_file, 'r') as f:
				f.visit(lambda item: f[item])
			# file is OK and there is no need to process it,
			# skip ahead in the loop
			continue

		except IOError:
			# file does not exist or is corrupted, need to reprocess
			logger.info(
				"checkpoint: {0} of {1} files completed "
				"and continuing with channel subset {2}".format(
					(subset_id - 1),
					len(data_source_info.channel_subsets),
					subset_id,
				),
			)

		logger.info("processing channel subset {:d} of {:d}".format(
			subset_id,
			len(data_source_info.channel_subsets))
		)

	#
	# if web services serving up bottle routes are enabled,
	# create a new, empty, Bottle application and make it the
	# current default, then start http server to serve it up
	#

	if not options.disable_web_service:
		bottle.default_app.push()
		# uncomment the next line to show tracebacks when something fails
		# in the web server
		import base64, uuid  # FIXME:  don't import when the uniquification scheme is fixed
		httpservers = httpinterface.HTTPServers(
			# FIXME:  either switch to using avahi's native name
			# uniquification system or adopt a naturally unique naming
			# scheme (e.g., include a search identifier and job
			# number).
			service_name="gstlal_idq (%s)" % base64.urlsafe_b64encode(uuid.uuid4().bytes),
			service_properties={},
			verbose=options.verbose
		)

		# Set up a registry of the resources that this job provides
		@bottle.route("/")
		@bottle.route("/index.html")
		def index(channel_list=channel_subset):
			# get the host and port to report in the links from the
			# request we've received so that the URLs contain the IP
			# address by which client has contacted us
			netloc = bottle.request.urlparts[1]
			server_address = "http://%s" % netloc
			yield "<html><body>\n<h3>%s %s</h3>\n<p>\n" % (netloc, " ".join(sorted(channel_list)))
			for route in sorted(bottle.default_app().routes, key=lambda route: route.rule):
				# don't create links back to this page
				if route.rule in ("/", "/index.html"):
					continue
				# only create links for GET methods
				if route.method != "GET":
					continue
				yield "<a href=\"%s%s\">%s</a><br>\n" % (server_address, route.rule, route.rule)
			yield "</p>\n</body></html>"
		# FIXME:  get service-discovery working, then don't do this
		with open("registry.txt", "w") as f:
			f.write("http://%s:%s/\n" % (socket.gethostname(), httpservers[0][0].port))

	#
	# building the event loop and pipeline
	#

	logger.info("assembling pipeline...")

	mainloop = GObject.MainLoop()
	pipeline = Gst.Pipeline(sys.argv[0])

	# generate multiple channel sources, and link up pipeline
	head = snaxparts.mkmultisrc(pipeline, data_source_info, channel_subset, verbose=options.verbose)
	src = {}

	for channel in channel_subset:
		# define sampling rates used
		samp_rate = int(data_source_info.channel_dict[channel]['fsamp'])
		max_rate = min(data_source_info.max_sample_rate, samp_rate)
		min_rate = min(data_source_info.min_sample_rate, max_rate)
		n_rates = int(numpy.log2(max_rate / min_rate) + 1)
		rates = [min_rate * 2**i for i in range(n_rates)]

		# choose range of basis parameters
		# NOTE: scale down frequency range by downsample_factor to deal with rolloff from downsampler
		downsample_factor = 0.8
		qlow = 3.3166
		if data_source_info.extension == 'ini':
			flow = max(data_source_info.channel_dict[channel]['flow'], min_rate / 4.)
			fhigh = min(data_source_info.channel_dict[channel]['fhigh'], max_rate / 2.)
			qhigh = min(data_source_info.channel_dict[channel]['qhigh'], options.qhigh)
		else:
			flow = min_rate / 4.
			fhigh = max_rate / 2.
			qhigh = options.qhigh

		# generate templates
		if 'sine_gaussian' in options.waveform:
			parameter_range = {'frequency': (flow, fhigh), 'q': (qlow, qhigh)}
			if options.waveform == 'half_sine_gaussian':
				waveforms[channel] = fxwaveforms.HalfSineGaussianGenerator(
					parameter_range,
					rates,
					mismatch=options.mismatch,
					downsample_factor=downsample_factor
				)
			elif options.waveform == 'sine_gaussian':
				waveforms[channel] = fxwaveforms.SineGaussianGenerator(
					parameter_range,
					rates,
					mismatch=options.mismatch,
					downsample_factor=downsample_factor
				)
			elif options.waveform == 'tapered_sine_gaussian':
				waveforms[channel] = fxwaveforms.TaperedSineGaussianGenerator(
					parameter_range,
					rates,
					mismatch=options.mismatch,
					downsample_factor=downsample_factor,
					max_latency=options.max_latency
				)
		else:
			raise NotImplementedError

		if options.latency_output:
			head[channel] = pipeparts.mklatency(
				pipeline,
				head[channel],
				name=utils.latency_name('beforewhitening', 2, channel)
			)

		# whiten auxiliary channel data
		head[channel] = snaxparts.mkcondition(
			pipeline,
			head[channel],
			rates,
			samp_rate,
			instrument,
			channel_name=channel,
			width=32,
			nxydump_segment=options.nxydump_segment,
			psd_fft_length=options.psd_fft_length,
		)

		# split whitened data into multiple frequency bands
		multiband = snaxparts.mkmultiband(
			pipeline,
			head[channel],
			rates,
			samp_rate,
			instrument,
			min_rate=options.min_downsample_rate
		)

		for rate, thishead in multiband.items():
			if options.latency_output:
				thishead = pipeparts.mklatency(
					pipeline,
					thishead,
					name=utils.latency_name('afterwhitening', 3, channel, rate)
				)

			# extract features
			thishead = snaxparts.mkextract(
				pipeline,
				thishead,
				instrument,
				channel,
				rate,
				waveforms,
				snr_threshold=options.snr_threshold,
				feature_sample_rate=options.sample_rate,
				min_downsample_rate=options.min_downsample_rate,
				nxydump_segment=options.nxydump_segment,
				feature_mode=options.feature_mode,
				latency_output=options.latency_output
			)

			if options.latency_output:
				thishead = pipeparts.mklatency(
					pipeline,
					thishead,
					name=utils.latency_name('aftertrigger', 5, channel, rate)
				)

			# link to src for processing by appsync
			src[(channel, rate)] = thishead

	# define structures to synchronize output streams and extract triggers from buffer
	logger.info("setting up pipeline handler...")
	handler = feature_extractor.MultiChannelHandler(
		mainloop,
		pipeline,
		logger,
		data_source_info,
		options,
		keys=src.keys(),
		waveforms=waveforms,
		basename=basename,
		subset_id=subset_id
	)

	logger.info("attaching appsinks to pipeline...")

	appsync = feature_extractor.LinkedAppSync(appsink_new_buffer=handler.bufhandler)
	for channel, rate in src.keys():
		appsync.add_sink(pipeline, src[(channel, rate)], name="sink_%s_%s" % (rate, channel))

	logger.info("attached %d appsinks to pipeline." % len(src.keys()))

	# Allow Ctrl+C or sig term to gracefully shut down the program for online
	# sources, otherwise it will just kill it
	if data_source_info.data_source in data_source_info.live_sources:  # what about nds online?
		simplehandler.OneTimeSignalHandler(pipeline)

	# Seek
	if pipeline.set_state(Gst.State.READY) == Gst.StateChangeReturn.FAILURE:
		raise RuntimeError("pipeline failed to enter READY state")

	if data_source_info.data_source not in data_source_info.live_sources:  # what about nds online?
		datasource.pipeline_seek_for_gps(pipeline, options.gps_start_time, options.gps_end_time)

	#
	# Run pipeline
	#

	if pipeline.set_state(Gst.State.PLAYING) == Gst.StateChangeReturn.FAILURE:
		raise RuntimeError("pipeline failed to enter PLAYING state")

	logger.info("running pipeline...")

	mainloop.run()

	# save remaining triggers
	logger.info("persisting features to disk...")
	handler.flush_and_save_features()

	#
	# Shut down pipeline
	#

	logger.info("shutting down pipeline...")

	#
	# Shutdown the web interface servers and garbage collect the Bottle
	# app.  This should release the references the Bottle app's routes
	# hold to the pipeline's data (like template banks and so on).
	#

	if not options.disable_web_service:
		del httpservers
		bottle.default_app.pop()

	#
	# Set pipeline state to NULL and garbage collect the handler
	#

	if pipeline.set_state(Gst.State.NULL) != Gst.StateChangeReturn.SUCCESS:
		raise RuntimeError("pipeline could not be set to NULL")

	del handler.pipeline
	del handler

#
# Cleanup local frame file cache and related frames
#

if options.local_frame_caching:
	logger.info("deleting temporary cache file and frames...")

	# remove frame cache
	os.remove(data_source_info.frame_cache)

	# remove local frames
	for cacheentry in data_source_info.local_cache_list:
		os.remove(cacheentry.path)

	del data_source_info.local_cache_list

#
# close program manually if data source is live
#

if options.data_source in data_source_info.live_sources:
	sys.exit(0)
