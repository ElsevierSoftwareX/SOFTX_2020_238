#!/usr/bin/env python

# Copyright (C) 2018  Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

__usage__ = "gstlal_snax_sink [--options]"
__description__ = "an executable to dump streaming data to disk via hdf5"
__author__ = "Patrick Godwin (patrick.godwin@ligo.org)"

#-------------------------------------------------
#                  Preamble
#-------------------------------------------------

from collections import deque
import itertools
import json
import logging
import optparse
import os
import shutil

import h5py
import numpy

from ligo.scald import events

from gstlal import aggregator

from gstlal.snax import multichannel_datasource
from gstlal.snax import utils

#-------------------------------------------------
#                  Functions
#-------------------------------------------------

def parse_command_line():

    parser = optparse.OptionParser(usage=__usage__, description=__description__)
    group = optparse.OptionGroup(parser, "File Sink Options", "General settings for configuring the file sink.")
    group.add_option("-v","--verbose", default=False, action="store_true", help = "Print to stdout in addition to writing to automatically generated log.")
    group.add_option("--log-level", type = "int", default = 10, help = "Sets the verbosity of logging. Default = 10.")
    group.add_option("--rootdir", metavar = "path", default = ".", help = "Sets the root directory where logs and metadata are stored.")
    group.add_option("--features-path", metavar = "path", default = ".", help = "Write features to this path. Default = .")
    group.add_option("--basename", metavar = "string", default = "SNAX_FEATURES", help = "Sets the basename for files written to disk. Default = SNAX_FEATURES")
    group.add_option("--instrument", metavar = "string", default = "H1", help = "Sets the instrument for files written to disk. Default = H1")
    group.add_option("--tag", metavar = "string", default = "test", help = "Sets the name of the tag used. Default = 'test'")
    group.add_option("--waveform", type="string", default = "sine_gaussian", help = "Set the waveform used for producing features. Default = sine_gaussian.")
    group.add_option("--sample-rate", type = "int", metavar = "Hz", default = 1, help = "Set the sample rate for feature timeseries output, must be a power of 2. Default = 1 Hz.")
    group.add_option("--write-cadence", type = "int", default = 100, help = "Rate at which the feature data is written to disk. Default = 100 seconds.")
    group.add_option("--persist-cadence", type = "int", default = 10000, help = "Rate at which new hdf5 files are written to disk. Default = 10000 seconds.")
    group.add_option("--processing-cadence", type = "float", default = 0.1, help = "Rate at which the synchronizer acquires and processes data. Default = 0.1 seconds.")
    group.add_option("--request-timeout", type = "float", default = 0.2, help = "Timeout for requesting messages from a topic. Default = 0.2 seconds.")
    group.add_option("--kafka-server", metavar = "string", help = "Sets the server url that the kafka topic is hosted on. Required.")
    group.add_option("--input-topic-basename", metavar = "string", help = "Sets the input kafka topic basename. Required.")
    parser.add_option_group(group)

    group = optparse.OptionGroup(parser, "Channel Options", "Settings used for deciding which auxiliary channels to process.")
    group.add_option("--channel-list", type="string", metavar = "name", help = "Set the list of the channels to process. Command given as --channel-list=location/to/file")
    group.add_option("--channel-name", metavar = "name", action = "append", help = "Set the name of the channels to process.  Can be given multiple times as --channel-name=IFO:AUX-CHANNEL-NAME:RATE")
    group.add_option("--section-include", default=[], type="string", action="append", help="Set the channel sections to be included from the INI file. Can be given multiple times. Pass in spaces as underscores instead. If not specified, assumed to include all sections")
    group.add_option("--safety-include", default=["safe"], type="string", action="append", help="Set the safety values for channels to be included from the INI file. Can be given multiple times. Default = 'safe'.")
    group.add_option("--fidelity-exclude", default=[], type="string", action="append", help="Set the fidelity values for channels to be excluded from the INI file. Can supply multiple values by repeating this argument. Each must be on of (add here)")
    group.add_option("--safe-channel-include", default=[], action="append", type="string", help="Include this channel when reading the INI file (requires exact match). Can be repeated. If not specified, assume to include all channels.")
    group.add_option("--unsafe-channel-include", default=[], action="append", type="string", help="Include this channel when reading the INI file, disregarding safety information (requires exact match). Can be repeated.")
    parser.add_option_group(group)

    options, args = parser.parse_args()

    return options, args

#-------------------------------------------------
#                   Classes
#-------------------------------------------------

class HDF5StreamSink(events.EventProcessor):
    """
    Handles the processing of incoming streaming features, saving datasets to disk in hdf5 format.
    """
    _name = 'hdf5_sink'

    def __init__(self, options):
        logging.info('setting up hdf5 stream sink...')

        events.EventProcessor.__init__(
            self,
            process_cadence=options.processing_cadence,
            request_timeout=options.request_timeout,
            kafka_server=options.kafka_server,
            input_topic=options.input_topic_basename,
            tag=options.tag
        )

        ### initialize queues
        self.feature_queue = deque(maxlen = 300)

        ### set up keys needed to do processing
        name, extension = options.channel_list.rsplit('.', 1)
        if extension == 'ini':
            self.keys = multichannel_datasource.channel_dict_from_channel_ini(options).keys()
        else:
            self.keys = multichannel_datasource.channel_dict_from_channel_file(options.channel_list).keys()

        ### iDQ saving properties
        self.timestamp = None
        self.last_save_time = None
        self.last_persist_time = None
        self.rootdir = options.rootdir
        self.base_features_path = options.features_path
        self.sample_rate = options.sample_rate
        self.write_cadence = options.write_cadence
        self.persist_cadence = options.persist_cadence
        self.waveform = options.waveform
        self.basename = '%s-%s' % (options.instrument[:1], options.basename)
        self.columns = ['time', 'frequency', 'q', 'snr', 'phase', 'duration']
        self.feature_data = utils.HDF5TimeseriesFeatureData(
			self.columns,
			keys = self.keys,
			cadence = self.write_cadence,
			sample_rate = self.sample_rate,
			waveform = self.waveform
		)

        ### get base temp directory
        if '_CONDOR_SCRATCH_DIR' in os.environ:
            self.tmp_dir = os.environ['_CONDOR_SCRATCH_DIR']
        else:
            self.tmp_dir = os.environ['TMPDIR']


    def set_hdf_file_properties(self, start_time, duration):
        """
        Returns the file name, as well as locations of temporary and permanent locations of
        directories where triggers will live, when given the current gps time and a gps duration.
        Also takes care of creating new directories as needed and removing any leftover temporary files.
        """
        # set/update file names and directories with new gps time and duration
        self.feature_name = os.path.splitext(utils.to_trigger_filename(self.basename, start_time, duration, 'h5'))[0]
        self.feature_path = utils.to_trigger_path(os.path.abspath(self.base_features_path), self.basename, start_time)
        self.tmp_path = utils.to_trigger_path(self.tmp_dir, self.basename, start_time)

        # create temp and output directories if they don't exist
        aggregator.makedir(self.feature_path)
        aggregator.makedir(self.tmp_path)

        # delete leftover temporary files
        tmp_file = os.path.join(self.tmp_path, self.feature_name)+'.h5.tmp'
        if os.path.isfile(tmp_file):
            os.remove(tmp_file)


    def ingest(self, message):
        """
        requests for a new message from an individual topic,
        and add to the feature queue
        """
        features = json.loads(message.value())
        self.feature_queue.appendleft((
            features['timestamp'],
            features['features']
        ))


    def handle(self):
        """
        takes data from the queue and adds to datasets, periodically persisting to disk
        """

        while self.feature_queue:
            ### remove data with oldest timestamp and process
            self.timestamp, features = self.feature_queue.pop()
            logging.info('processing features for timestamp %f' % self.timestamp)

            # set save times and initialize specific saving properties if not already set
            if self.last_save_time is None:
                self.last_save_time = self.timestamp
                self.last_persist_time = self.timestamp
                duration = utils.floor_div(self.timestamp + self.persist_cadence, self.persist_cadence) - self.timestamp + 1
                self.set_hdf_file_properties(self.timestamp, duration)

            # Save triggers once per cadence if saving to disk
            if self.timestamp and utils.in_new_epoch(self.timestamp, self.last_save_time, self.write_cadence):
                logging.info("saving features to disk at timestamp = %f" % self.timestamp)
                save_time = utils.floor_div(self.last_save_time, self.write_cadence)
                self.feature_data.dump(self.tmp_path, self.feature_name, save_time, tmp = True)
                self.last_save_time = self.timestamp

            # persist triggers once per persist cadence if using hdf5 format
            if self.timestamp and utils.in_new_epoch(self.timestamp, self.last_persist_time, self.persist_cadence):
                logging.info("persisting features to disk for gps range %f - %f" % (self.timestamp-self.persist_cadence, self.timestamp))
                self.persist_to_disk()
                self.last_persist_time = self.timestamp
                self.set_hdf_file_properties(self.timestamp, self.persist_cadence)

            ### add new feature vector to dataset
            self.feature_data.append(self.timestamp, features)


    def persist_to_disk(self):
        """
        moves a file from its temporary to final position
        """
        final_path = os.path.join(self.feature_path, self.feature_name)+".h5"
        tmp_path = os.path.join(self.tmp_path, self.feature_name)+".h5.tmp"
        shutil.move(tmp_path, final_path)


#-------------------------------------------------
#                    Main
#-------------------------------------------------

if __name__ == '__main__':
    options, args = parse_command_line()

    ### set up logging
    log_level = logging.DEBUG if options.verbose else logging.INFO
    logging.basicConfig(format='%(asctime)s | snax_sink : %(levelname)s : %(message)s')
    logging.getLogger().setLevel(log_level)

    # start up hdf5 sink
    sink = HDF5StreamSink(options)
    sink.start()
