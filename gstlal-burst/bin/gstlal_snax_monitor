#!/usr/bin/env python

# Copyright (C) 2018  Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

__usage__ = "gstlal_snax_monitor [--options]"
__description__ = "an executable to collect and monitor streaming features"
__author__ = "Patrick Godwin (patrick.godwin@ligo.org)"

#-------------------------------------------------
#                  Preamble
#-------------------------------------------------

from collections import defaultdict, deque
import json
import logging
import optparse

from ligo.scald import events
from ligo.scald import utils
from ligo.scald.io import hdf5, influx

from gstlal.snax import multichannel_datasource


#-------------------------------------------------
#                  Functions
#-------------------------------------------------

def parse_command_line():

    parser = optparse.OptionParser(usage=__usage__, description=__description__)
    group = optparse.OptionGroup(parser, "Monitor Options", "General settings for configuring the monitor.")
    group.add_option("-v","--verbose", default=False, action="store_true", help = "Print to stdout in addition to writing to automatically generated log.")
    group.add_option("--log-level", type = "int", default = 10, help = "Sets the verbosity of logging. Default = 10.")
    group.add_option("--instrument", metavar = "string", default = "H1", help = "Sets the instrument for files written to disk. Default = H1")
    group.add_option("--target-channel", metavar = "string", help = "Sets the target channel to view.")
    group.add_option("--rootdir", metavar = "path", default = ".", help = "Location where log messages and sqlite database lives")
    group.add_option("--tag", metavar = "string", default = "test", help = "Sets the name of the tag used. Default = 'test'")
    group.add_option("--sample-rate", type = "int", metavar = "Hz", default = 1, help = "Set the sample rate for feature timeseries output, must be a power of 2. Default = 1 Hz.")
    group.add_option("--num-channels", type = "int", help = "Set the full number of channels being processed upstream, used for monitoring purposes.")
    group.add_option("--channel-list", type="string", metavar = "name", help = "Set the list of the channels to process. Command given as --channel-list=location/to/file")
    group.add_option("--processing-cadence", type = "float", default = 0.1, help = "Rate at which the monitor acquires and processes data. Default = 0.1 seconds.")
    group.add_option("--request-timeout", type = "float", default = 0.2, help = "Timeout for requesting messages from a topic. Default = 0.2 seconds.")
    group.add_option("--kafka-server", metavar = "string", help = "Sets the server url that the kafka topic is hosted on. Required.")
    group.add_option("--input-topic-basename", metavar = "string", help = "Sets the input kafka topic basename. Required.")
    group.add_option("--data-backend", default="hdf5", help = "Choose the backend for data to be stored into, options: [hdf5|influx]. default = hdf5.")
    group.add_option("--influx-hostname", help = "Specify the hostname for the influxDB database. Required if --data-backend = influx.")
    group.add_option("--influx-port", help = "Specify the port for the influxDB database. Required if --data-backend = influx.")
    group.add_option("--influx-database-name", help = "Specify the database name for the influxDB database. Required if --data-backend = influx.")
    group.add_option("--enable-auth", default=False, action="store_true", help = "If set, enables authentication for the influx aggregator.")
    group.add_option("--enable-https", default=False, action="store_true", help = "If set, enables HTTPS connections for the influx aggregator.")
    group.add_option("--data-type", metavar="string", help="Specify datatypes to aggregate from 'min', 'max', 'median'. Default = max")
    group.add_option("--num-processes", type = "int", default = 2, help = "Number of processes to use concurrently, default 2.")
    parser.add_option_group(group)

    options, args = parser.parse_args()

    return options, args


#-------------------------------------------------
#                   Classes
#-------------------------------------------------

class StreamMonitor(events.EventProcessor):
    """
    Listens to incoming streaming features, collects metrics and pushes relevant metrics to a data store.
    """
    _name = 'feature_monitor'

    def __init__(self, options):
        logging.info('setting up feature monitor...')

        events.EventProcessor.__init__(
            self,
            process_cadence=options.processing_cadence,
            request_timeout=options.request_timeout,
            num_messages=options.sample_rate,
            kafka_server=options.kafka_server,
            input_topic=options.input_topic_basename,
            tag=options.tag
        )

        ### initialize queues
        self.sample_rate = options.sample_rate
        self.feature_queue = deque(maxlen = 60 * self.sample_rate)

        ### other settings
        if options.target_channel:
            self.target_channel = options.target_channel
        else:
            self.target_channel = '%s:CAL-DELTAL_EXTERNAL_DQ'%options.instrument
        self.num_channels = options.num_channels
        self.data_type = options.data_type

        ### keep track of last timestamp processed and saved
        self.last_save = None
        self.timestamp = None

        ### set up aggregator 
        logging.info("setting up monitor with backend: {}".format(options.data_backend))
        if options.data_backend == 'influx':
            self.agg_sink = influx.Aggregator(
                hostname=options.influx_hostname,
                port=options.influx_port,
                db=options.influx_database_name,
                auth=options.enable_auth,
                https=options.enable_https,
                reduce_across_tags=False,
            )
        else: ### hdf5 data backend
            self.agg_sink = hdf5.Aggregator(
                rootdir=options.rootdir,
                num_processes=options.num_processes,
                reduce_across_tags=False,
            )

        ### determine channels to be processed
        name, _ = options.channel_list.rsplit('.', 1)
        self.channels = set(multichannel_datasource.channel_dict_from_channel_file(options.channel_list).keys())

        ### define measurements to be stored
        for metric in ('target_snr', 'synchronizer_latency', 'percent_missed'):
            self.agg_sink.register_schema(metric, columns='data', column_key='data', tags='job', tag_key='job')


    def ingest(self, message):
        """
        parse a message containing feature data
        """
        features = json.loads(message.value())
        self.feature_queue.appendleft((
            features['timestamp'],
            features['features']
        ))
        self.timestamp = features['timestamp']


    def handle(self):
        """
        process features and generate metrics from synchronizer on a regular cadence
        """
        if self.timestamp:
            if not self.last_save or utils.in_new_epoch(self.timestamp, self.last_save, 1):

                ### check for missing channels
                missing_channels = set()

                metrics = defaultdict(list)
                while len(self.feature_queue) > 0:
                    ### remove data with oldest timestamp and process
                    timestamp, features = self.feature_queue.pop()
                    latency = utils.gps_to_latency(timestamp) 

                    ### check for missing channels
                    these_channels = set(features.keys())
                    missing_channels = self.channels - these_channels

                    ### generate metrics
                    metrics['time'].append(timestamp)
                    metrics['synchronizer_latency'].append(latency)
                    metrics['percent_missed'].append(100 * (float(self.num_channels - len(features.keys())) / self.num_channels))

                    if features.has_key(self.target_channel):
                        metrics['target_time'].append(timestamp)
                        metrics['target_snr'].append(features[self.target_channel][0]['snr'])

                ### store and aggregate features
                for metric in ('synchronizer_latency', 'percent_missed'):
                    data = {'time': metrics['time'], 'fields': {'data': metrics[metric]}}
                    self.agg_sink.store_columns(metric, {'synchronizer': data}, aggregate=self.data_type)
                if len(metrics['target_time']) > 0:
                    data = {'time': metrics['target_time'], 'fields': {'data': metrics['target_snr']}}
                    self.agg_sink.store_columns('target_snr', {'synchronizer': data}, aggregate=self.data_type)

                self.last_save = timestamp
                logging.info(
                    'processed features up to timestamp {:.3f}, '
                    'max latency = {:.3f} s, '
                    'percent missing channels = {:.3f}'.format(
                        timestamp,
                        max(metrics['synchronizer_latency']),
                        max(metrics['percent_missed'])
                    )
                )
                if missing_channels:
                    logging.info('channels missing @ timestamp={:.3f}: {}'.format(timestamp, repr(list(missing_channels))))


#-------------------------------------------------
#                    Main
#-------------------------------------------------

if __name__ == '__main__':
    options, args = parse_command_line()

    ### set up logging
    log_level = logging.DEBUG if options.verbose else logging.INFO
    logging.basicConfig(format='%(asctime)s | snax_monitor : %(levelname)s : %(message)s')
    logging.getLogger().setLevel(log_level)

    # start up monitor
    monitor = StreamMonitor(options=options)
    monitor.start()
