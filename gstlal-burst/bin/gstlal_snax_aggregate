#!/usr/bin/env python3

# Copyright (C) 2018  Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

__usage__ = "gstlal_snax_aggregate [--options]"
__description__ = "an executable to aggregate and generate job metrics for streaming features"
__author__ = "Patrick Godwin (patrick.godwin@ligo.org)"

#-------------------------------------------------
#                  Preamble
#-------------------------------------------------

from collections import defaultdict, deque
import json
import logging
import optparse

import numpy

from ligo.scald import utils
from ligo.scald.io import hdf5, influx

from gstlal import events


#-------------------------------------------------
#                  Functions
#-------------------------------------------------

def parse_command_line():

    parser = optparse.OptionParser(usage=__usage__, description=__description__)
    group = optparse.OptionGroup(parser, "Aggregator Options", "General settings for configuring the online aggregator.")
    group.add_option("-v","--verbose", default=False, action="store_true", help = "Print to stdout in addition to writing to automatically generated log.")
    group.add_option("--log-level", type = "int", default = 10, help = "Sets the verbosity of logging. Default = 10.")
    group.add_option("--rootdir", metavar = "path", default = ".", help = "Location where log messages and sqlite database lives")
    group.add_option("--tag", metavar = "string", default = "test", help = "Sets the name of the tag used. Default = 'test'")
    group.add_option("--sample-rate", type = "int", metavar = "Hz", default = 1, help = "Set the sample rate for feature timeseries output, must be a power of 2. Default = 1 Hz.")
    group.add_option("--processing-cadence", type = "float", default = 0.1, help = "Rate at which the aggregator acquires and processes data. Default = 0.1 seconds.")
    group.add_option("--request-timeout", type = "float", default = 0.2, help = "Timeout for requesting messages from a topic. Default = 0.2 seconds.")
    group.add_option("--kafka-server", metavar = "string", help = "Sets the server url that the kafka topic is hosted on. Required.")
    group.add_option("--input-topic-basename", metavar = "string", help = "Sets the input kafka topic basename. Required.")
    group.add_option("--jobs", action="append", help="Specify jobs to process. Can be given multiple times.")
    group.add_option("--data-backend", default="hdf5", help = "Choose the backend for data to be stored into, options: [hdf5|influx]. default = hdf5.")
    group.add_option("--influx-hostname", help = "Specify the hostname for the influxDB database. Required if --data-backend = influx.")
    group.add_option("--influx-port", help = "Specify the port for the influxDB database. Required if --data-backend = influx.")
    group.add_option("--influx-database-name", help = "Specify the database name for the influxDB database. Required if --data-backend = influx.")
    group.add_option("--enable-auth", default=False, action="store_true", help = "If set, enables authentication for the influx aggregator.")
    group.add_option("--enable-https", default=False, action="store_true", help = "If set, enables HTTPS connections for the influx aggregator.")
    group.add_option("--data-type", metavar = "string", help="Specify datatypes to aggregate from 'min', 'max', 'median'. Default: max")
    group.add_option("--num-processes", type = "int", default = 2, help = "Number of processes to use concurrently, default 2.")
    parser.add_option_group(group)

    options, args = parser.parse_args()

    return options, args


#-------------------------------------------------
#                   Classes
#-------------------------------------------------

class StreamAggregator(events.EventProcessor):
    """
    Ingests and aggregates incoming streaming features, collects job metrics.
    """
    _name = 'aggregator'

    def __init__(self, options):
        logging.info('setting up feature aggregator...')

        self.jobs = options.jobs
        input_topics = ['%s_%s'%(options.input_topic_basename, job) for job in self.jobs]
        events.EventProcessor.__init__(
            self,
            process_cadence=options.processing_cadence,
            request_timeout=options.request_timeout,
            num_messages=len(self.jobs),
            kafka_server=options.kafka_server,
            input_topic=input_topics,
            tag='aggregator_%s_%s'%(options.tag, self.jobs[0])
        )

        ### other aggregator options
        self.data_type = options.data_type
        self.last_save = utils.gps_now()
        self.sample_rate = options.sample_rate

        ### initialize 30 second queue for incoming buffers
        self.feature_queue = {job: deque(maxlen = 30 * self.sample_rate) for job in self.jobs}

        ### set up aggregator 
        logging.info("setting up aggregator with backend: %s"%options.data_backend)
        if options.data_backend == 'influx':
            self.agg_sink = influx.Aggregator(
                hostname=options.influx_hostname,
                port=options.influx_port,
                db=options.influx_database_name,
                auth=options.enable_auth,
                https=options.enable_https,
                reduce_across_tags=False,
            )
        else: ### hdf5 data backend
            self.agg_sink = hdf5.Aggregator(
                rootdir=options.rootdir,
                num_processes=options.num_processes,
                reduce_across_tags=False,
            )

        ### define measurements to be stored from aggregators
        self.agg_sink.register_schema(
            'latency',
             columns='data',
             column_key='data',
             tags='job',
             tag_key='job'
        )
        self.agg_sink.register_schema(
            'snr',
            columns='data',
            column_key='data',
            tags=('channel', 'subsystem'),
            tag_key='channel'
        )


    def ingest(self, message):
        """
        parse a message containing feature data
        """
        _, job = message.topic().rsplit('_', 1)
        feature_subset = json.loads(message.value())
        self.feature_queue[job].appendleft((
            feature_subset['timestamp'],
            feature_subset['features']
        ))


    def handle(self):
        """
        process and aggregate features from feature extraction jobs
        """
        if utils.in_new_epoch(utils.gps_now(), self.last_save, 1):
            self.last_save = utils.gps_now()

            ### format incoming packets into metrics and timeseries
            feature_packets = [(job, self.feature_queue[job].pop()) for job in self.jobs for i in range(len(self.feature_queue[job]))]
            all_timeseries, all_metrics = self.packets_to_timeseries(feature_packets)

            ### store and aggregate metrics
            metric_data = {job: {'time': metrics['time'], 'fields': {'data': metrics['latency']}} for job, metrics in all_metrics.items()}
            self.agg_sink.store_columns('latency', metric_data, aggregate=self.data_type)

            ### store and aggregate features
            timeseries_data = {(channel, self._channel_to_subsystem(channel)): {'time': timeseries['time'], 'fields': {'data': timeseries['snr']}} for channel, timeseries in all_timeseries.items()}
            self.agg_sink.store_columns('snr', timeseries_data, aggregate=self.data_type)

            try:
                max_latency = max(max(metrics['latency']) for metrics in all_metrics.values())
                logging.info('processed features at time %d, highest latency is %.3f' % (self.last_save, max_latency))
            except:
                logging.info('no features to process at time %d' % self.last_save)


    def packets_to_timeseries(self, packets):
        """
        splits up a series of packets into ordered timeseries, keyed by channel
        """
        metrics = defaultdict(lambda: {'time': [], 'latency': []})

        ### process each packet sequentially and split rows by channel
        channel_rows = defaultdict(list)
        for job, packet in packets:
            timestamp, features = packet
            metrics[job]['time'].append(timestamp)
            metrics[job]['latency'].append(utils.gps_to_latency(timestamp))
            for channel, row in features.items():
                channel_rows[channel].extend(row) 

        ### break up rows into timeseries
        timeseries = {}
        for channel, rows in channel_rows.items():
             timeseries[channel] = {column: [row[column] for row in rows] for column in rows[0].keys()}

        return timeseries, metrics


    @staticmethod
    def _channel_to_subsystem(channel):
        """
        given a channel, returns the subsystem the channel lives in
        """
        return channel.split(':')[1].split('-')[0]


#-------------------------------------------------
#                    Main
#-------------------------------------------------

if __name__ == '__main__':
    options, args = parse_command_line()

    ### set up logging
    log_level = logging.DEBUG if options.verbose else logging.INFO
    logging.basicConfig(format='%(asctime)s | snax_aggregate : %(levelname)s : %(message)s')
    logging.getLogger().setLevel(log_level)

    # start up aggregator
    aggregator = StreamAggregator(options)
    aggregator.start()
