#!/usr/bin/env python

# Copyright (C) 2018  Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

__usage__ = "gstlal_feature_hdf5_sink [--options]"
__description__ = "an executable to dump streaming data to disk via hdf5"
__author__ = "Patrick Godwin (patrick.godwin@ligo.org)"

#-------------------------------------------------
#                  Preamble
#-------------------------------------------------

import itertools
import json
import os
import signal
import sys
import time
import shutil
from collections import deque
from optparse import OptionParser

from confluent_kafka import Consumer, KafkaError
import h5py
import numpy

from gstlal import aggregator

from gstlal.fxtools import multichannel_datasource
from gstlal.fxtools import utils

#-------------------------------------------------
#                  Functions
#-------------------------------------------------

def parse_command_line():

    parser = OptionParser(usage=__usage__, description=__description__)
    parser.add_option("-v","--verbose", default=False, action="store_true", help = "Print to stdout in addition to writing to automatically generated log.")
    parser.add_option("--log-level", type = "int", default = 10, help = "Sets the verbosity of logging. Default = 10.")
    parser.add_option("--rootdir", metavar = "path", default = ".", help = "Sets the root directory where features, logs, and metadata are stored.")
    parser.add_option("--basename", metavar = "string", default = "GSTLAL_IDQ_FEATURES", help = "Sets the basename for files written to disk. Default = GSTLAL_IDQ_FEATURES")
    parser.add_option("--instrument", metavar = "string", default = "H1", help = "Sets the instrument for files written to disk. Default = H1")
    parser.add_option("--tag", metavar = "string", default = "test", help = "Sets the name of the tag used. Default = 'test'")
    parser.add_option("--waveform", type="string", default = "sine_gaussian", help = "Set the waveform used for producing features. Default = sine_gaussian.")
    parser.add_option("--channel-list", type="string", metavar = "name", help = "Set the list of the channels to process. Command given as --channel-list=location/to/file")
    parser.add_option("--sample-rate", type = "int", metavar = "Hz", default = 1, help = "Set the sample rate for feature timeseries output, must be a power of 2. Default = 1 Hz.")
    parser.add_option("--write-cadence", type = "int", default = 100, help = "Rate at which the feature data is written to disk. Default = 100 seconds.")
    parser.add_option("--persist-cadence", type = "int", default = 10000, help = "Rate at which new hdf5 files are written to disk. Default = 10000 seconds.")
    parser.add_option("--processing-cadence", type = "float", default = 0.1, help = "Rate at which the synchronizer acquires and processes data. Default = 0.1 seconds.")
    parser.add_option("--request-timeout", type = "float", default = 0.2, help = "Timeout for requesting messages from a topic. Default = 0.2 seconds.")
    parser.add_option("--kafka-server", metavar = "string", help = "Sets the server url that the kafka topic is hosted on. Required.")
    parser.add_option("--input-topic-basename", metavar = "string", help = "Sets the input kafka topic basename. Required.")

    options, args = parser.parse_args()

    return options, args

#-------------------------------------------------
#                   Classes
#-------------------------------------------------

class HDF5StreamSink(object):
    """
    Handles the processing of incoming streaming features, saving datasets to disk in hdf5 format.
    """
    def __init__(self, logger, options):
        logger.info('setting up hdf5 stream sink...')

        ### initialize timing options
        self.request_timeout = options.request_timeout
        self.processing_cadence = options.processing_cadence
        self.is_running = False

        ### kafka settings
        self.kafka_settings = {'bootstrap.servers': options.kafka_server,
                               'group.id': 'group_1'}

        ### initialize consumers
        self.consumer = Consumer(self.kafka_settings)
        self.consumer.subscribe([options.input_topic_basename])

        ### initialize queues
        self.feature_queue = deque(maxlen = 300)

        ### set up keys needed to do processing
        self.keys = multichannel_datasource.channel_dict_from_channel_file(options.channel_list).keys()

        ### iDQ saving properties
        self.timestamp = None
        self.last_save_time = None
        self.last_persist_time = None
        self.rootdir = options.rootdir
        self.sample_rate = options.sample_rate
        self.write_cadence = options.write_cadence
        self.persist_cadence = options.persist_cadence
        self.waveform = options.waveform
        self.basename = '%s-%s' % (options.instrument[:1], options.basename)
        self.columns = ['trigger_time', 'frequency', 'q', 'snr', 'phase']
        self.feature_data = utils.HDF5TimeseriesFeatureData(self.columns, keys = self.keys, cadence = self.write_cadence, sample_rate = self.sample_rate, waveform = self.waveform)

        ### get base temp directory
        if '_CONDOR_SCRATCH_DIR' in os.environ:
            self.tmp_dir = os.environ['_CONDOR_SCRATCH_DIR']
        else:
            self.tmp_dir = os.environ['TMPDIR']

    def set_hdf_file_properties(self, start_time, duration):
        """
        Returns the file name, as well as locations of temporary and permanent locations of
        directories where triggers will live, when given the current gps time and a gps duration.
        Also takes care of creating new directories as needed and removing any leftover temporary files.
        """
        # set/update file names and directories with new gps time and duration
        job_id = '0001'
        subset_id = '0001'
        self.feature_name = os.path.splitext(utils.to_trigger_filename(self.basename, start_time, duration, 'h5'))[0]
        self.feature_path = utils.to_trigger_path(os.path.abspath(self.rootdir), self.basename, start_time, job_id, subset_id)
        self.tmp_path = utils.to_trigger_path(self.tmp_dir, self.basename, start_time, job_id, subset_id)

        # create temp and output directories if they don't exist
        aggregator.makedir(self.feature_path)
        aggregator.makedir(self.tmp_path)

        # delete leftover temporary files
        tmp_file = os.path.join(self.tmp_path, self.feature_name)+'.h5.tmp'
        if os.path.isfile(tmp_file):
            os.remove(tmp_file)

    def fetch_data(self):
        """
        requests for a new message from an individual topic,
        and add to the feature queue
        """
        message = self.consumer.poll(timeout=self.request_timeout)

        ### only add to queue if no errors in receiving data
        if message and not message.error():

            ### parse and add to queue
            features = json.loads(message.value())
            self.add_to_queue(features['timestamp'], features['features'])

    def add_to_queue(self, timestamp, data):
        """
        add a set of features for a given timestamp to the feature queue
        """
        self.feature_queue.appendleft((timestamp, data))

    def process_queue(self):
        """
        takes data from the queue and adds to datasets, periodically persisting to disk
        """

        while self.feature_queue:
            ### remove data with oldest timestamp and process
            self.timestamp, features = self.feature_queue.pop()
            logger.info('processing features for timestamp %d' % self.timestamp)

            # set save times and initialize specific saving properties if not already set
            if self.last_save_time is None:
                self.last_save_time = self.timestamp
                self.last_persist_time = self.timestamp
                duration = utils.floor_div(self.timestamp + self.persist_cadence, self.persist_cadence) - self.timestamp
                self.set_hdf_file_properties(self.timestamp, duration)

            # Save triggers once per cadence if saving to disk
            if self.timestamp and utils.in_new_epoch(self.timestamp, self.last_save_time, self.write_cadence):
                logger.info("saving features to disk at timestamp = %d" % self.timestamp)
                self.feature_data.dump(self.tmp_path, self.feature_name, utils.floor_div(self.last_save_time, self.write_cadence), tmp = True)
                self.last_save_time = self.timestamp

            # persist triggers once per persist cadence if using hdf5 format
            if self.timestamp and utils.in_new_epoch(self.timestamp, self.last_persist_time, self.persist_cadence):
                logger.info("persisting features to disk for gps range %d - %d" % (self.timestamp-self.persist_cadence, self.timestamp))
                self.persist_to_disk()
                self.last_persist_time = self.timestamp
                self.set_hdf_file_properties(self.timestamp, self.persist_cadence)

            ### add new feature vector to dataset
            self.feature_data.append(self.timestamp, features)

    def persist_to_disk(self):
        """
        moves a file from its temporary to final position
        """
        final_path = os.path.join(self.feature_path, self.feature_name)+".h5"
        tmp_path = os.path.join(self.tmp_path, self.feature_name)+".h5.tmp"
        shutil.move(tmp_path, final_path)

    def start(self):
        """
        starts ingesting data and saving features to disk
        """
        logger.info('starting streaming hdf5 sink...')
        self.is_running = True
        while self.is_running:
            ### ingest and combine incoming feature subsets, dropping late data
            self.fetch_data()
            ### push combined features downstream
            while self.feature_queue:
                self.process_queue()
            ### repeat with processing cadence
            time.sleep(self.processing_cadence)

    def stop(self):
        """
        stops ingesting data and save rest of features to disk
        """
        logger.info('shutting down hdf5 sink...')
        self.persist_to_disk()
        ### FIXME: should also handle pushing rest of data in buffer
        self.is_running = False

class SignalHandler(object):
    """
    helper class to shut down the hdf5 sink gracefully before exiting
    """
    def __init__(self, sink, signals = [signal.SIGINT, signal.SIGTERM]):
        self.sink = sink
        for sig in signals:
            signal.signal(sig, self)

    def __call__(self, signum, frame):
        self.sink.stop()
        sys.exit(0)

#-------------------------------------------------
#                    Main
#-------------------------------------------------

if __name__ == '__main__':
    # parse arguments
    options, args = parse_command_line()

    ### set up logging
    logger = utils.get_logger(
        '-'.join([options.tag, 'hdf5_sink']),
        log_level=options.log_level,
        rootdir=options.rootdir,
        verbose=options.verbose
    )

    # create hdf5 sink instance
    sink = HDF5StreamSink(logger, options=options)

    # install signal handler
    SignalHandler(sink)

    # start up hdf5 sink
    sink.start()
