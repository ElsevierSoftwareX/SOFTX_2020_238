#!/usr/bin/env python

# Copyright (C) 2018  Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

__usage__ = "gstlal_feature_hdf5_sink [--options]"
__description__ = "an executable to dump streaming data to disk via hdf5"
__author__ = "Patrick Godwin (patrick.godwin@ligo.org)"

#-------------------------------------------------
#                  Preamble
#-------------------------------------------------

import itertools
import json
import optparse
import os
import signal
import sys
import time
import shutil
from collections import deque

from confluent_kafka import Consumer, KafkaError
import h5py
import numpy

from gstlal import aggregator

from gstlal.fxtools import multichannel_datasource
from gstlal.fxtools import utils

#-------------------------------------------------
#                  Functions
#-------------------------------------------------

def parse_command_line():

    parser = optparse.OptionParser(usage=__usage__, description=__description__)
    group = optparse.OptionGroup(parser, "File Sink Options", "General settings for configuring the file sink.")
    group.add_option("-v","--verbose", default=False, action="store_true", help = "Print to stdout in addition to writing to automatically generated log.")
    group.add_option("--log-level", type = "int", default = 10, help = "Sets the verbosity of logging. Default = 10.")
    group.add_option("--rootdir", metavar = "path", default = ".", help = "Sets the root directory where features, logs, and metadata are stored.")
    group.add_option("--basename", metavar = "string", default = "GSTLAL_IDQ_FEATURES", help = "Sets the basename for files written to disk. Default = GSTLAL_IDQ_FEATURES")
    group.add_option("--instrument", metavar = "string", default = "H1", help = "Sets the instrument for files written to disk. Default = H1")
    group.add_option("--tag", metavar = "string", default = "test", help = "Sets the name of the tag used. Default = 'test'")
    group.add_option("--waveform", type="string", default = "sine_gaussian", help = "Set the waveform used for producing features. Default = sine_gaussian.")
    group.add_option("--sample-rate", type = "int", metavar = "Hz", default = 1, help = "Set the sample rate for feature timeseries output, must be a power of 2. Default = 1 Hz.")
    group.add_option("--write-cadence", type = "int", default = 100, help = "Rate at which the feature data is written to disk. Default = 100 seconds.")
    group.add_option("--persist-cadence", type = "int", default = 10000, help = "Rate at which new hdf5 files are written to disk. Default = 10000 seconds.")
    group.add_option("--processing-cadence", type = "float", default = 0.1, help = "Rate at which the synchronizer acquires and processes data. Default = 0.1 seconds.")
    group.add_option("--request-timeout", type = "float", default = 0.2, help = "Timeout for requesting messages from a topic. Default = 0.2 seconds.")
    group.add_option("--kafka-server", metavar = "string", help = "Sets the server url that the kafka topic is hosted on. Required.")
    group.add_option("--input-topic-basename", metavar = "string", help = "Sets the input kafka topic basename. Required.")
    parser.add_option_group(group)

    group = optparse.OptionGroup(parser, "Channel Options", "Settings used for deciding which auxiliary channels to process.")
    group.add_option("--channel-list", type="string", metavar = "name", help = "Set the list of the channels to process. Command given as --channel-list=location/to/file")
    group.add_option("--channel-name", metavar = "name", action = "append", help = "Set the name of the channels to process.  Can be given multiple times as --channel-name=IFO:AUX-CHANNEL-NAME:RATE")
    group.add_option("--section-include", default=[], type="string", action="append", help="Set the channel sections to be included from the INI file. Can be given multiple times. Pass in spaces as underscores instead. If not specified, assumed to include all sections")
    group.add_option("--safety-include", default=["safe"], type="string", action="append", help="Set the safety values for channels to be included from the INI file. Can be given multiple times. Default = 'safe'.")
    group.add_option("--fidelity-exclude", default=[], type="string", action="append", help="Set the fidelity values for channels to be excluded from the INI file. Can supply multiple values by repeating this argument. Each must be on of (add here)")
    group.add_option("--safe-channel-include", default=[], action="append", type="string", help="Include this channel when reading the INI file (requires exact match). Can be repeated. If not specified, assume to include all channels.")
    group.add_option("--unsafe-channel-include", default=[], action="append", type="string", help="Include this channel when reading the INI file, disregarding safety information (requires exact match). Can be repeated.")
    parser.add_option_group(group)

    options, args = parser.parse_args()

    return options, args

#-------------------------------------------------
#                   Classes
#-------------------------------------------------

class HDF5StreamSink(object):
    """
    Handles the processing of incoming streaming features, saving datasets to disk in hdf5 format.
    """
    def __init__(self, logger, options):
        logger.info('setting up hdf5 stream sink...')
        self.tag = options.tag

        ### initialize timing options
        self.request_timeout = options.request_timeout
        self.processing_cadence = options.processing_cadence
        self.is_running = False

        ### kafka settings
        self.kafka_settings = {
            'bootstrap.servers': options.kafka_server,
            'group.id': 'hdf5_sink_{}'.format(self.tag)
        }

        ### initialize consumers
        self.consumer = Consumer(self.kafka_settings)
        self.consumer.subscribe([options.input_topic_basename])

        ### initialize queues
        self.feature_queue = deque(maxlen = 300)

        ### set up keys needed to do processing
        name, extension = options.channel_list.rsplit('.', 1)
        if extension == 'ini':
            self.keys = multichannel_datasource.channel_dict_from_channel_ini(options).keys()
        else:
            self.keys = multichannel_datasource.channel_dict_from_channel_file(options.channel_list).keys()

        ### iDQ saving properties
        self.timestamp = None
        self.last_save_time = None
        self.last_persist_time = None
        self.rootdir = options.rootdir
        self.sample_rate = options.sample_rate
        self.write_cadence = options.write_cadence
        self.persist_cadence = options.persist_cadence
        self.waveform = options.waveform
        self.basename = '%s-%s' % (options.instrument[:1], options.basename)
        self.columns = ['time', 'frequency', 'q', 'snr', 'phase', 'duration']
        self.feature_data = utils.HDF5TimeseriesFeatureData(
			self.columns,
			keys = self.keys,
			cadence = self.write_cadence,
			sample_rate = self.sample_rate,
			waveform = self.waveform
		)

        ### get base temp directory
        if '_CONDOR_SCRATCH_DIR' in os.environ:
            self.tmp_dir = os.environ['_CONDOR_SCRATCH_DIR']
        else:
            self.tmp_dir = os.environ['TMPDIR']

    def set_hdf_file_properties(self, start_time, duration):
        """
        Returns the file name, as well as locations of temporary and permanent locations of
        directories where triggers will live, when given the current gps time and a gps duration.
        Also takes care of creating new directories as needed and removing any leftover temporary files.
        """
        # set/update file names and directories with new gps time and duration
        self.feature_name = os.path.splitext(utils.to_trigger_filename(self.basename, start_time, duration, 'h5'))[0]
        self.feature_path = utils.to_trigger_path(os.path.abspath(self.rootdir), self.basename, start_time)
        self.tmp_path = utils.to_trigger_path(self.tmp_dir, self.basename, start_time)

        # create temp and output directories if they don't exist
        aggregator.makedir(self.feature_path)
        aggregator.makedir(self.tmp_path)

        # delete leftover temporary files
        tmp_file = os.path.join(self.tmp_path, self.feature_name)+'.h5.tmp'
        if os.path.isfile(tmp_file):
            os.remove(tmp_file)

    def fetch_data(self):
        """
        requests for a new message from an individual topic,
        and add to the feature queue
        """
        message = self.consumer.poll(timeout=self.request_timeout)

        ### only add to queue if no errors in receiving data
        if message and not message.error():

            ### parse and add to queue
            features = json.loads(message.value())
            self.add_to_queue(features['timestamp'], features['features'])

    def add_to_queue(self, timestamp, data):
        """
        add a set of features for a given timestamp to the feature queue
        """
        self.feature_queue.appendleft((timestamp, data))

    def process_queue(self):
        """
        takes data from the queue and adds to datasets, periodically persisting to disk
        """

        while self.feature_queue:
            ### remove data with oldest timestamp and process
            self.timestamp, features = self.feature_queue.pop()
            logger.info('processing features for timestamp %f' % self.timestamp)

            # set save times and initialize specific saving properties if not already set
            if self.last_save_time is None:
                self.last_save_time = self.timestamp
                self.last_persist_time = self.timestamp
                duration = utils.floor_div(self.timestamp + self.persist_cadence, self.persist_cadence) - self.timestamp + 1
                self.set_hdf_file_properties(self.timestamp, duration)

            # Save triggers once per cadence if saving to disk
            if self.timestamp and utils.in_new_epoch(self.timestamp, self.last_save_time, self.write_cadence):
                logger.info("saving features to disk at timestamp = %f" % self.timestamp)
                save_time = utils.floor_div(self.last_save_time, self.write_cadence)
                self.feature_data.dump(self.tmp_path, self.feature_name, save_time, tmp = True)
                self.last_save_time = self.timestamp

            # persist triggers once per persist cadence if using hdf5 format
            if self.timestamp and utils.in_new_epoch(self.timestamp, self.last_persist_time, self.persist_cadence):
                logger.info("persisting features to disk for gps range %f - %f" % (self.timestamp-self.persist_cadence, self.timestamp))
                self.persist_to_disk()
                self.last_persist_time = self.timestamp
                self.set_hdf_file_properties(self.timestamp, self.persist_cadence)

            ### add new feature vector to dataset
            self.feature_data.append(self.timestamp, features)

    def persist_to_disk(self):
        """
        moves a file from its temporary to final position
        """
        final_path = os.path.join(self.feature_path, self.feature_name)+".h5"
        tmp_path = os.path.join(self.tmp_path, self.feature_name)+".h5.tmp"
        shutil.move(tmp_path, final_path)

    def start(self):
        """
        starts ingesting data and saving features to disk
        """
        logger.info('starting streaming hdf5 sink...')
        self.is_running = True
        while self.is_running:
            ### ingest and combine incoming feature subsets, dropping late data
            self.fetch_data()
            ### push combined features downstream
            while self.feature_queue:
                self.process_queue()
            ### repeat with processing cadence
            time.sleep(self.processing_cadence)

    def stop(self):
        """
        stops ingesting data and save rest of features to disk
        """
        logger.info('shutting down hdf5 sink...')
        self.persist_to_disk()
        ### FIXME: should also handle pushing rest of data in buffer
        self.is_running = False

class SignalHandler(object):
    """
    helper class to shut down the hdf5 sink gracefully before exiting
    """
    def __init__(self, sink, signals = [signal.SIGINT, signal.SIGTERM]):
        self.sink = sink
        for sig in signals:
            signal.signal(sig, self)

    def __call__(self, signum, frame):
        self.sink.stop()
        sys.exit(0)

#-------------------------------------------------
#                    Main
#-------------------------------------------------

if __name__ == '__main__':
    # parse arguments
    options, args = parse_command_line()

    ### set up logging
    logger = utils.get_logger(
        '-'.join([options.tag, 'hdf5_sink']),
        log_level=options.log_level,
        rootdir=options.rootdir,
        verbose=options.verbose
    )

    # create hdf5 sink instance
    sink = HDF5StreamSink(logger, options=options)

    # install signal handler
    SignalHandler(sink)

    # start up hdf5 sink
    sink.start()
