#!/usr/bin/env python3
#
# Copyright (C) 2011-2018 Chad Hanna, Duncan Meacher, Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
This program makes a dag to run a series of gstlal_snax_extract jobs online
"""

__author__ = 'Duncan Meacher <duncan.meacher@ligo.org>, Patrick Godwin <patrick.godwin@ligo.org>'

# =============================
#
#           preamble
#
# =============================

import optparse
import os

from gstlal import aggregator
from gstlal import dagparts

from gstlal.snax import feature_extractor
from gstlal.snax import multichannel_datasource
from gstlal.snax import utils

# =============================
#
#          functions
#
# =============================

def generate_options(options):
	"""
	Generates a list of command line options to pass into DAG nodes.
	"""
	# data source options
	if options.data_source == 'lvshm':
		data_source_options = {
			"data-source": options.data_source,
			"shared-memory-partition": options.shared_memory_partition,
			"shared-memory-assumed-duration": options.shared_memory_assumed_duration
		}
	elif options.data_source == 'framexmit':
		data_source_options = {"data-source": options.data_source}

	# waveform options
	waveform_options = {
		"waveform": options.waveform,
		"mismatch": options.mismatch,
		"qhigh": options.qhigh
	}

	# data transfer options
	if options.save_format == 'kafka':
		save_options = {
			"save-format": options.save_format,
			"sample-rate": options.sample_rate,
			"kafka-partition": options.kafka_partition,
			"kafka-topic": options.kafka_topic,
			"kafka-server": options.kafka_server
		}
	elif options.save_format == 'hdf5':
		save_options = {
			"description": options.description,
			"save-format": options.save_format,
			"sample-rate": options.sample_rate,
			"cadence": options.cadence,
			"persist-cadence": options.persist_cadence
		}
	else:
		raise NotImplementedError, 'not an available option for online jobs at this time'

	# program behavior options
	program_options = {"psd-fft-length": options.psd_fft_length}
	if options.verbose:
		program_options.update({"verbose": options.verbose})

	# gobble options together
	out_options = {}
	out_options.update(data_source_options)
	out_options.update(waveform_options)
	out_options.update(save_options)
	out_options.update(program_options)

	return out_options

def feature_extractor_node_gen(feature_extractor_job, dag, parent_nodes, ifo, options, data_source_info):
	feature_extractor_nodes = {}
	channel_list = []

	# generate common command line options
	command_line_options = generate_options(options)

	# parallelize jobs by channel subsets
	for ii, channel_subset in enumerate(data_source_info.channel_subsets):

		if options.verbose:
			print("Creating node for channel subset %d"%ii)

		# creates a list of channel names with entries of the form --channel-name=IFO:CHANNEL_NAME:RATE
		channels = [''.join(["--channel-name=",':'.join([channel, str(int(data_source_info.channel_dict[channel]['fsamp']))])]) for channel in channel_subset]
		channels[0] = channels[0].split('=')[1] # this is done to peel off --channel-name option off first channel
		channel_list.extend([(channel, int(data_source_info.channel_dict[channel]['fsamp'])) for channel in channel_subset])

		# create specific options for each channel subset
		subset_options = {
			"max-streams": options.max_streams * 2, # FIXME: done to force all channels to be processed in parallel, but should be handled upstream more gracefully
			"job-id": str(ii + 1).zfill(4),
			"channel-name":' '.join(channels)
		}
		subset_options.update(command_line_options)

		feature_extractor_nodes[ii] = \
			dagparts.DAGNode(feature_extractor_job, dag, parent_nodes = parent_nodes,
				opts = subset_options,
				output_files = {"out-path": os.path.join(options.out_path, "gstlal_snax_extract")}
			)

	num_channels = len(channel_list)

	print("Writing channel list of all channels processed")
	listpath = os.path.join(options.out_path, "full_channel_list.txt")
	with open(listpath, 'w') as f:
		for channel, rate in channel_list:
			f.write('%s\t%d\n'%(channel, rate))

	return feature_extractor_nodes, num_channels


# =============================
#
#     command line parser
#
# =============================

def parse_command_line():
	parser = optparse.OptionParser(usage = '%prog [options]', description = __doc__)

	# generic data source and feature extraction options
	multichannel_datasource.append_options(parser)
	feature_extractor.append_options(parser)

	# Condor commands
	group = optparse.OptionGroup(parser, "Condor Options", "Adjust parameters used for HTCondor")
	parser.add_option("--condor-command", action = "append", default = [], metavar = "command=value", help = "set condor commands of the form command=value; can be given multiple times")
	parser.add_option("--condor-universe", default = "vanilla", metavar = "universe", help = "set the condor universe to run jobs in DAG, options are local/vanilla, default = vanilla")
	parser.add_option("--disable-agg-jobs", action = "store_true", help = "If set, do not launch aggregation jobs to process and aggregate incoming features.")
	parser.add_option("--request-cpu", default = "2", metavar = "integer", help = "set the requested node CPU count for feature extraction jobs, default = 2")
	parser.add_option("--request-memory", default = "8GB", metavar = "integer", help = "set the requested node memory for feature extraction jobs, default = 8GB")
	parser.add_option("--auxiliary-request-cpu", default = "2", metavar = "integer", help = "set the requested node CPU count for auxiliary processes, default = 2")
	parser.add_option("--auxiliary-request-memory", default = "2GB", metavar = "integer", help = "set the requested node memory for auxiliary processes, default = 2GB")
	parser.add_option_group(group)

	# Synchronizer/File Sink commands
	group = optparse.OptionGroup(parser, "Synchronizer/File Sink Options", "Adjust parameters used for synchronization and dumping of features to disk.")
	parser.add_option("--tag", metavar = "string", default = "test", help = "Sets the name of the tag used. Default = 'test'")
	parser.add_option("--no-drop", default=False, action="store_true", help = "If set, do not drop incoming features based on the latency timeout. Default = False.")
	parser.add_option("--features-path", metavar = "path", default = ".", help = "Write features to this path. Default = .")
	parser.add_option("--processing-cadence", type = "float", default = 0.1, help = "Rate at which the streaming jobs acquire and processes data. Default = 0.1 seconds.")
	parser.add_option("--request-timeout", type = "float", default = 0.2, help = "Timeout for requesting messages from a topic. Default = 0.2 seconds.")
	parser.add_option("--latency-timeout", type = "float", default = 5, help = "Maximum time before incoming data is dropped for a given timestamp. Default = 5 seconds.")
	parser.add_option_group(group)

	# Aggregation/Monitoring commands
	group = optparse.OptionGroup(parser, "Aggregator Options", "Adjust parameters used for aggregation and monitoring of features.")
	parser.add_option("--target-channel", metavar = "channel", help = "Target channel for monitoring.")
	parser.add_option("--num-agg-jobs", type = "int", default = 4, help = "Number of aggregator jobs to aggregate incoming features. Default = 4.")
	parser.add_option("--num-agg-processes-per-job", type = "int", default = 2, help = "Number of processes per aggregator job to aggregate incoming features. Used if --agg-data-backend = hdf5. Default = 2.")
	parser.add_option("--agg-data-backend", default="hdf5", help = "Choose the backend for data to be stored into, options: [hdf5|influx]. default = hdf5.")
	parser.add_option("--influx-hostname", help = "Specify the hostname for the influxDB database. Required if --agg-data-backend = influx.")
	parser.add_option("--influx-port", help = "Specify the port for the influxDB database. Required if --agg-data-backend = influx.")
	parser.add_option("--influx-database-name", help = "Specify the database name for the influxDB database. Required if --agg-data-backend = influx.")
	parser.add_option_group(group)

	options, filenames = parser.parse_args()

	return options, filenames

# =============================
#
#             main
#
# =============================

#
# parsing and setting up core structures
#

options, filenames = parse_command_line()

data_source_info = multichannel_datasource.DataSourceInfo(options)
ifo = data_source_info.instrument
channels = data_source_info.channel_dict.keys()

#
# create directories if needed
#

for dir_ in ('features', 'synchronizer', 'monitor', 'aggregator', 'logs'):
	aggregator.makedir(dir_)

#
# set up dag and job classes
#

dag = dagparts.DAG("%s_feature_extraction_pipe" % ifo)

# feature extractor job
if options.condor_universe == 'local':
	condor_options = {"want_graceful_removal":"True", "kill_sig":"15"}
else:
	condor_options = {"request_memory":options.request_memory, "request_cpus":options.request_cpu, "want_graceful_removal":"True", "kill_sig":"15"}
condor_commands = dagparts.condor_command_dict_from_opts(options.condor_command, condor_options)
feature_extractor_job = dagparts.DAGJob("gstlal_snax_extract", condor_commands = condor_commands, universe = options.condor_universe)
feature_extractor_nodes, num_channels = feature_extractor_node_gen(feature_extractor_job, dag, [], ifo, options, data_source_info)

# auxiliary jobs
if options.save_format == 'kafka':
	if options.condor_universe == 'local':
		auxiliary_condor_options = {"want_graceful_removal":"True", "kill_sig":"15"}
	else:
		auxiliary_condor_options = {"request_memory":options.auxiliary_request_memory, "request_cpus":options.auxiliary_request_cpu, "want_graceful_removal":"True", "kill_sig":"15"}
	auxiliary_condor_commands = dagparts.condor_command_dict_from_opts(options.condor_command, auxiliary_condor_options)
	synchronizer_job = dagparts.DAGJob("gstlal_snax_synchronize", condor_commands = auxiliary_condor_commands, universe = options.condor_universe)
	hdf5_sink_job = dagparts.DAGJob("gstlal_snax_sink", condor_commands = auxiliary_condor_commands, universe = options.condor_universe)
	monitor_job = dagparts.DAGJob("gstlal_snax_monitor", condor_commands = auxiliary_condor_commands, universe = options.condor_universe)

	# aggregator jobs
	if not options.disable_agg_jobs:
		aggregator_job = dagparts.DAGJob("gstlal_snax_aggregate", condor_commands = auxiliary_condor_commands, universe = options.condor_universe)

	#
	# set up options for auxiliary jobs
	#
	common_options = {
		"verbose": options.verbose,
		"tag": options.tag,
		"processing-cadence": options.processing_cadence,
		"request-timeout": options.request_timeout,
		"kafka-server": options.kafka_server
	}

	synchronizer_options = {
		"latency-timeout": options.latency_timeout,
		"sample-rate": options.sample_rate,
		"input-topic-basename": options.kafka_topic,
		"output-topic-basename": '_'.join(['synchronizer', options.tag])
	}
	if options.no_drop:
		synchronizer_options.update({"no-drop": options.no_drop})

	monitor_options = {
		"instrument": ifo,
		"target-channel": options.target_channel,
		"sample-rate": options.sample_rate,
		"input-topic-basename": '_'.join(['synchronizer', options.tag]),
		"num-channels": num_channels,
		"data-backend": options.agg_data_backend,
		"data-type": "max",
	}

	hdf5_sink_options = {
		"instrument": ifo,
		"channel-list": options.channel_list,
		"features-path": options.features_path,
		"basename": options.description,
		"waveform": options.waveform,
		"sample-rate": options.sample_rate,
		"write-cadence": options.cadence,
		"persist-cadence": options.persist_cadence,
		"input-topic-basename": '_'.join(['synchronizer', options.tag])
	}

	extra_hdf5_channel_options = {
		"section-include": options.section_include,
		"safety-include": list(options.safety_include),
		"fidelity-exclude": list(options.fidelity_exclude),
		"safe-channel-include": options.safe_channel_include,
		"unsafe-channel-include": options.unsafe_channel_include,
	}

	aggregator_options = {
		"sample-rate": options.sample_rate,
		"input-topic-basename": options.kafka_topic,
		"data-backend": options.agg_data_backend,
		"data-type": "max",
	}
	if options.agg_data_backend == 'influx':
		backend_options = {
			"influx-database-name": options.influx_database_name,
			"influx-hostname": options.influx_hostname,
			"influx-port": options.influx_port,
		}
	else:
		backend_options = {"num-processes": options.num_agg_processes_per_job}

	aggregator_options.update(backend_options)
	monitor_options.update(backend_options)
	### FIXME: hack to deal with condor DAG utilities not playing nice with empty settings
	for option_name, option in extra_hdf5_channel_options.items():
		if option:
			hdf5_sink_options[option_name] = option

	synchronizer_options.update(common_options)
	hdf5_sink_options.update(common_options)
	aggregator_options.update(common_options)
	monitor_options.update(common_options)
	monitor_options.update({"channel-list": os.path.join(options.out_path, "full_channel_list.txt")})


#
# set up jobs
#

if options.save_format == 'kafka':
	synchronizer_options.update({"num-topics": len(feature_extractor_nodes)})
	synchronizer_node = dagparts.DAGNode(synchronizer_job, dag, [], opts = synchronizer_options, output_files = {"rootdir": os.path.join(options.out_path, "synchronizer")})
	hdf5_sink_node = dagparts.DAGNode(hdf5_sink_job, dag, [], opts = hdf5_sink_options, output_files = {"rootdir": os.path.join(options.out_path, "features")})
	monitor_node = dagparts.DAGNode(monitor_job, dag, [], opts = monitor_options, output_files = {"rootdir": os.path.join(options.out_path, "monitor")})

	### aggregator jobs
	if not options.disable_agg_jobs:
		all_fx_jobs = [(str(ii).zfill(4), channel_subset) for ii, channel_subset in enumerate(data_source_info.channel_subsets)]
		for job_subset in dagparts.groups(all_fx_jobs, options.num_agg_jobs):
			jobs, channels = zip(*job_subset)
			job_channel_options = {"jobs": jobs}
			job_channel_options.update(aggregator_options)
			agg_node = dagparts.DAGNode(aggregator_job, dag, [], opts = job_channel_options, output_files = {"rootdir": os.path.join(options.out_path, "aggregator")})

#
# write out dag and sub files
#

dag.write_sub_files()
dag.write_dag()
dag.write_script()
