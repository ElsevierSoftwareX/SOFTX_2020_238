#!/usr/bin/env python
#
# Copyright (C) 2011-2018 Chad Hanna, Duncan Meacher, Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
This program makes a dag to run a series of gstlal_feature_extractor jobs online
"""

__author__ = 'Duncan Meacher <duncan.meacher@ligo.org>, Patrick Godwin <patrick.godwin@ligo.org>'

# =============================
#
#           preamble
#
# =============================

import optparse
import os

from gstlal import inspiral_pipe
from gstlal import dagparts as gstlaldagparts

from gstlal.fxtools import multichannel_datasource
from gstlal.fxtools import multirate_datasource
from gstlal.fxtools import utils

# =============================
#
#          functions
#
# =============================

def feature_extractor_node_gen(gstlalFeatureExtractorJob, dag, parent_nodes, ifo, options, data_source_info):
	feature_extractor_nodes = {}

	# parallelize jobs by channel subsets
	for ii, channel_subset in enumerate(data_source_info.channel_subsets):

		# creates a list of channel names with entries of the form --channel-name=IFO:CHANNEL_NAME:RATE
		channels = [''.join(["--channel-name=",':'.join([channel, str(int(data_source_info.channel_dict[channel]['fsamp']))])]) for channel in channel_subset]

		# FIXME: hacky way of getting options to get passed correctly for channels
		channels[0] = channels[0].split('=')[1]

		outpath = os.path.join(options.out_path, "gstlal_feature_extractor")

		feature_extractor_nodes[ii] = \
			inspiral_pipe.generic_node(gstlalFeatureExtractorJob, dag, parent_nodes = parent_nodes,
				opts = {"mismatch":options.mismatch,
					"shared-memory-partition": options.shared_memory_partition,
					"shared-memory-assumed-duration": options.shared_memory_assumed_duration,
					"data-source":"lvshm",
					"mismatch":options.mismatch,
					"waveform":options.waveform,
					"qhigh":options.qhigh,
					"max-streams":options.max_streams,
					"job-id":str(ii + 1).zfill(4),
					"cadence":options.cadence,
					"disable-web-service":options.disable_web_service,
					"save-format": options.save_format,
					"verbose":options.verbose,
					"channel-name":' '.join(channels)
				},
				output_files = {"out-path":outpath}
			)
		if options.verbose:
			print "Creating node for channel subset %d" % ii

	return feature_extractor_nodes

# =============================
#
#     command line parser
#
# =============================

def parse_command_line():
	parser = optparse.OptionParser(description = __doc__)

	# generic data source and feature extraction options
	multichannel_datasource.append_options(parser)
	feature_extractor.append_options(parser)

	# Condor commands
	group = optparse.OptionGroup(parser, "Condor Options", "Adjust parameters used for HTCondor")
	parser.add_option("--condor-command", action = "append", default = [], metavar = "command=value", help = "set condor commands of the form command=value; can be given multiple times")
	parser.add_option("--request-cpu", default = "2", metavar = "integer", help = "set the requested node CPU count, default = 2")
	parser.add_option("--request-memory", default = "8GB", metavar = "integer", help = "set the requested node memory, default = 8GB")
	parser.add_option_group(group)

	options, filenames = parser.parse_args()

	return options, filenames

# =============================
#
#             main
#
# =============================

#
# parsing and setting up core structures
#

options, filenames = parse_command_line()

data_source_info = multichannel_datasource.DataSourceInfo(options)
ifo = data_source_info.instrument
channels = data_source_info.channel_dict.keys()

#
# create directories if needed
#

listdir = os.path.join(options.out_path, "gstlal_feature_extractor/channel_lists")
aggregator.makedir(listdir)
aggregator.makedir("logs")

#
# set up dag and job classes
#

dag = inspiral_pipe.DAG("feature_extractor_pipe")

condor_options = {"request_memory":options.request_memory, "request_cpus":options.request_cpu, "want_graceful_removal":"True", "kill_sig":"15"}
condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, condor_options)
gstlalFeatureExtractorJob = inspiral_pipe.generic_job("gstlal_feature_extractor", condor_commands = condor_commands)

#
# set up feature extractor jobs
#

feature_extractor_nodes = feature_extractor_node_gen(gstlalFeatureExtractorJob, dag, [], ifo, options, data_source_info)

#
# write out dag and sub files
#

dag.write_sub_files()
dag.write_dag()
dag.write_script()
