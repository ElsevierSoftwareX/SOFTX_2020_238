#!/usr/bin/env python3

# Copyright (C) 2017-2018  Patrick Godwin
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

__usage__ = "gstlal_snax_synchronize [--options]"
__description__ = "an executable to synchronize incoming feature streams and send downstream"
__author__ = "Patrick Godwin (patrick.godwin@ligo.org)"

#-------------------------------------------------
#                  Preamble
#-------------------------------------------------

import heapq
import json
import logging

from collections import deque
from Queue import PriorityQueue
from optparse import OptionParser

from ligo.scald import utils

from gstlal import events


#-------------------------------------------------
#                  Functions
#-------------------------------------------------

def parse_command_line():

    parser = OptionParser(usage=__usage__, description=__description__)
    parser.add_option("-v","--verbose", default=False, action="store_true", help = "Print to stdout in addition to writing to automatically generated log.")
    parser.add_option("--log-level", type = "int", default = 10, help = "Sets the verbosity of logging. Default = 10.")
    parser.add_option("--rootdir", metavar = "path", default = ".", help = "Sets the root directory where logs and metadata are stored.")
    parser.add_option("--tag", metavar = "string", default = "test", help = "Sets the name of the tag used. Default = 'test'")
    parser.add_option("--processing-cadence", type = "float", default = 0.1, help = "Rate at which the synchronizer acquires and processes data. Default = 0.1 seconds.")
    parser.add_option("--request-timeout", type = "float", default = 0.2, help = "Timeout for requesting messages from a topic. Default = 0.2 seconds.")
    parser.add_option("--latency-timeout", type = "float", default = 5, help = "Maximum time before incoming data is dropped for a given timestamp. Default = 5 seconds.")
    parser.add_option("--sample-rate", type = "int", metavar = "Hz", default = 1, help = "Set the sample rate for feature timeseries output, must be a power of 2. Default = 1 Hz.")
    parser.add_option("--no-drop", default=False, action="store_true", help = "If set, do not drop incoming features based on the latency timeout. Default = False.")
    parser.add_option("--kafka-server", metavar = "string", help = "Sets the server url that the kafka topic is hosted on. Required.")
    parser.add_option("--input-topic-basename", metavar = "string", help = "Sets the input kafka topic basename, i.e. {basename}_%02d. Required.")
    parser.add_option("--output-topic-basename", metavar = "string", help = "Sets the output kafka topic name. Required.")
    parser.add_option("--num-topics", type = "int", help = "Sets the number of input kafka topics to read from. Required.")

    options, args = parser.parse_args()

    return options, args


#-------------------------------------------------
#                   Classes
#-------------------------------------------------

class StreamSynchronizer(events.EventProcessor):
    """
    Handles the synchronization of several incoming streams, populating data queues
    and pushing feature vectors to a queue for downstream processing.
    """
    _name = 'synchronizer'

    def __init__(self, options):
        logging.info('setting up stream synchronizer...')

        self.num_topics = options.num_topics
        self.topics = ['%s_%s' % (options.input_topic_basename, str(i).zfill(4)) for i in range(1, self.num_topics + 1)]
        events.EventProcessor.__init__(
            self,
            process_cadence=options.processing_cadence,
            request_timeout=options.request_timeout,
            num_messages=self.num_topics,
            kafka_server=options.kafka_server,
            input_topic=self.topics,
            tag=options.tag
        )

        ### synchronizer settings
        self.sample_rate = options.sample_rate
        self.latency_timeout = options.latency_timeout
        self.producer_name = options.output_topic_basename
        self.no_drop = options.no_drop

        ### initialize queues
        self.last_timestamp = 0
        # 30 second queue for incoming buffers
        self.feature_queue = PriorityQueue(maxsize = 30 * self.sample_rate * self.num_topics)
        # 5 minute queue for outgoing buffers
        self.feature_buffer = deque(maxlen = 300)


    def ingest(self, message):
        """
        parse a new message from a feature extractor,
        and add to the feature queue
        """
        ### decode json and parse data
        feature_subset = json.loads(message.value())

        ### add to queue if timestamp is within timeout
        if self.no_drop or (feature_subset['timestamp'] >= self.max_timeout()):
            self.feature_queue.put((
                feature_subset['timestamp'],
                feature_subset['features']
            ))


    def handle(self):
        """
        combines subsets from the feature queue at a given timestamp,
        and send the resulting data downstream
        """
        ### clear out queue of any stale data
        while not self.feature_queue.empty() and self.last_timestamp >= self.feature_queue.queue[0][0]:
            self.feature_queue.get()

        ### inspect timestamps in front of queue
        num_elems = min(self.num_topics, self.feature_queue.qsize())
        timestamps = [block[0] for block in heapq.nsmallest(num_elems, self.feature_queue.queue)]

        ### check if either all timestamps are identical, or if the timestamps
        ### are old enough to process regardless. if so, process elements from queue
        if timestamps:
            if timestamps[0] <= self.max_timeout() or (len(set(timestamps)) == 1 and num_elems == self.num_topics):

                ### find number of elements to remove from queue
                if timestamps[0] <= self.max_timeout():
                    num_subsets = len([timestamp for timestamp in timestamps if timestamp == timestamps[0]])
                else:
                    num_subsets = num_elems

                ### remove data with oldest timestamp and process
                subsets = [self.feature_queue.get() for i in range(num_subsets)]
                logging.info(
                    'combining {:d} / {:d} feature subsets '
                    'for timestamp {:f}'.format(len(subsets), self.num_topics, timestamps[0])
                )
                features = self.combine_subsets(subsets)
                self.feature_buffer.appendleft((timestamps[0], features))
                self.last_timestamp = timestamps[0]

        ### push combined features downstream
        while self.feature_buffer:
            self.push_features()


    def combine_subsets(self, subsets):
        """
        combine subsets of features from multiple streams in a sensible way
        """
        datum = [subset[1] for subset in subsets]
        return {ch: rows for channel_subsets in datum for ch, rows in channel_subsets.items()}


    def push_features(self):
        """
        pushes any features that have been combined downstream in an outgoing topic
        """
        # push full feature vector to producer if buffer isn't empty
        if self.feature_buffer:
            timestamp, features = self.feature_buffer.pop()
            logging.info(
                'pushing features with timestamp {:f} downstream, '
                'latency is {:.3f}'.format(timestamp, utils.gps_to_latency(timestamp))
            )
            feature_packet = {'timestamp': timestamp, 'features': features}
            self.producer.produce(
                timestamp=timestamp,
                topic=self.producer_name,
                value=json.dumps(feature_packet)
            )
            self.producer.poll(0)


    def max_timeout(self):
        """
        calculates the oldest timestamp allowed for incoming data
        """
        return utils.gps_now() - self.latency_timeout


#-------------------------------------------------
#                    Main
#-------------------------------------------------

if __name__ == '__main__':
    options, args = parse_command_line()

    ### set up logging
    log_level = logging.DEBUG if options.verbose else logging.INFO
    logging.basicConfig(format='%(asctime)s | snax_synchronize : %(levelname)s : %(message)s')
    logging.getLogger().setLevel(log_level)

    # start up synchronizer
    synchronizer = StreamSynchronizer(options=options)
    synchronizer.start()
