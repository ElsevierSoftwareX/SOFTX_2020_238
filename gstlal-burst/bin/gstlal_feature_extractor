#!/usr/bin/env python

# Copyright (C) 2017-2018  Sydney J. Chamberlin, Patrick Godwin, Chad Hanna, Duncan Meacher
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.



# =============================
# 
#           preamble
#
# =============================

### A program to extract features from auxiliary channel data in real time or in offline mode
###
### .. graphviz::
###
###    digraph llpipe {
###    	labeljust = "r";
###    	label="gstlal_feature_extractor"
###    	rankdir=LR;
###    	graph [fontname="Roman", fontsize=24];
###    	edge [ fontname="Roman", fontsize=10 ];
###    	node [fontname="Roman", shape=box, fontsize=11];
###
###
###    	subgraph clusterNodeN {
###
###    		style=rounded;
###    		label="gstreamer pipeline";
###    		labeljust = "r";
###    		fontsize = 14;
###
###    		H1L1src [label="H1(L1) data source:\n mkbasicmultisrc()", color=red4];
###
###    		Aux1 [label="Auxiliary channel 1", color=red4];
###    		Aux2 [label="Auxiliary channel 2", color=green4];
###    		AuxN [label="Auxiliary channel N", color=magenta4];
###
###    		Multirate1 [label="Auxiliary channel 1 whitening and downsampling:\nmkwhitened_multirate_src()", color=red4];
###    		Multirate2 [label="Auxiliary channel 2 whitening and downsampling:\nmkwhitened_multirate_src()", color=green4];
###    		MultirateN [label="Auxiliary channel N whitening and downsampling:\nmkwhitened_multirate_src()", color=magenta4];
###
###    		FilterBankAux1Rate1 [label="Auxiliary Channel 1:\nGlitch Filter Bank", color=red4];
###    		FilterBankAux1Rate2 [label="Auxiliary Channel 1:\nGlitch Filter Bank", color=red4];
###    		FilterBankAux1RateN [label="Auxiliary Channel 1:\nGlitch Filter Bank", color=red4];
###    		FilterBankAux2Rate1 [label="Auxiliary Channel 2:\nGlitch Filter Bank", color=green4];
###    		FilterBankAux2Rate2 [label="Auxiliary Channel 2:\nGlitch Filter Bank", color=green4];
###    		FilterBankAux2RateN [label="Auxiliary Channel 2:\nGlitch Filter Bank", color=green4];
###    		FilterBankAuxNRate1 [label="Auxiliary Channel N:\nGlitch Filter Bank", color=magenta4];
###    		FilterBankAuxNRate2 [label="Auxiliary Channel N:\nGlitch Filter Bank", color=magenta4];
###    		FilterBankAuxNRateN [label="Auxiliary Channel N:\nGlitch Filter Bank", color=magenta4];
###
###    		TriggerAux1Rate1 [label="Auxiliary Channel 1:\nTrigger Max (1 sec)", color=red4];
###    		TriggerAux1Rate2 [label="Auxiliary Channel 1:\nTrigger Max (1 sec)", color=red4];
###    		TriggerAux1RateN [label="Auxiliary Channel 1:\nTrigger Max (1 sec)", color=red4];
###    		TriggerAux2Rate1 [label="Auxiliary Channel 2:\nTrigger Max (1 sec)", color=green4];
###    		TriggerAux2Rate2 [label="Auxiliary Channel 2:\nTrigger Max (1 sec)", color=green4];
###    		TriggerAux2RateN [label="Auxiliary Channel 2:\nTrigger Max (1 sec)", color=green4];
###    		TriggerAuxNRate1 [label="Auxiliary Channel N:\nTrigger Max (1 sec)", color=magenta4];
###    		TriggerAuxNRate2 [label="Auxiliary Channel N:\nTrigger Max (1 sec)", color=magenta4];
###    		TriggerAuxNRateN [label="Auxiliary Channel N:\nTrigger Max (1 sec)", color=magenta4];
###
###    		H1L1src -> Aux1;
###    		H1L1src -> Aux2;
###    		H1L1src -> AuxN;
###
###    		Aux1 -> Multirate1;
###    		Aux2 -> Multirate2;
###    		AuxN -> MultirateN;
###
###    		Multirate1 -> FilterBankAux1Rate1 [label="Aux 1 4096Hz"];
###    		Multirate2 -> FilterBankAux2Rate1 [label="Aux 2 4096Hz"];
###    		MultirateN -> FilterBankAuxNRate1 [label="Aux N 4096Hz"];
###    		Multirate1 -> FilterBankAux1Rate2 [label="Aux 1 2048Hz"];
###    		Multirate2 -> FilterBankAux2Rate2 [label="Aux 2 2048Hz"];
###    		MultirateN -> FilterBankAuxNRate2 [label="Aux N 2048Hz"];
###    		Multirate1 -> FilterBankAux1RateN [label="Aux 1 Nth-pow-of-2 Hz"];
###    		Multirate2 -> FilterBankAux2RateN [label="Aux 2 Nth-pow-of-2 Hz"];
###    		MultirateN -> FilterBankAuxNRateN [label="Aux N Nth-pow-of-2 Hz"];
###
###    		FilterBankAux1Rate1 -> TriggerAux1Rate1;
###    		FilterBankAux1Rate2 -> TriggerAux1Rate2;
###    		FilterBankAux1RateN -> TriggerAux1RateN;
###    		FilterBankAux2Rate1 -> TriggerAux2Rate1;
###    		FilterBankAux2Rate2 -> TriggerAux2Rate2;
###    		FilterBankAux2RateN -> TriggerAux2RateN;
###    		FilterBankAuxNRate1 -> TriggerAuxNRate1;
###    		FilterBankAuxNRate2 -> TriggerAuxNRate2;
###    		FilterBankAuxNRateN -> TriggerAuxNRateN;
###    	}
###
###
###    	Synchronize [label="Synchronize buffers by timestamp"];
###    	Extract [label="Extract features from buffer"];
###    	Save [label="Save triggers to disk"];
###    	Kafka [label="Push features to queue"];
###
###    	TriggerAux1Rate1 -> Synchronize;
###    	TriggerAux1Rate2 -> Synchronize;
###    	TriggerAux1RateN -> Synchronize;
###    	TriggerAux2Rate1 -> Synchronize;
###    	TriggerAux2Rate2 -> Synchronize;
###    	TriggerAux2RateN -> Synchronize;
###    	TriggerAuxNRate1 -> Synchronize;
###    	TriggerAuxNRate2 -> Synchronize;
###    	TriggerAuxNRateN -> Synchronize;
###
###    	Synchronize -> Extract;
###
###    	Extract -> Save [label="Option 1"];
###    	Extract -> Kafka [label="Option 2"];
###
###    }
###

from collections import deque, namedtuple
import itertools
import json
import math
from optparse import OptionParser, OptionGroup
import os
import resource
import StringIO
import socket
import sys
import tempfile
import threading
import traceback
import shutil

import h5py
import numpy

import gi
gi.require_version('Gst', '1.0')
from gi.repository import GObject, Gst
GObject.threads_init()
Gst.init(None)

import lal
from lal import LIGOTimeGPS

from glue import iterutils
from glue import segments
from glue import segmentsUtils
from glue.lal import CacheEntry
from glue.ligolw import ligolw
from glue.ligolw import utils as ligolw_utils
from glue.ligolw.utils import process as ligolw_process
from glue.ligolw.utils import segments as ligolw_segments

from gstlal import aggregator
from gstlal import bottle
from gstlal import datasource
from gstlal import httpinterface
from gstlal import pipeio
from gstlal import pipeparts
from gstlal import simplehandler

from gstlal.fxtools import auxcache
from gstlal.fxtools import multichannel_datasource
from gstlal.fxtools import multirate_datasource
from gstlal.fxtools import sngltriggertable
from gstlal.fxtools import utils

#
# Make sure we have sufficient resources
# We allocate far more memory than we need, so this is okay
#

def setrlimit(res, lim):
	hard_lim = resource.getrlimit(res)[1]
	resource.setrlimit(res, (lim if lim is not None else hard_lim, hard_lim))

# set the number of processes and total set size up to hard limit and
# shrink the per-thread stack size (default is 10 MiB)
setrlimit(resource.RLIMIT_NPROC, None)
setrlimit(resource.RLIMIT_AS, None)
setrlimit(resource.RLIMIT_RSS, None)

# FIXME:  tests at CIT show that this next tweak has no effect.  it's
# possible that SL7 has lowered the default stack size from SL6 and we
# don't need to do this anymore.  remove?
setrlimit(resource.RLIMIT_STACK, 1024 * 1024) # 1 MiB per thread


# =============================
# 
#           classes
#
# =============================


class MultiChannelHandler(simplehandler.Handler):
	"""
	A subclass of simplehandler.Handler to be used with 
	multiple channels.

	Implements additional message handling for dealing with spectrum
	messages and creates trigger files containing features for use in iDQ.
	"""
	def __init__(self, mainloop, pipeline, data_source_info, options, **kwargs):
		self.lock = threading.Lock()
		self.out_path = options.out_path
		self.instrument = data_source_info.instrument
		self.frame_segments = data_source_info.frame_segments
		self.keys = kwargs.pop("keys")
		self.num_samples = len(self.keys)
		self.sample_rate = options.sample_rate
		self.waveforms = kwargs.pop("waveforms")
		self.basename = kwargs.pop("basename")
		self.waveform_type = options.waveform

		# format keys used for saving, etc.
		self.aggregate_rate = True # NOTE: hard-coded for now
		if self.aggregate_rate:
			self.keys = list(set([key[0] for key in self.keys]))
		else:
			self.keys = [os.path.join(key[0], str(key[1]).zfill(4)) for key in self.keys]

		# format id for aesthetics
		self.job_id = str(options.job_id).zfill(4)
		self.subset_id = str(kwargs.pop("subset_id")).zfill(4)

		### iDQ saving properties
		self.timestamp = None
		self.last_save_time = None
		self.last_persist_time = None
		self.cadence = options.cadence
		self.persist_cadence = options.persist_cadence
		self.feature_start_time = options.feature_start_time
		self.feature_end_time = options.feature_end_time
		self.columns = ['start_time', 'stop_time', 'trigger_time', 'frequency', 'q', 'snr', 'phase', 'sigmasq', 'chisq']
		self.feature_queue = utils.FeatureQueue(self.keys, self.columns, self.sample_rate)

		# set whether data source is live
		self.is_live = data_source_info.data_source in data_source_info.live_sources

		# get base temp directory
		if '_CONDOR_SCRATCH_DIR' in os.environ:
			self.tmp_dir = os.environ['_CONDOR_SCRATCH_DIR']
		else:
			self.tmp_dir = os.environ['TMPDIR']

		# feature saving properties
		if options.save_format == 'hdf5':
			self.fdata = utils.HDF5FeatureData(self.columns, keys = self.keys, cadence = self.cadence, sample_rate = self.sample_rate)

		elif options.save_format == 'ascii':
			self.header = "# %18s\t%20s\t%20s\t%10s\t%8s\t%8s\t%8s\t%10s\t%s\n" % ("start_time", "stop_time", "trigger_time", "frequency", "phase", "q", "chisq", "snr", "channel")
			self.fdata = deque(maxlen = 25000)
			self.fdata.append(self.header)

		elif options.save_format == 'kafka':
			self.kafka_partition = options.kafka_partition
			self.kafka_topic = options.kafka_topic
			self.kafka_conf = {'bootstrap.servers': options.kafka_server}
			self.producer = Producer(self.kafka_conf)

		elif options.save_format == 'bottle':
			assert not options.disable_web_service, 'web service is not available to use bottle to transfer features'
			self.feature_data = deque(maxlen = 2000)
			bottle.route("/feature_subset")(self.web_get_feature_data)

		# set up bottle routes for PSDs
		self.psds = {}
		if not options.disable_web_service:
			bottle.route("/psds.xml")(self.web_get_psd_xml)

		super(MultiChannelHandler, self).__init__(mainloop, pipeline, **kwargs)

	def do_on_message(self, bus, message):
		"""!
		Handle application-specific message types, 
		e.g., spectrum messages.
		
		@param bus: A reference to the pipeline's bus
		@param message: A reference to the incoming message
		"""
		#
		# return value of True tells parent class that we have done
		# all that is needed in response to the message, and that
		# it should ignore it.  a return value of False means the
		# parent class should do what it thinks should be done
		#
		if message.type == Gst.MessageType.ELEMENT:
			if message.get_structure().get_name() == "spectrum":
				# get the channel name & psd.
				instrument, info = message.src.get_name().split("_", 1)
				channel, _ = info.rsplit("_", 1)
				psd = pipeio.parse_spectrum_message(message)
				# save psd
				self.psds[channel] = psd
				return True		
		return False

	def bufhandler(self, elem, sink_dict):
		"""
		Processes rows from a Gstreamer buffer and
		handles conditions for file saving.

		@param elem: A reference to the gstreamer element being processed
		@param sink_dict: A dictionary containing references to gstreamer elements
		"""
		with self.lock:
			buf = elem.emit("pull-sample").get_buffer()
			buftime = int(buf.pts / 1e9)
			channel, rate  = sink_dict[elem]

			# push new stream event to queue if done processing current timestamp
			if len(self.feature_queue):
				feature_subset = self.feature_queue.pop()
				self.timestamp = feature_subset['timestamp']

				# set save times and initialize specific saving properties if not already set
				if self.last_save_time is None:
					self.last_save_time = self.timestamp
					self.last_persist_time = self.timestamp
					if options.save_format =='hdf5':
						duration = utils.floor_div(self.timestamp + self.persist_cadence, self.persist_cadence) - self.timestamp
						self.set_hdf_file_properties(self.timestamp, duration)

				# Save triggers once per cadence if saving to disk
				if options.save_format == 'hdf5' or options.save_format == 'ascii':
					if self.timestamp and utils.in_new_epoch(self.timestamp, self.last_save_time, self.cadence) or (self.timestamp == self.feature_end_time):
						logger.info("saving features to disk at timestamp = %d" % self.timestamp)
						if options.save_format == 'hdf5':
							self.to_hdf_file()
						elif options.save_format == 'ascii':
							self.to_trigger_file(self.timestamp)
							self.fdata.clear()
							self.fdata.append(self.header)
						self.last_save_time = self.timestamp

				# persist triggers once per persist cadence if using hdf5 format
				if options.save_format == 'hdf5':
					if self.timestamp and utils.in_new_epoch(self.timestamp, self.last_persist_time, self.persist_cadence):
						logger.info("persisting features to disk at timestamp = %d" % self.timestamp)
						self.finish_hdf_file()
						self.last_persist_time = self.timestamp
						self.set_hdf_file_properties(self.timestamp, self.persist_cadence)

				# add features to respective format specified
				if options.save_format == 'kafka':
					if options.data_transfer == 'table':
						self.producer.produce(timestamp = self.timestamp, topic = self.kafka_topic, value = json.dumps(feature_subset))
					elif options.data_transfer == 'row':
						for row in itertools.chain(*feature_subset['features'].values()):
							if row:
								self.producer.produce(timestamp = self.timestamp, topic = self.kafka_topic, value = json.dumps(row))
					self.producer.poll(0) ### flush out queue of sent packets
				elif options.save_format == 'bottle':
					self.feature_data.append(feature_subset)
				elif options.save_format == 'hdf5':
					self.fdata.append(self.timestamp, feature_subset['features'])

			# read buffer contents
			for i in range(buf.n_memory()):
				memory = buf.peek_memory(i)
				result, mapinfo = memory.map(Gst.MapFlags.READ)
				assert result
				# NOTE NOTE NOTE NOTE
				# It is critical that the correct class'
				# .from_buffer() method be used here.  This
				# code is interpreting the buffer's
				# contents as an array of C structures and
				# building instances of python wrappers of
				# those structures but if the python
				# wrappers are for the wrong structure
				# declaration then terrible terrible things
				# will happen
				if mapinfo.data:
					if (buftime >= self.feature_start_time and buftime <= self.feature_end_time):
						for row in sngltriggertable.GSTLALSnglTrigger.from_buffer(mapinfo.data):
							self.process_row(channel, rate, buftime, row)
				memory.unmap(mapinfo)

			del buf
			return Gst.FlowReturn.OK

	def process_row(self, channel, rate, buftime, row):
		"""
		Given a channel, rate, and the current buffer
		time, will process a row from a gstreamer buffer.
		"""
		# if segments provided, ensure that trigger falls within these segments
		if self.frame_segments[self.instrument]:
			trigger_seg = segments.segment(LIGOTimeGPS(row.end_time, row.end_time_ns), LIGOTimeGPS(row.end_time, row.end_time_ns))

		if not self.frame_segments[self.instrument] or self.frame_segments[self.instrument].intersects_segment(trigger_seg):
			freq, q, duration = self.waveforms[channel].parameter_grid[rate][row.channel_index]
			filter_duration = self.waveforms[channel].filter_duration[rate]
			filter_stop_time = row.end_time + row.end_time_ns * 1e-9

			# set trigger time based on waveform
			if self.waveform_type == 'sine_gaussian':
				trigger_time = filter_stop_time
				start_time = trigger_time - duration / 2.
				stop_time = trigger_time + duration / 2.

			elif self.waveform_type == 'half_sine_gaussian':
				trigger_time = filter_stop_time
				start_time = trigger_time - duration
				stop_time = trigger_time

			# append row for data transfer/saving
			timestamp = int(numpy.floor(trigger_time))
			feature_row = {'timestamp':timestamp, 'channel':channel, 'start_time':start_time, 'stop_time':stop_time, 'snr':row.snr,
			               'trigger_time':trigger_time, 'frequency':freq, 'q':q, 'phase':row.phase, 'sigmasq':row.sigmasq, 'chisq':row.chisq}
			self.feature_queue.append(timestamp, channel, feature_row)

			# save iDQ compatible data
			if options.save_format == 'ascii':
				channel_tag = ('%s_%i_%i' %(channel, rate/4, rate/2)).replace(":","_",1)
				self.fdata.append("%20.9f\t%20.9f\t%20.9f\t%10.3f\t%8.3f\t%8.3f\t%8.3f\t%10.3f\t%s\n" % (start_time, stop_time, trigger_time, freq, row.phase, q, row.chisq, row.snr, channel_tag))


	def to_trigger_file(self, buftime = None):
		"""
		Dumps triggers saved in memory to disk, following an iDQ ingestion format.
		Contains a header specifying aligned columns, along with triggers, one per row.
		Uses the T050017 filenaming convention.
		NOTE: This method should only be called by an instance that is locked.
		"""
		# Only write triggers to disk where the associated data structure has more
		# than the header stored within.
		if len(self.fdata) > 1 :
			fname = '%s-%d-%d.%s' % (self.tag, utils.floor_div(self.last_save_time, self.cadence), self.cadence, "trg")
			path = os.path.join(self.out_path, self.tag, self.tag+"-"+str(fname.split("-")[2])[:5])
			fpath = os.path.join(path, fname)
			tmpfile = fpath+"~"
			try:
				os.makedirs(path)
			except OSError:
				pass
			with open(tmpfile, 'w') as f:
 				f.write(''.join(self.fdata))
			shutil.move(tmpfile, fpath)
			if buftime:
				latency = numpy.round(int(aggregator.now()) - buftime)
				logger.info("buffer timestamp = %d, latency at write stage = %d" % (buftime, latency))

	def to_hdf_file(self):
		"""
		Dumps triggers saved in memory to disk in hdf5 format.
		Uses the T050017 filenaming convention.
		NOTE: This method should only be called by an instance that is locked.
		"""
		self.fdata.dump(self.tmp_path, self.fname, utils.floor_div(self.last_save_time, self.cadence), tmp = True)

	def finish_hdf_file(self):
		"""
		Move a temporary hdf5 file to its final location after
		all file writes have been completed.
		"""
		final_path = os.path.join(self.fpath, self.fname)+".h5"
		tmp_path = os.path.join(self.tmp_path, self.fname)+".h5.tmp"
		shutil.move(tmp_path, final_path)

	def finalize(self):
		"""
		Clears out remaining features from the queue for saving to disk.
		"""
		# save remaining triggers
		if options.save_format == 'hdf5':
			self.feature_queue.flush()
			while len(self.feature_queue):
				feature_subset = self.feature_queue.pop()
				self.fdata.append(feature_subset['timestamp'], feature_subset['features'])

			self.to_hdf_file()
			self.finish_hdf_file()

		elif options.save_format == 'ascii':
			self.to_trigger_file()

	def set_hdf_file_properties(self, start_time, duration):
		"""
		Returns the file name, as well as locations of temporary and permanent locations of
		directories where triggers will live, when given the current gps time and a gps duration.
		Also takes care of creating new directories as needed and removing any leftover temporary files.
		"""
		# set/update file names and directories with new gps time and duration
		self.fname = os.path.splitext(utils.to_trigger_filename(self.basename, start_time, duration, 'h5'))[0]
		self.fpath = utils.to_trigger_path(os.path.abspath(self.out_path), self.basename, start_time, self.job_id, self.subset_id)
		self.tmp_path = utils.to_trigger_path(self.tmp_dir, self.basename, start_time, self.job_id, self.subset_id)

		# create temp and output directories if they don't exist
		aggregator.makedir(self.fpath)
		aggregator.makedir(self.tmp_path)

		# delete leftover temporary files
		tmp_file = os.path.join(self.tmp_path, self.fname)+'.h5.tmp'
		if os.path.isfile(tmp_file):
			os.remove(tmp_file)

	def gen_psd_xmldoc(self):
		xmldoc = lal.series.make_psd_xmldoc(self.psds)
		process = ligolw_process.register_to_xmldoc(xmldoc, "gstlal_idq", {})
		ligolw_process.set_process_end_time(process)
		return xmldoc

	def web_get_psd_xml(self):
		with self.lock:
			output = StringIO.StringIO()
			ligolw_utils.write_fileobj(self.gen_psd_xmldoc(), output)
			outstr = output.getvalue()
			output.close()
		return outstr

	def web_get_feature_data(self):
		header = {'Content-type': 'application/json'}
		# if queue is empty, send appropriate response
		if not self.feature_data:
			status = 204
			body = json.dumps({'error': "No Content"})
		# else, get feature data and send as JSON
		else:
			status = 200
			with self.lock:
				body = json.dumps(self.feature_data.popleft())
		return bottle.HTTPResponse(status = status, headers = header, body = body)

class LinkedAppSync(pipeparts.AppSync):
	def __init__(self, appsink_new_buffer, sink_dict = None):
		self.time_ordering = 'full'
		if sink_dict:
			self.sink_dict = sink_dict
		else:
			self.sink_dict = {}
		super(LinkedAppSync, self).__init__(appsink_new_buffer, self.sink_dict.keys())
	
	def attach(self, appsink):
		"""
		connect this AppSync's signal handlers to the given appsink
		element.  the element's max-buffers property will be set to
		1 (required for AppSync to work).
		"""
		if appsink in self.appsinks:
			raise ValueError("duplicate appsinks %s" % repr(appsink))
		appsink.set_property("max-buffers", 1)
		handler_id = appsink.connect("new-preroll", self.new_preroll_handler)
		assert handler_id > 0
		handler_id = appsink.connect("new-sample", self.new_sample_handler)
		assert handler_id > 0
		handler_id = appsink.connect("eos", self.eos_handler)
		assert handler_id > 0
		self.appsinks[appsink] = None
		_, rate, channel = appsink.name.split("_", 2)
		self.sink_dict.setdefault(appsink, (channel, int(rate)))
		return appsink
	
	def pull_buffers(self, elem):
		"""
		for internal use.  must be called with lock held.
		"""

		while 1:
			if self.time_ordering == 'full':
				# retrieve the timestamps of all elements that
				# aren't at eos and all elements at eos that still
				# have buffers in them
				timestamps = [(t, e) for e, t in self.appsinks.iteritems() if e not in self.at_eos or t is not None]
				# if all elements are at eos and none have buffers,
				# then we're at eos
				if not timestamps:
					return Gst.FlowReturn.EOS
				# find the element with the oldest timestamp.  None
				# compares as less than everything, so we'll find
				# any element (that isn't at eos) that doesn't yet
				# have a buffer (elements at eos and that are
				# without buffers aren't in the list)
				timestamp, elem_with_oldest = min(timestamps)
				# if there's an element without a buffer, quit for
				# now --- we require all non-eos elements to have
				# buffers before proceding
				if timestamp is None:
					return Gst.FlowReturn.OK
				# clear timestamp and pass element to handler func.
				# function call is done last so that all of our
				# book-keeping has been taken care of in case an
				# exception gets raised
				self.appsinks[elem_with_oldest] = None
				self.appsink_new_buffer(elem_with_oldest, self.sink_dict)
	
			elif self.time_ordering == 'partial':
				# retrieve the timestamps of elements of a given channel
				# that aren't at eos and all elements at eos that still
				# have buffers in them
				channel = self.sink_dict[elem][0]
				timestamps = [(t, e) for e, t in self.appsinks.iteritems() if self.sink_dict[e][0] == channel and (e not in self.at_eos or t is not None)]
				# if all elements are at eos and none have buffers,
				# then we're at eos
				if not timestamps:
					return Gst.FlowReturn.EOS
				# find the element with the oldest timestamp.  None
				# compares as less than everything, so we'll find
				# any element (that isn't at eos) that doesn't yet
				# have a buffer (elements at eos and that are
				# without buffers aren't in the list)
				timestamp, elem_with_oldest = min(timestamps)
				# if there's an element without a buffer, quit for
				# now --- we require all non-eos elements to have
				# buffers before proceding
				if timestamp is None:
					return Gst.FlowReturn.OK
				# clear timestamp and pass element to handler func.
				# function call is done last so that all of our
				# book-keeping has been taken care of in case an
				# exception gets raised
				self.appsinks[elem_with_oldest] = None
				self.appsink_new_buffer(elem_with_oldest, self.sink_dict)
			
			elif self.time_ordering == 'none':
				if not elem in self.appsinks:
					return Gst.FlowReturn.EOS
				if self.appsinks[elem] is None:
					return Gst.FlowReturn.OK
				self.appsinks[elem] = None
				self.appsink_new_buffer(elem, self.sink_dict)

# =============================
# 
#     command line parser
#
# =============================

def parse_command_line():

	parser = OptionParser(description = __doc__)

	# First append the datasource common options
	multichannel_datasource.append_options(parser)

	group = OptionGroup(parser, "Waveform Options", "Adjust waveforms/parameter space used for feature extraction")
	group.add_option("-m", "--mismatch", type = "float", default = 0.2, help = "Mismatch between templates, mismatch = 1 - minimal match. Default = 0.2.")
	group.add_option("-q", "--qhigh", type = "float", default = 20, help = "Q high value for half sine-gaussian waveforms. Default = 20.")
	group.add_option("--waveform", metavar = "string", default = "half_sine_gaussian", help = "Specifies the waveform used for matched filtering. Possible options: (half_sine_gaussian, sine_gaussian). Default = half_sine_gaussian")
	parser.add_option_group(group)

	group = OptionGroup(parser, "Saving Options", "Adjust parameters used for saving/persisting features to disk as well as directories specified")
	group.add_option("--out-path", metavar = "path", default = ".", help = "Write to this path. Default = .")
	group.add_option("--description", metavar = "string", default = "GSTLAL_IDQ_FEATURES", help = "Set the filename description in which to save the output.")
	group.add_option("--save-format", metavar = "string", default = "hdf5", help = "Specifies the save format (ascii/hdf5/kafka/bottle) of features written to disk. Default = hdf5")
	group.add_option("--data-transfer", metavar = "string", default = "table", help = "Specifies the format of features transferred over-the-wire (table/row). Default = table")
	group.add_option("--cadence", type = "int", default = 20, help = "Rate at which to write trigger files to disk. Default = 20 seconds.")
	group.add_option("--persist-cadence", type = "int", default = 200, help = "Rate at which to persist trigger files to disk, used with hdf5 files. Needs to be a multiple of save cadence. Default = 200 seconds.")
	parser.add_option_group(group)

	group = OptionGroup(parser, "Kafka Options", "Adjust settings used for pushing extracted features to a Kafka topic.")
	group.add_option("--kafka-partition", metavar = "string", help = "If using Kafka, sets the partition that this feature extractor is assigned to.")
	group.add_option("--kafka-topic", metavar = "string", help = "If using Kafka, sets the topic name that this feature extractor publishes feature vector subsets to.")
	group.add_option("--kafka-server", metavar = "string", help = "If using Kafka, sets the server url that the kafka topic is hosted on.")
	group.add_option("--job-id", type = "string", default = "0001", help = "Sets the job identication of the feature extractor with a 4 digit integer string code, padded with zeros. Default = 0001")
	parser.add_option_group(group)

	group = OptionGroup(parser, "Program Behavior")
	group.add_option("--local-frame-caching", action = "store_true", help = "Pre-reads frame data and stores to local filespace.")
	group.add_option("--disable-web-service", action = "store_true", help = "If set, disables web service that allows monitoring of PSDS of aux channels.")
	group.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	group.add_option("--nxydump-segment", metavar = "start:stop", help = "Set the time interval to dump from nxydump elements (optional).")
	group.add_option("--sample-rate", type = "int", metavar = "Hz", default = 1, help = "Set the sample rate for feature timeseries output, must be a power of 2. Default = 1 Hz.")
	group.add_option("--feature-start-time", type = "int", metavar = "seconds", help = "Set the start time of the segment to output features in GPS seconds. Required unless --data-source=lvshm")
	group.add_option("--feature-end-time", type = "int", metavar = "seconds", help = "Set the end time of the segment to output features in GPS seconds.  Required unless --data-source=lvshm")
	parser.add_option_group(group)

	#
	# parse the arguments and sanity check
	#

	options, filenames = parser.parse_args()

	# Sanity check the options

	# set gps ranges for live and offline sources
	if options.data_source in ("lvshm", "framexmit"):
		options.gps_start_time = int(aggregator.now())
		# NOTE: set the gps end time to be "infinite"
		options.gps_end_time = 2000000000

	if options.feature_start_time is None:
		options.feature_start_time = int(options.gps_start_time)
	if options.feature_end_time is None:
		options.feature_end_time = int(options.gps_end_time)

	# check if input sample rate is sensible
	assert options.sample_rate == 1 or options.sample_rate % 2 == 0

	# check if persist and save cadence times are sensible
	assert options.persist_cadence >= options.cadence
	assert (options.persist_cadence % options.cadence) == 0

	# check if there are any segments to dump to disk
	if options.nxydump_segment:
		options.nxydump_segment, = segmentsUtils.from_range_strings([options.nxydump_segment], boundtype = LIGOTimeGPS)

	return options, filenames


# =============================
# 
#             main
#
# =============================

#  
# parsing and setting up some core structures
#

options, filenames = parse_command_line()

data_source_info = multichannel_datasource.DataSourceInfo(options)
instrument = data_source_info.instrument
basename = '%s-%s' % (instrument[:1], options.description)
waveforms = {}

# only load kafka library if triggers are transferred via kafka topic
if options.save_format == 'kafka':
	from confluent_kafka import Producer

#
# set up logging
#

duration = options.feature_end_time - options.feature_start_time
logdir = os.path.join(options.out_path, 'logs', options.job_id)
aggregator.makedir(logdir)

logger = utils.get_logger('gstlal-feature-extractor_%d-%d' % (options.feature_start_time, duration), rootdir=logdir, verbose=options.verbose)
logger.info("writing log to %s" % logdir)

#
# set up local frame caching, if specified
#

if options.local_frame_caching:

	# get base temp directory
	if '_CONDOR_SCRATCH_DIR' in os.environ:
		tmp_dir = os.environ['_CONDOR_SCRATCH_DIR']
	else:
		tmp_dir = os.environ['TMPDIR']

	# create local frame directory
	local_path = os.path.join(tmp_dir, 'local_frames/')
	aggregator.makedir(local_path)

	# save local frame cache
	logger.info("caching frame data locally to %s" % local_path)
	f, fname = tempfile.mkstemp(".cache")
	f = open(fname, "w")

	data_source_info.local_cache_list = auxcache.cache_aux(data_source_info, logger, output_path = local_path, verbose = options.verbose)
	for cacheentry in data_source_info.local_cache_list:
		# guarantee a lal cache compliant file with only integer starts and durations
		cacheentry.segment = segments.segment( int(cacheentry.segment[0]), int(math.ceil(cacheentry.segment[1])) )
		print >>f, str(cacheentry)
	f.close()

	data_source_info.frame_cache = fname

#
# process channel subsets in serial
#

for subset_id, channel_subset in enumerate(data_source_info.channel_subsets, 1):

	#
	# checkpointing for offline analysis for hdf5 output
	#

	if options.data_source not in ("lvshm", "framexmit") and options.save_format == 'hdf5':
		try:
			# get path where triggers are located
			duration = options.feature_end_time - options.feature_start_time
			fname = utils.to_trigger_filename(basename, options.feature_start_time, duration, 'h5')
			fpath = utils.to_trigger_path(os.path.abspath(options.out_path), basename, options.feature_start_time, options.job_id, str(subset_id).zfill(4))
			trg_file = os.path.join(fpath, fname)

			# visit groups within a given hdf5 file
			with h5py.File(trg_file, 'r') as f:
				f.visit(lambda item: f[item])
			# file is OK and there is no need to process it,
			# skip ahead in the loop
			continue

		except IOError:
			# file does not exist or is corrupted, need to
			# reprocess
			logger.info("checkpoint: {0} of {1} files completed and continuing with channel subset {2}".format((subset_id - 1), len(data_source_info.channel_subsets), subset_id))
			pass

		logger.info("processing channel subset %d of %d" % (subset_id, len(data_source_info.channel_subsets)))

	#
	# if web services serving up bottle routes are enabled,
	# create a new, empty, Bottle application and make it the
	# current default, then start http server to serve it up
	#

	if not options.disable_web_service:
		bottle.default_app.push()
		# uncomment the next line to show tracebacks when something fails
		# in the web server
		#bottle.app().catchall = False
		import base64, uuid	# FIXME:  don't import when the uniquification scheme is fixed
		httpservers = httpinterface.HTTPServers(
			# FIXME:  either switch to using avahi's native name
			# uniquification system or adopt a naturally unique naming
			# scheme (e.g., include a search identifier and job
			# number).
			service_name = "gstlal_idq (%s)" % base64.urlsafe_b64encode(uuid.uuid4().bytes),
			service_properties = {},
			verbose = options.verbose
		)

		# Set up a registry of the resources that this job provides
		@bottle.route("/")
		@bottle.route("/index.html")
		def index(channel_list = channel_subset):
			# get the host and port to report in the links from the
			# request we've received so that the URLs contain the IP
			# address by which client has contacted us
			netloc = bottle.request.urlparts[1]
			server_address = "http://%s" % netloc
			yield "<html><body>\n<h3>%s %s</h3>\n<p>\n" % (netloc, " ".join(sorted(channel_list)))
			for route in sorted(bottle.default_app().routes, key = lambda route: route.rule):
				# don't create links back to this page
				if route.rule in ("/", "/index.html"):
					continue
				# only create links for GET methods
				if route.method != "GET":
					continue
				yield "<a href=\"%s%s\">%s</a><br>\n" % (server_address, route.rule, route.rule)
			yield "</p>\n</body></html>"
		# FIXME:  get service-discovery working, then don't do this
		open("registry.txt", "w").write("http://%s:%s/\n" % (socket.gethostname(), httpservers[0][0].port))

	#
	# building the event loop and pipeline
	#

	logger.info("assembling pipeline...")

	mainloop = GObject.MainLoop()
	pipeline = Gst.Pipeline(sys.argv[0])

	# generate multiple channel sources, and link up pipeline
	head = multichannel_datasource.mkbasicmultisrc(pipeline, data_source_info, channel_subset, verbose = options.verbose)
	src = {}

	for channel in channel_subset:
		# define sampling rates used
		samp_rate = int(data_source_info.channel_dict[channel]['fsamp'])
		max_rate = min(data_source_info.max_sample_rate, samp_rate)
		min_rate = min(data_source_info.min_sample_rate, max_rate)
		n_rates = int(numpy.log2(max_rate/min_rate) + 1)
		rates = [min_rate*2**i for i in range(n_rates)]

		# choose range of basis parameters
		# NOTE: scale down frequency range by downsample_factor to deal with rolloff from downsampler
		downsample_factor = 0.8
		flow = min_rate/4.
		fhigh = max_rate/2.
		qlow = 3.3166
		if data_source_info.extension == 'ini':
			qhigh = data_source_info.channel_dict[channel]['qhigh']
		else:
			qhigh = options.qhigh

		# generate templates
		if options.waveform == 'half_sine_gaussian':
			waveforms[channel] = utils.HalfSineGaussianGenerator((flow, fhigh), (qlow, qhigh), rates, mismatch=options.mismatch, downsample_factor=downsample_factor)
		elif options.waveform == 'sine_gaussian':
			waveforms[channel] = utils.SineGaussianGenerator((flow, fhigh), (qlow, qhigh), rates, mismatch=options.mismatch, downsample_factor=downsample_factor)
		else:
			raise NotImplementedError

		if options.latency_output:
			head[channel] = pipeparts.mklatency(pipeline, head[channel], name=utils.latency_name('beforewhitening', 2, channel))

		# whiten auxiliary channel data
		for rate, thishead in multirate_datasource.mkwhitened_multirate_src(pipeline, head[channel], rates, samp_rate, instrument, channel_name = channel, width=32, nxydump_segment=options.nxydump_segment).items():
			if options.latency_output:
				thishead = pipeparts.mklatency(pipeline, thishead, name=utils.latency_name('afterwhitening', 3, channel, rate))

			# determine whether to do time-domain or frequency-domain convolution
			time_domain = (waveforms[channel].sample_pts[rate]*rate) < (5*waveforms[channel].sample_pts[rate]*numpy.log2(rate))

			# create FIR bank of half sine-gaussian templates
			fir_matrix = numpy.array([waveform for waveform in waveforms[channel].generate_templates(rate)])
			thishead = pipeparts.mkqueue(pipeline, thishead, max_size_buffers = 0, max_size_bytes = 0, max_size_time = Gst.SECOND * 30)
			thishead = pipeparts.mkfirbank(pipeline, thishead, fir_matrix = fir_matrix, time_domain = time_domain, block_stride = int(rate), latency = waveforms[channel].latency[rate])

			# add queues, change stream format, add tags
			if options.latency_output:
				thishead = pipeparts.mklatency(pipeline, thishead, name=utils.latency_name('afterFIRbank', 4, channel, rate))
			thishead = pipeparts.mkqueue(pipeline, thishead, max_size_buffers = 1, max_size_bytes = 0, max_size_time = 0)
			thishead = pipeparts.mktogglecomplex(pipeline, thishead)
			thishead = pipeparts.mkcapsfilter(pipeline, thishead, caps = "audio/x-raw, format=Z64LE, rate=%i" % rate)
			thishead = pipeparts.mktaginject(pipeline, thishead, "instrument=%s,channel-name=%s" %( instrument, channel))

			# dump segments to disk if specified
			tee = pipeparts.mktee(pipeline, thishead)
			if options.nxydump_segment:
				pipeparts.mknxydumpsink(pipeline, pipeparts.mkqueue(pipeline, tee), "snrtimeseries_%s_%s.txt" % (channel, repr(rate)), segment = options.nxydump_segment)

			# extract features from time series
			thishead = pipeparts.mktrigger(pipeline, tee, int(rate // options.sample_rate), max_snr = True)

			if options.latency_output:
				thishead = pipeparts.mklatency(pipeline, thishead, name=utils.latency_name('aftertrigger', 5, channel, rate))

			# link to src for processing by appsync
			src[(channel, rate)] = thishead

	# define structures to synchronize output streams and extract triggers from buffer
	logger.info("setting up pipeline handler...")
	handler = MultiChannelHandler(mainloop, pipeline, data_source_info, options, keys = src.keys(), waveforms = waveforms, basename = basename, subset_id = subset_id)

	logger.info("attaching appsinks to pipeline...")
	appsync = LinkedAppSync(appsink_new_buffer = handler.bufhandler)
	appsinks = set(appsync.add_sink(pipeline, src[(channel, rate)], name = "sink_%s_%s" % (rate, channel)) for channel, rate in src.keys())
	logger.info("attached %d appsinks to pipeline." % len(appsinks))

	# Allow Ctrl+C or sig term to gracefully shut down the program for online
	# sources, otherwise it will just kill it
	if data_source_info.data_source in ("lvshm", "framexmit"):# what about nds online?
		simplehandler.OneTimeSignalHandler(pipeline)

	# Seek
	if pipeline.set_state(Gst.State.READY) == Gst.StateChangeReturn.FAILURE:
		raise RuntimeError("pipeline failed to enter READY state")
	if data_source_info.data_source not in ("lvshm", "framexmit"):# what about nds online?
		datasource.pipeline_seek_for_gps(pipeline, options.gps_start_time, options.gps_end_time)

	#
	# Run pipeline
	#

	if pipeline.set_state(Gst.State.PLAYING) == Gst.StateChangeReturn.FAILURE:
		raise RuntimeError("pipeline failed to enter PLAYING state")

	logger.info("running pipeline...")

	mainloop.run()

	# save remaining triggers
	logger.info("persisting features to disk...")
	handler.finalize()

	#
	# Shut down pipeline
	#

	logger.info("shutting down pipeline...")

	#
	# Shutdown the web interface servers and garbage collect the Bottle
	# app.  This should release the references the Bottle app's routes
	# hold to the pipeline's data (like template banks and so on).
	#

	if not options.disable_web_service:
		del httpservers
		bottle.default_app.pop()

	#
	# Set pipeline state to NULL and garbage collect the handler
	#

	if pipeline.set_state(Gst.State.NULL) != Gst.StateChangeReturn.SUCCESS:
		raise RuntimeError("pipeline could not be set to NULL")

	del handler.pipeline
	del handler

#
# Cleanup local frame file cache and related frames
#

if options.local_frame_caching:
	logger.info("deleting temporary cache file and frames...")

	# remove frame cache
	os.remove(data_source_info.frame_cache)

	# remove local frames
	for cacheentry in data_source_info.local_cache_list:
		os.remove(cacheentry.path)

	del data_source_info.local_cache_list

#
# close program manually if data source is live
#

if options.data_source in ("lvshm", "framexmit"):
	sys.exit(0)
