#!/usr/bin/env python
# Copyright (C) 2010  Erin Kara, Chad Hanna
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#


import sys, os
from optparse import OptionParser
from glue import cbcwebpage, lal
from pylal import git_version
from pylal.followup_page import *

__author__ = "Erin Kara <ekara@ligo.caltech.edu>"
__version__ = "git id %s" % git_version.id
__date__ = git_version.date


#######################################################################
# Main Code
#######################################################################


parser = OptionParser(version = "%prog CVS $Id$")
parser.add_option("-f", "--followups", action="store_true", default=False, help="Enable followup pipeline output.  Disabled by default.")
opts,args = parser.parse_args()


if len(args) > 0:
	raise RuntimeError("Did not understand extra command line arguments: %r" % args)



# Make an instance of cbcpage
page = cbcwebpage.cbcpage(title="gstlal 8-hour Job Page")


# Section for Correlations between sngl_inspiral parameters
correlations = page.add_subpage("correlations","Correlations","Correlations")


perf = correlations.add_section("perf", "Performance")
perf.add_table(cbcwebpage.image_glob("gstlal_inspiral.*.out.png"), "perf", "Single Detector Performance")


snr_chisq = correlations.add_section("snr_chisq","Chisq SNR for Triggers")
snr_chisq.add_table(cbcwebpage.image_glob("snr_chisq_*.png"), "snr_chisq", "Chisq vs. SNR for Triggers")


m1m2 = correlations.add_section("m1_m2","Mass Distribution for Triggers")
m1m2.add_table(cbcwebpage.image_glob("m1_m2_*.png"), "m1_m2_H1", "mass1 vs. mass2 for Triggers")


snr_endtime = correlations.add_section("snr_endtime","SNR Time Series for Triggers")
snr_endtime.add_table(cbcwebpage.image_glob("snr_endtime_*.png"), "snr_endtime", "SNR vs. end_time for Triggers")


snr_deff = correlations.add_section("snr_deff", "Effective Distace vs. SNR for Triggers")
snr_deff.add_table(cbcwebpage.image_glob("snr_effdistance_*.png"), "snr_deff", "Effective Distace vs. SNR for Triggers")



# Parse the cache.
if opts.followups:
	coinc_info, cache = cache_parser("followup_pipe.cache")

	# Loop over different "searches" like full data, etc.
	for search, coincs in coinc_info.items():
		events = []


		# pull out the events and sort them
		for coinc in coincs:
			events.append(Coinc(coinc, search, cache))
		events.sort(key=lambda x: x.rank)


		# loop over the followed up events
		for event in events:
			print >>sys.stderr, "processing coinc@%s in %s" % (event.coinctime, search)
			key = (str(event.coinctime))
			page.add_subpage(key,"my new page",link_text=key)
			#section for param table
			if search.lower() != "gps_only": event.write_param_table(page.subpages[key])
			#section for qscans
			event.add_htqscan(page.subpages[key])
			#section for seismic qscans
			event.add_seismicqscan(page.subpages[key])
			#section for rds qscans (CVT)
			event.add_rdsqscan(page.subpages[key])
			#dq
			event.add_dq(page.subpages[key])


# write the page to disk
page.write()
