#!/usr/bin/env python
#
# Copyright (C) 2010  Kipp Cannon <kipp.cannon@ligo.org>
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
gstlal_inspiral dag generator
"""


import sys, os
import tempfile
import ConfigParser
from optparse import OptionParser


from glue import pipeline
from glue import segments
from glue.lal import CacheEntry
import glue.ligolw.utils as utils
import glue.ligolw.utils.segments as ligolw_segments
from pylal import ligolw_tisi
from lalapps import power
from pylal.datatypes import LIGOTimeGPS
from gstlal import dagparts as gstlaldagparts

__author__ = "Kipp Cannon <kipp.cannon@ligo.org>"
__date__ = "" #FIXME
__version__ = "" #FIXME


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		usage = "%prog [options] ...",
		description = "FIXME"
	)
	parser.add_option("-i", "--injections", action = "store_true", help = "Add software inspiral.")
	parser.add_option("-f", "--config-file", metavar = "filename", help = "Use this configuration file (required).")
	parser.add_option("-l", "--log-path", metavar = "path", help = "Make condor put log files in this directory (required).")
	parser.add_option("-t", "--time-slides", metavar = "filename", help = "Load time-slide vectors from this XML file (required).")
	parser.add_option("--vetoes", metavar = "filename", help = "Load veto segments from this XML file (required).")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")

	options, filenames = parser.parse_args()

	required_options = ["log_path", "config_file", "time_slides", "vetoes"]
	missing_options = [option for option in required_options if getattr(options, option) is None]
	if missing_options:
		raise ValueError, "missing required options %s" % (sorted("--%s" % option.replace("_", "-") for option in missing_options))

	return options, filenames


#
# =============================================================================
#
#                               Main DAG Branch
#
# =============================================================================
#


def make_coinc_branch(dag, datafinds, seglists, time_slides, vetoes_cache, tag, do_injections = False, extentlimit = None, verbose = False):
	#
	# injection job
	#

	if do_injections:
		injnodes = gstlaldagparts.make_inspinj_fragment(dag, seglists.extent_all(), tag, 0.0)
	else:
		injnodes = set()

	# artificial parent-child relationship to force inspinj jobs to run before all
	# datafinds.  makes dag run faster because it allows string search jobs to
	# move onto the cluster before all the datafinds complete
	for injnode in injnodes:
		for datafindnode in datafinds:
			datafindnode.add_parent(injnode)

	#
	# trigger generator jobs
	#

	trigger_nodes = gstlaldagparts.make_single_instrument_stage(dag, datafinds, seglists, tag, inspinjnodes = injnodes, verbose = verbose)

	#
	# coincidence analysis
	#

	coinc_nodes = set()
	inj_cache = set([cache_entry for node in injnodes for cache_entry in node.get_output_cache()])
	for n, (time_slides_cache_entry, these_time_slides) in enumerate(time_slides.items()):
		if verbose:
			print >>sys.stderr, "%s %d/%d (%s):" % (tag, n + 1, len(time_slides), time_slides_cache_entry.path())

		#
		# ligolw_cafe & ligolw_add
		#

		tisi_cache = set([time_slides_cache_entry])
		nodes = set()
		for seg, parents, cache, clipseg in power.group_coinc_parents(trigger_nodes, these_time_slides, verbose = verbose):
			nodes |= power.make_lladd_fragment(dag, parents | injnodes, "%s_%d" % (tag, n), segment = seg, input_cache = cache | inj_cache, extra_input_cache = tisi_cache | vetoes_cache, remove_input = False, preserve_cache = inj_cache | tisi_cache | vetoes_cache)

		#
		# ligolw_thinca
		#

		if verbose:
			print >>sys.stderr, "building thinca jobs ..."
		if extentlimit is None:
			coinc_nodes |= gstlaldagparts.make_thinca_fragment(dag, nodes, "%s_%d" % (tag, n), verbose = verbose)
		else:
			coinc_nodes |= gstlaldagparts.make_thinca_fragment_maxextent(dag, nodes, "%s_%d" % (tag, n), verbose = verbose)
		if verbose:
			print >>sys.stderr, "done %s %d/%d" % (tag, n + 1, len(time_slides))

		#
		# ligolw_sqlite
		#

		if verbose:
			print >>sys.stderr, "building sqlite jobs ..."
		coinc_nodes = power.make_sqlite_fragment(dag, coinc_nodes, tag, verbose = verbose)

		#
		# lalapps_run_sqlite 
		#
		
                if verbose:
                        print >>sys.stderr, "building lalapps_run_sqlite jobs ..."
                coinc_nodes = gstlaldagparts.make_runsqlite_fragment(dag, coinc_nodes, tag, verbose = verbose)

	#
	# ligolw_inspinjfind
	#

	if do_injections:
		if verbose:
			print >>sys.stderr, "building inspinjfind jobs ..."
		coinc_nodes = gstlaldagparts.make_sqlitetoxml_fragment(dag, coinc_nodes, tag, verbose = verbose)
		coinc_nodes = gstlaldagparts.make_inspinjfind_fragment(dag, coinc_nodes, tag, verbose = verbose)
		coinc_nodes = power.make_sqlite_fragment(dag, coinc_nodes, tag+"inspinjfind", verbose = verbose)

	#
	# done
	#

	power.write_output_cache(coinc_nodes, "%s_%s_output.cache" % (os.path.splitext(dag.get_dag_file())[0], tag))
	return coinc_nodes


#
# =============================================================================
#
#                                  Initialize
#
# =============================================================================
#


#
# command line
#


options, filenames = parse_command_line()


#
# base string for file names
#


basename = os.path.splitext(os.path.basename(options.config_file))[0]


#
# create the config parser object and read in the ini file
#


config_parser = ConfigParser.ConfigParser()
config_parser.read(options.config_file)


#
# initialize lalapps.power and gstlal.dagparts modules
#


power.init_job_types(config_parser, job_types = ("datafind", "lladd", "sqlite"))
gstlaldagparts.init_job_types(config_parser, job_types = ("inspinj", "gstlalinspiral", "inspinjfind", "thinca", "runsqlite", "sqlitetoxml"))


#
# make directories to store the cache files, job logs, and trigger output
#


power.make_dag_directories(config_parser)
try:
	os.mkdir("triggers")
except:
	pass


#
# create a log file that the Condor jobs will write to
#


logfile = tempfile.mkstemp(prefix = basename, suffix = '.log', dir = options.log_path)[1]


#
# create the DAG writing the log to the specified directory
#


dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)


#
# get the instruments and raw segments
#


seglists = ligolw_segments.segmenttable_get_by_name(utils.load_filename(config_parser.get("input", "segments")), "RESULT")
if config_parser.has_option("pipeline", "pad_data"):
	seglists.contract(float(config_parser.get("pipeline", "pad_data")))
vetoes_cache = set([CacheEntry(None, None, None, "file://localhost" + os.path.abspath(options.vetoes))])


#
# Using time slide information, construct segment lists describing times
# requiring trigger construction.
#


if options.verbose:
	print >>sys.stderr, "computing segments for which gstlal_inspiral jobs are required ..."
min_segment_length = config_parser.getfloat("pipeline", "min_segment_length")
gstlal_inspiral_overlap = config_parser.getfloat("pipeline", "gstlal_inspiral_overlap")
background_time_slides = {}
background_seglists = segments.segmentlistdict()
for filename in [options.time_slides]:
	cache_entry = CacheEntry(None, None, None, "file://localhost" + os.path.abspath(filename))
	background_time_slides[cache_entry] = ligolw_tisi.load_time_slides(filename, verbose = options.verbose, gz = filename.endswith(".gz")).values()

	# Convert all time slide values to LIGOTimeGPS (which is a fixed width integer time format)
	# because otherwise segmentlistdict.offsets.clear() introduces nanosecond-scale
	# errors through inconsistent rounding.
	#
	# FIXME: come up with a better solution to this.  Make ligolw_tisi produce
	# integer nanosecond offsets?  Fix rounding issue in segmentlistdict.offsets.clear()?
	#
	# See PR 3434.

	for v1 in background_time_slides.values():
		for v2 in v1:
			for k in v2.iterkeys():
				v2[k] = LIGOTimeGPS(v2[k])

	background_seglists |= gstlaldagparts.compute_segment_lists(seglists, background_time_slides[cache_entry], min_segment_length, verbose = options.verbose)


#
# =============================================================================
#
#                                  Build DAG
#
# =============================================================================
#


#
# datafinds
#


datafinds = power.make_datafind_stage(dag, background_seglists, verbose = options.verbose)
gstlaldagparts.breakupseglists(background_seglists, min_segment_length, gstlal_inspiral_overlap)


#
# build analysis branch
#

if config_parser.has_option("pipeline", "max_cafe_extent"):
	extentlimit = config_parser.getfloat("pipeline", "max_cafe_extent")
else:
	extentlimit = None

coinc_nodes = make_coinc_branch(dag, datafinds, background_seglists, background_time_slides, vetoes_cache, config_parser.get("pipeline", "user_tag"), do_injections = options.injections, extentlimit = extentlimit, verbose = options.verbose)


#
# =============================================================================
#
#                                     Done
#
# =============================================================================
#


#
# write DAG
#


dag.write_sub_files()
dag.write_dag()
dag.write_script()

#
# write a message telling the user that the DAG has been written
#


print "\n\nDone\nwrote \"%s\"\n" % dag.get_dag_file()
