#!/usr/bin/python
#
# Copyright (C) 2010  Kipp Cannon <kipp.cannon@ligo.org>
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


"""
gstlal_inspiral dag generator
"""


import sys, os
import tempfile
import ConfigParser
from optparse import OptionParser


from glue import pipeline
from glue import segments
from glue import segmentsUtils
from glue.lal import CacheEntry
from pylal import ligolw_tisi
from lalapps import power
from gstlal import dagparts as gstlaldagparts


__author__ = "Kipp Cannon <kipp.cannon@ligo.org>"
__date__ = "" #FIXME
__version__ = "" #FIXME


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		usage = "%prog [options] ...",
		description = "FIXME"
	)
	parser.add_option("-i", "--injections", action = "store_true", help = "Add software inspiral.")
	parser.add_option("-f", "--config-file", metavar = "filename", help = "Use this configuration file (required).")
	parser.add_option("-l", "--log-path", metavar = "path", help = "Make condor put log files in this directory (required).")
	parser.add_option("-t", "--time-slides", metavar = "filename", help = "Load time-slide vectors from this XML file (required).")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")

	options, filenames = parser.parse_args()

	required_options = ["log_path", "config_file", "time_slides"]
	missing_options = [option for option in required_options if getattr(options, option) is None]
	if missing_options:
		raise ValueError, "missing required options %s" % (sorted("--%s" % option.replace("_", "-") for option in missing_options))

	return options, filenames


#
# =============================================================================
#
#                               Main DAG Branch
#
# =============================================================================
#


def make_coinc_branch(dag, datafinds, seglists, time_slides, tag, do_injections = False, verbose = False):
	#
	# injection job
	#

	if options.injections:
		injnodes = power.make_inspinj_fragment(dag, seglists.extent_all(), tag, 0.0)
	else:
		injnodes = set()

	# artificial parent-child relationship to force inspinj jobs to run before all
	# datafinds.  makes dag run faster because it allows string search jobs to
	# move onto the cluster before all the datafinds complete
	for injnode in injnodes:
		for datafindnode in datafinds:
			datafindnode.add_parent(injnode)

	#
	# trigger generator jobs
	#

	trigger_nodes = gstlaldagparts.make_single_instrument_stage(dag, datafinds, seglists, tag, inspinjnodes = injnodes, verbose = verbose)

	#
	# ligolw_sicluster
	#

	trigger_nodes = gstlaldagparts.make_sicluster_fragment(dag, trigger_nodes, "%s" % tag, verbose = verbose)

	#
	# coincidence analysis
	#

	coinc_nodes = set()
	inj_cache = set([cache_entry for node in injnodes for cache_entry in node.get_output_cache()])
	for n, (time_slides_cache_entry, these_time_slides) in enumerate(time_slides.items()):
		if verbose:
			print >>sys.stderr, "%s %d/%d (%s):" % (tag, n + 1, len(time_slides), time_slides_cache_entry.path())

		#
		# ligolw_cafe & ligolw_add
		#

		tisi_cache = set([time_slides_cache_entry])
		nodes = set()
		for seg, parents, cache in power.group_coinc_parents(trigger_nodes, these_time_slides, verbose = verbose):
			preserve_cache = inj_cache | tisi_cache
			if not options.injections:
				preserve_cache |= cache
			nodes |= power.make_lladd_fragment(dag, parents | injnodes, "%s_%d" % (tag, n), segment = seg, input_cache = cache | inj_cache, extra_input_cache = tisi_cache, preserve_cache = preserve_cache)

		#
		# ligolw_thinca
		#

		if verbose:
			print >>sys.stderr, "building thinca jobs ..."
		coinc_nodes |= gstlaldagparts.make_thinca_fragment(dag, nodes, "%s_%d" % (tag, n), verbose = verbose)
		if verbose:
			print >>sys.stderr, "done %s %d/%d" % (tag, n + 1, len(time_slides))

	#
	# ligolw_inspinjfind
	#

	if do_injections:
		if verbose:
			print >>sys.stderr, "building inspinjfind jobs ..."
		coinc_nodes = gstlaldagparts.make_inspinjfind_fragment(dag, coinc_nodes, tag, verbose = verbose)

	#
	# ligolw_sqlite
	#

	if verbose:
		print >>sys.stderr, "building sqlite jobs ..."
	coinc_nodes = power.make_sqlite_fragment(dag, coinc_nodes, tag, verbose = verbose)

	#
	# done
	#

	power.write_output_cache(coinc_nodes, "%s_%s_output.cache" % (os.path.splitext(dag.get_dag_file())[0], tag))
	return coinc_nodes


#
# =============================================================================
#
#                                  Initialize
#
# =============================================================================
#


#
# command line
#


options, filenames = parse_command_line()


#
# base string for file names
#


basename = os.path.splitext(os.path.basename(options.config_file))[0]


#
# create the config parser object and read in the ini file
#


config_parser = ConfigParser.ConfigParser()
config_parser.read(options.config_file)


#
# initialize lalapps.power and gstlal.dagparts modules
#


power.init_job_types(config_parser, job_types = ("datafind", "lladd", "sqlite"))
gstlaldagparts.init_job_types(config_parser, job_types = ("inspinj", "gstlalinspiral", "inspinjfind", "sicluster", "thinca"))


#
# make directories to store the cache files, job logs, and trigger output
#


power.make_dag_directories(config_parser)
try:
	os.mkdir("triggers")
except:
	pass


#
# create a log file that the Condor jobs will write to
#


logfile = tempfile.mkstemp(prefix = basename, suffix = '.log', dir = options.log_path)[1]


#
# create the DAG writing the log to the specified directory
#


dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)


#
# get the instruments and raw segments
#


seglists = segments.segmentlistdict((instrument, segmentsUtils.fromsegwizard(file(config_parser.get("input", "segments_%s" % instrument))).coalesce()) for instrument in set(s.strip() for s in config_parser.get("pipeline", "ifos").split(",")))


#
# Using time slide information, construct segment lists describing times
# requiring trigger construction.
#


if options.verbose:
	print >>sys.stderr, "computing segments for which gstlal_inspiral jobs are required ..."
min_segment_length = config_parser.getfloat("pipeline", "min_segment_length")
background_time_slides = {}
background_seglists = segments.segmentlistdict()
for filename in [options.time_slides]:
	cache_entry = CacheEntry(None, None, None, "file://localhost" + os.path.abspath(filename))
	background_time_slides[cache_entry] = ligolw_tisi.load_time_slides(filename, verbose = options.verbose, gz = filename.endswith(".gz")).values()
	background_seglists |= gstlaldagparts.compute_segment_lists(seglists, background_time_slides[cache_entry], min_segment_length, verbose = options.verbose)


#
# =============================================================================
#
#                                  Build DAG
#
# =============================================================================
#


#
# datafinds
#


datafinds = power.make_datafind_stage(dag, background_seglists, verbose = options.verbose)


#
# build analysis branch
#


coinc_nodes = make_coinc_branch(dag, datafinds, background_seglists, background_time_slides, config_parser.get("pipeline", "user_tag"), do_injections = options.injections, verbose = options.verbose)


#
# =============================================================================
#
#                                     Done
#
# =============================================================================
#


#
# write DAG
#


dag.write_sub_files()
dag.write_dag()


#
# write a message telling the user that the DAG has been written
#


print "\n\nDone\nwrote \"%s\"\n" % dag.get_dag_file()
